{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d37779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('./utils'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from grid2op.gym_compat import GymEnv\n",
    "from gym import Env\n",
    "\n",
    "from l2rpn_baselines.utils import ReplayBuffer\n",
    "from l2rpn_baselines.utils import TrainingParam\n",
    "from l2rpn_baselines.utils import cli_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33de8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import grid2op\n",
    "from grid2op.Exceptions import Grid2OpException\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "from grid2op.Converter import IdToAct\n",
    "\n",
    "#from l2rpn_baselines.utils.ReplayBuffer import ReplayBuffer\n",
    "#from l2rpn_baselines.utils.TrainingParam import TrainingParam\n",
    "\n",
    "try:\n",
    "    from grid2op.Chronics import MultifolderWithCache\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = True\n",
    "except ImportError:\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1922b14",
   "metadata": {},
   "source": [
    "# DeepQAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83b6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        from tensorflow.keras.models import Sequential, Model\n",
    "        from tensorflow.keras.layers import Activation, Dense\n",
    "        from tensorflow.keras.layers import Input\n",
    "    _CAN_USE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _CAN_USE_TENSORFLOW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801794dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent(AgentWithConverter):\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 nn_archi,\n",
    "                 name=\"DeepQAgent\",\n",
    "                 store_action=True,\n",
    "                 istraining=False,\n",
    "                 filter_action_fun=None,\n",
    "                 verbose=False,\n",
    "                 observation_space=None,\n",
    "                 **kwargs_converters):\n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct, **kwargs_converters)\n",
    "        self.filter_action_fun = filter_action_fun\n",
    "        if self.filter_action_fun is not None:\n",
    "            self.action_space.filter_action(self.filter_action_fun)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = None\n",
    "        self.__nb_env = None\n",
    "\n",
    "        self.deep_q = None\n",
    "        self._training_param = None\n",
    "        self._tf_writer = None\n",
    "        self.name = name\n",
    "        self._losses = None\n",
    "        self.__graph_saved = False\n",
    "        self.store_action = store_action\n",
    "        self.dict_action = {}\n",
    "        self.istraining = istraining\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        # for tensorbaord\n",
    "        self._train_lr = None\n",
    "\n",
    "        self._reset_num = None\n",
    "\n",
    "        self._max_iter_env_ = 1000000\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = 0.\n",
    "\n",
    "        # action type\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for over sampling the hard scenarios\n",
    "        self._prev_obs_num = 0\n",
    "        self._time_step_lived = None\n",
    "        self._nb_chosen = None\n",
    "        self._proba = None\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        # neural network architecture\n",
    "        self._nn_archi = nn_archi\n",
    "\n",
    "        # observation tranformers\n",
    "        self._obs_as_vect = None\n",
    "        self._tmp_obs = None\n",
    "        self._indx_obs = None\n",
    "        self.verbose = verbose\n",
    "        if observation_space is None:\n",
    "            pass\n",
    "        else:\n",
    "            self.init_obs_extraction(observation_space)\n",
    "\n",
    "        # for the frequency of action type\n",
    "        self.current_ = 0\n",
    "        self.nb_ = 10\n",
    "        self._nb_this_time = np.zeros((self.nb_, 7))\n",
    "\n",
    "        #\n",
    "        self._vector_size = None\n",
    "        self._actions_per_ksteps = None\n",
    "        self._illegal_actions_per_ksteps = None\n",
    "        self._ambiguous_actions_per_ksteps = None\n",
    "\n",
    "    def _fill_vectors(self, training_param):\n",
    "        self._vector_size  = self.nb_ * training_param.update_tensorboard_freq\n",
    "        self._actions_per_ksteps = np.zeros((self._vector_size, self.action_space.size()), dtype=np.int)\n",
    "        self._illegal_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "        self._ambiguous_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "\n",
    "    # grid2op.Agent interface\n",
    "    def convert_obs(self, observation):\n",
    "        obs_as_vect = observation.to_vect()\n",
    "        self._tmp_obs[:] = obs_as_vect[self._indx_obs]\n",
    "        return self._tmp_obs\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation,\n",
    "                                                                epsilon=0.0,\n",
    "                                                                training=False)\n",
    "        res = int(predict_movement_int)\n",
    "        self._store_action_played(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_size(action_space, filter_fun, kwargs_converters):\n",
    "        converter = IdToAct(action_space)\n",
    "        converter.init_converter(**kwargs_converters)\n",
    "        if filter_fun is not None:\n",
    "            converter.filter_action(filter_fun)\n",
    "        return converter.n\n",
    "\n",
    "    def init_obs_extraction(self, observation_space):\n",
    "        tmp = np.zeros(0, dtype=np.uint)  # TODO platform independant\n",
    "        for obs_attr_name in self._nn_archi.get_obs_attr():\n",
    "            beg_, end_, dtype_ = observation_space.get_indx_extract(obs_attr_name)\n",
    "            tmp = np.concatenate((tmp, np.arange(beg_, end_, dtype=np.uint)))\n",
    "        self._indx_obs = tmp\n",
    "        self._tmp_obs = np.zeros((1, tmp.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # baseline interface\n",
    "    def load(self, path):\n",
    "        # not modified compare to original implementation\n",
    "        tmp_me = os.path.join(path, self.name)\n",
    "        if not os.path.exists(tmp_me):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(tmp_me))\n",
    "        self._load_action_space(tmp_me)\n",
    "\n",
    "        # TODO handle case where training param class has been overidden\n",
    "        self._training_param = TrainingParam.from_json(os.path.join(tmp_me, \"training_params.json\".format(self.name)))\n",
    "        self.deep_q = self._nn_archi.make_nn(self._training_param)\n",
    "        try:\n",
    "            self.deep_q.load_network(tmp_me, name=self.name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to load the model located at \\\"{}\\\" with error \\n{}\".format(path, e))\n",
    "\n",
    "        for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "            conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "            if os.path.exists(conv_path):\n",
    "                setattr(self, nm_attr, np.load(file=conv_path))\n",
    "\n",
    "    def save(self, path):\n",
    "        if path is not None:\n",
    "            tmp_me = os.path.join(path, self.name)\n",
    "            if not os.path.exists(tmp_me):\n",
    "                os.mkdir(tmp_me)\n",
    "            nm_conv = \"action_space.npy\"\n",
    "            conv_path = os.path.join(tmp_me, nm_conv)\n",
    "            if not os.path.exists(conv_path):\n",
    "                self.action_space.save(path=tmp_me, name=nm_conv)\n",
    "\n",
    "            self._training_param.save_as_json(tmp_me, name=\"training_params.json\")\n",
    "            self._nn_archi.save_as_json(tmp_me, \"nn_architecture.json\")\n",
    "            self.deep_q.save_network(tmp_me, name=self.name)\n",
    "\n",
    "            # TODO save the \"oversampling\" part, and all the other info\n",
    "            for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "                conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "                attr_ = getattr(self, nm_attr)\n",
    "                if attr_ is not None:\n",
    "                    np.save(arr=attr_, file=conv_path)\n",
    "\n",
    "    def train(self,\n",
    "              env,\n",
    "              iterations,\n",
    "              save_path,\n",
    "              logdir,\n",
    "              training_param=None):\n",
    "\n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "\n",
    "        self._train_lr = training_param.lr\n",
    "\n",
    "        if self._training_param is None:\n",
    "            self._training_param = training_param\n",
    "        else:\n",
    "            training_param = self._training_param\n",
    "        self._init_deep_q(self._training_param, env)\n",
    "        self._fill_vectors(self._training_param)\n",
    "\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "        # efficient reading of the data (read them by chunk of roughly 1 day\n",
    "        nb_ts_one_day = 24 * 60 / 5  # number of time steps per day\n",
    "        self._set_chunk(env, nb_ts_one_day)\n",
    "\n",
    "        # Create file system related vars\n",
    "        if save_path is not None:\n",
    "            save_path = os.path.abspath(save_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if logdir is not None:\n",
    "            logpath = os.path.join(logdir, self.name)\n",
    "            self._tf_writer = tf.summary.create_file_writer(logpath, name=self.name)\n",
    "        else:\n",
    "            logpath = None\n",
    "            self._tf_writer = None\n",
    "        UPDATE_FREQ = training_param.update_tensorboard_freq  # update tensorboard every \"UPDATE_FREQ\" steps\n",
    "        SAVING_NUM = training_param.save_model_each\n",
    "\n",
    "        if hasattr(env, \"nb_env\"):\n",
    "            nb_env = env.nb_env\n",
    "            warnings.warn(\"Training using {} environments\".format(nb_env))\n",
    "            self.__nb_env = nb_env\n",
    "        else:\n",
    "            self.__nb_env = 1\n",
    "        # if isinstance(env, grid2op.Environment.Environment):\n",
    "        #     self.__nb_env = 1\n",
    "        # else:\n",
    "        #     import warnings\n",
    "        #     nb_env = env.nb_env\n",
    "        #     warnings.warn(\"Training using {} environments\".format(nb_env))\n",
    "        #     self.__nb_env = nb_env\n",
    "\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "        training_step = self._training_param.last_step\n",
    "\n",
    "        # some parameters have been move to a class named \"training_param\" for convenience\n",
    "        self.epsilon = self._training_param.initial_epsilon\n",
    "\n",
    "        # now the number of alive frames and total reward depends on the \"underlying environment\". It is vector instead\n",
    "        # of scalar\n",
    "        alive_frame, total_reward = self._init_global_train_loop()\n",
    "        reward, done = self._init_local_train_loop()\n",
    "        epoch_num = 0\n",
    "        self._losses = np.zeros(iterations)\n",
    "        alive_frames = np.zeros(iterations)\n",
    "        total_rewards = np.zeros(iterations)\n",
    "        new_state = None\n",
    "        self._reset_num = 0\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = env.reward_range[1]\n",
    "\n",
    "        # action types\n",
    "        # injection, voltage, topology, line, redispatching = action.get_types()\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for non uniform random sampling of the scenarios\n",
    "        th_size = None\n",
    "        self._prev_obs_num = 0\n",
    "        if self.__nb_env == 1:\n",
    "            # TODO make this available for multi env too\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    th_size = env.chronics_handler.real_data.cache_size\n",
    "            if th_size is None:\n",
    "                th_size = len(env.chronics_handler.real_data.subpaths)\n",
    "\n",
    "            # number of time step lived per possible scenarios\n",
    "            if self._time_step_lived is None or self._time_step_lived.shape[0] != th_size:\n",
    "                self._time_step_lived = np.zeros(th_size, dtype=np.uint64)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._nb_chosen is None or self._nb_chosen.shape[0] != th_size:\n",
    "                self._nb_chosen = np.zeros(th_size, dtype=np.uint)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._proba is None or self._proba.shape[0] != th_size:\n",
    "                self._proba = np.ones(th_size, dtype=np.float64)\n",
    "\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        with tqdm(total=iterations - training_step, disable=not self.verbose) as pbar:\n",
    "            while training_step < iterations:\n",
    "                # reset or build the environment\n",
    "                initial_state = self._need_reset(env, training_step, epoch_num, done, new_state)\n",
    "\n",
    "                # Slowly decay the exploration parameter epsilon\n",
    "                # if self.epsilon > training_param.FINAL_EPSILON:\n",
    "                self.epsilon = self._training_param.get_next_epsilon(current_step=training_step)\n",
    "\n",
    "                # then we need to predict the next moves. Agents have been adapted to predict a batch of data\n",
    "                pm_i, pq_v, act = self._next_move(initial_state, self.epsilon, training_step)\n",
    "\n",
    "                # todo store the illegal / ambiguous / ... actions\n",
    "                reward, done = self._init_local_train_loop()\n",
    "                if self.__nb_env == 1:\n",
    "                    # still the \"hack\" to have same interface between multi env and env...\n",
    "                    # yeah it's a pain\n",
    "                    act = act[0]\n",
    "\n",
    "                temp_observation_obj, temp_reward, temp_done, info = env.step(act)\n",
    "                if self.__nb_env == 1:\n",
    "                    # dirty hack to wrap them into list\n",
    "                    temp_observation_obj = [temp_observation_obj]\n",
    "                    temp_reward = np.array([temp_reward], dtype=np.float32)\n",
    "                    temp_done = np.array([temp_done], dtype=np.bool)\n",
    "                    info = [info]\n",
    "\n",
    "                new_state = self._convert_obs_train(temp_observation_obj)\n",
    "                self._updage_illegal_ambiguous(training_step, info)\n",
    "                done, reward, total_reward, alive_frame, epoch_num \\\n",
    "                    = self._update_loop(done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num)\n",
    "\n",
    "                # update the replay buffer\n",
    "                self._store_new_state(initial_state, pm_i, reward, done, new_state)\n",
    "\n",
    "                # now train the model\n",
    "                if not self._train_model(training_step):\n",
    "                    # infinite loss in this case\n",
    "                    raise RuntimeError(\"ERROR INFINITE LOSS\")\n",
    "\n",
    "                # Save the network every 1000 iterations\n",
    "                if training_step % SAVING_NUM == 0 or training_step == iterations - 1:\n",
    "                    self.save(save_path)\n",
    "\n",
    "                # save some information to tensorboard\n",
    "                alive_frames[epoch_num] = np.mean(alive_frame)\n",
    "                total_rewards[epoch_num] = np.mean(total_reward)\n",
    "                self._store_action_played_train(training_step, pm_i)\n",
    "                self._save_tensorboard(training_step, epoch_num, UPDATE_FREQ, total_rewards, alive_frames)\n",
    "                training_step += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        self.save(save_path)\n",
    "\n",
    "    # auxiliary functions\n",
    "    # two below function: to train with multiple environments\n",
    "    def _convert_obs_train(self, observations):\n",
    "        \"\"\" create the observations that are used for training.\"\"\"\n",
    "        if self._obs_as_vect is None:\n",
    "            size_obs = self.convert_obs(observations[0]).shape[1]\n",
    "            self._obs_as_vect = np.zeros((self.__nb_env, size_obs), dtype=np.float32)\n",
    "\n",
    "        for i, obs in enumerate(observations):\n",
    "            self._obs_as_vect[i, :] = self.convert_obs(obs).reshape(-1)\n",
    "        return self._obs_as_vect\n",
    "\n",
    "    def _create_action_if_not_registered(self, action_int):\n",
    "        \"\"\"make sure that `action_int` is present in dict_action\"\"\"\n",
    "        if action_int not in self.dict_action:\n",
    "            act = self.action_space.all_actions[action_int]\n",
    "            is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_dn = \\\n",
    "                False, False, False, False, False, False, False\n",
    "            try:\n",
    "                # feature unavailble in grid2op <= 0.9.2\n",
    "                try:\n",
    "                    # storage introduced in grid2op 1.5.0 so if below it is not supported\n",
    "                    is_inj, is_volt, is_topo, is_line_status, is_redisp = act.get_types()\n",
    "                except ValueError as exc_:\n",
    "                    is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage = act.get_types()\n",
    "\n",
    "                is_dn = (not is_inj) and (not is_volt) and (not is_topo) and (not is_line_status) and (not is_redisp)\n",
    "                is_dn = is_dn and (not is_storage)\n",
    "            except Exception as exc_:\n",
    "                pass\n",
    "\n",
    "            self.dict_action[action_int] = [0, act,\n",
    "                                            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_dn)]\n",
    "\n",
    "    def _store_action_played(self, action_int):\n",
    "        \"\"\"if activated, this function will store the action taken by the agent.\"\"\"\n",
    "        if self.store_action:\n",
    "            self._create_action_if_not_registered(action_int)\n",
    "\n",
    "            self.dict_action[action_int][0] += 1\n",
    "            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_dn) = self.dict_action[action_int][2]\n",
    "            if is_inj:\n",
    "                self.nb_injection += 1\n",
    "            if is_volt:\n",
    "                self.nb_voltage += 1\n",
    "            if is_topo:\n",
    "                self.nb_topology += 1\n",
    "            if is_line_status:\n",
    "                self.nb_line += 1\n",
    "            if is_redisp:\n",
    "                self.nb_redispatching += 1\n",
    "            if is_storage:\n",
    "                self.nb_storage += 1\n",
    "            if is_dn:\n",
    "                self.nb_do_nothing += 1\n",
    "\n",
    "    def _convert_all_act(self, act_as_integer):\n",
    "        \"\"\"this function converts the action given as a list of integer. It ouputs a list of valid grid2op Action\"\"\"\n",
    "        res = []\n",
    "        for act_id in act_as_integer:\n",
    "            res.append(self.convert_act(act_id))\n",
    "            self._store_action_played(act_id)\n",
    "        return res\n",
    "\n",
    "    def _load_action_space(self, path):\n",
    "        \"\"\" load the action space in case the model is reloaded\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(path))\n",
    "        try:\n",
    "            self.action_space.init_converter(\n",
    "                all_actions=os.path.join(path, \"action_space.npy\".format(self.name)))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to reload converter action space with error \\n{}\".format(e))\n",
    "\n",
    "    # utilities for data reading\n",
    "    def _set_chunk(self, env, nb):\n",
    "        \"\"\"\n",
    "        to optimize the data reading process. See the official grid2op documentation for the effect of setting\n",
    "        the chunk size for the environment.\n",
    "        \"\"\"\n",
    "        env.set_chunk_size(int(max(100, nb)))\n",
    "\n",
    "    def _train_model(self, training_step):\n",
    "        \"\"\"train the deep q networks.\"\"\"\n",
    "        self._training_param.tell_step(training_step)\n",
    "        if training_step > max(self._training_param.min_observation, self._training_param.minibatch_size) and \\\n",
    "            self._training_param.do_train():\n",
    "\n",
    "            # train the model\n",
    "            s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(self._training_param.minibatch_size)\n",
    "            tf_writer = None\n",
    "            if self.__graph_saved is False:\n",
    "                tf_writer = self._tf_writer\n",
    "            loss = self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch,\n",
    "                                     tf_writer)\n",
    "            # save learning rate for later\n",
    "            self._train_lr = self.deep_q._optimizer_model._decayed_lr('float32').numpy()\n",
    "            self.__graph_saved = True\n",
    "            if not np.all(np.isfinite(loss)):\n",
    "                # if the loss is not finite i stop the learning\n",
    "                return False\n",
    "            self.deep_q.target_train()\n",
    "            self._losses[training_step:] = np.sum(loss)\n",
    "        return True\n",
    "\n",
    "    def _updage_illegal_ambiguous(self, curr_step, info):\n",
    "        \"\"\"update the conunt of illegal and ambiguous actions\"\"\"\n",
    "        tmp_ = curr_step % self._vector_size\n",
    "        self._illegal_actions_per_ksteps[tmp_] = np.sum([el[\"is_illegal\"] for el in info])\n",
    "        self._ambiguous_actions_per_ksteps[tmp_] = np.sum([el[\"is_ambiguous\"] for el in info])\n",
    "\n",
    "    def _store_action_played_train(self, training_step, action_id):\n",
    "        \"\"\"store which action were played, for tensorboard only.\"\"\"\n",
    "        which_row = training_step % self._vector_size\n",
    "        self._actions_per_ksteps[which_row, :] = 0\n",
    "        self._actions_per_ksteps[which_row, action_id] += 1\n",
    "\n",
    "    def _fast_forward_env(self, env, time=7*24*60/5):\n",
    "        \"\"\"use this functio to skip some time steps when environment is reset.\"\"\"\n",
    "        my_int = np.random.randint(0, min(time, env.chronics_handler.max_timestep()))\n",
    "        env.fast_forward_chronics(my_int)\n",
    "\n",
    "    def _reset_env_clean_state(self, env):\n",
    "        \"\"\"\n",
    "        reset this environment to a proper state. This should rather be integrated in grid2op. And will probably\n",
    "        be integrated partially starting from grid2op 1.0.0\n",
    "        \"\"\"\n",
    "        # /!\\ DO NOT ATTEMPT TO MODIFY OTHERWISE IT WILL PROBABLY CRASH /!\\\n",
    "        # /!\\ THIS WILL BE PART OF THE ENVIRONMENT IN FUTURE GRID2OP RELEASE (>= 1.0.0) /!\\\n",
    "        # AND OF COURSE USING THIS METHOD DURING THE EVALUATION IS COMPLETELY FORBIDDEN\n",
    "        if self.__nb_env > 1:\n",
    "            return\n",
    "        env.current_obs = None\n",
    "        env.env_modification = None\n",
    "        env._reset_maintenance()\n",
    "        env._reset_redispatching()\n",
    "        env._reset_vectors_and_timings()\n",
    "        _backend_action = env._backend_action_class()\n",
    "        _backend_action.all_changed()\n",
    "        env._backend_action =_backend_action\n",
    "        env.backend.apply_action(_backend_action)\n",
    "        _backend_action.reset()\n",
    "        *_, fail_to_start, info = env.step(env.action_space())\n",
    "        if fail_to_start:\n",
    "            # this is happening because not enough care has been taken to handle these problems\n",
    "            # more care will be taken when this feature will be available in grid2op directly.\n",
    "            raise Grid2OpException(\"Impossible to initialize the powergrid, the powerflow diverge at iteration 0. \"\n",
    "                                   \"Available information are: {}\".format(info))\n",
    "        env._reset_vectors_and_timings()\n",
    "\n",
    "    def _need_reset(self, env, observation_num, epoch_num, done, new_state):\n",
    "        \"\"\"perform the proper reset of the environment\"\"\"\n",
    "        if self._training_param.step_increase_nb_iter is not None and \\\n",
    "           self._training_param.step_increase_nb_iter > 0:\n",
    "            self._max_iter_env(min(max(self._training_param.min_iter,\n",
    "                                       self._training_param.max_iter_fun(self._total_sucesses)),\n",
    "                                   self._training_param.max_iter))  # TODO\n",
    "        self._curr_iter_env += 1\n",
    "        if new_state is None:\n",
    "            # it's the first ever loop\n",
    "            obs = env.reset()\n",
    "            if self.__nb_env == 1:\n",
    "                # still hack to have same program interface between multi env and not multi env\n",
    "                obs = [obs]\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        elif self.__nb_env > 1:\n",
    "            # in multi env this is automatically handled\n",
    "            pass\n",
    "        elif done[0]:\n",
    "            nb_ts_one_day = 24*60/5\n",
    "            if False:\n",
    "                # the 3-4 lines below allow to reuse the loaded dataset and continue further up in the\n",
    "                try:\n",
    "                    self._reset_env_clean_state(env)\n",
    "                    # random fast forward between now and next day\n",
    "                    self._fast_forward_env(env, time=nb_ts_one_day)\n",
    "                except (StopIteration, Grid2OpException):\n",
    "                    env.reset()\n",
    "                    # random fast forward between now and next week\n",
    "                    self._fast_forward_env(env, time=7*nb_ts_one_day)\n",
    "\n",
    "            # update the number of time steps it has live\n",
    "            ts_lived = observation_num - self._prev_obs_num\n",
    "            if self._time_step_lived is not None:\n",
    "                self._time_step_lived[self._prev_id] += ts_lived\n",
    "            self._prev_obs_num = observation_num\n",
    "            if self._training_param.oversampling_rate is not None:\n",
    "                # proba = np.sqrt(1. / (self._time_step_lived +1))\n",
    "                # # over sampling some kind of \"UCB like\" stuff\n",
    "                # # https://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/\n",
    "\n",
    "                # proba = 1. / (self._time_step_lived + 1)\n",
    "                self._proba[:] = 1. / (self._time_step_lived ** self._training_param.oversampling_rate + 1)\n",
    "                self._proba /= np.sum(self._proba)\n",
    "\n",
    "            _prev_id = self._prev_id\n",
    "            self._prev_id = None\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    self._prev_id = env.chronics_handler.real_data.sample_next_chronics(self._proba)\n",
    "            if self._prev_id is None:\n",
    "                self._prev_id = _prev_id + 1\n",
    "                self._prev_id %= self._time_step_lived.shape[0]\n",
    "\n",
    "            obs = self._reset_env(env, epoch_num)\n",
    "            if self._training_param.sample_one_random_action_begin is not None and \\\n",
    "                    observation_num < self._training_param.sample_one_random_action_begin:\n",
    "                done = True\n",
    "                while done:\n",
    "                    act = env.action_space(env.action_space._sample_set_bus())\n",
    "                    obs, reward, done, info = env.step(act)\n",
    "                    if info[\"is_illegal\"] or info[\"is_ambiguous\"]:\n",
    "                        # there are no guarantee that sampled action are legal nor perfectly\n",
    "                        # correct.\n",
    "                        # if that is the case, i \"simply\" restart the process, as if the action\n",
    "                        # broke everything\n",
    "                        done = True\n",
    "\n",
    "                    if done:\n",
    "                        obs = self._reset_env(env, epoch_num)\n",
    "                    else:\n",
    "                        if self.verbose:\n",
    "                            print(\"step {}: {}\".format(observation_num, act))\n",
    "\n",
    "                obs = [obs]  # for compatibility with multi env...\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        return new_state\n",
    "\n",
    "    def _reset_env(self, env, epoch_num):\n",
    "        env.reset()\n",
    "        if self._nb_chosen is not None:\n",
    "            self._nb_chosen[self._prev_id] += 1\n",
    "\n",
    "        # random fast forward between now and next week\n",
    "        if self._training_param.random_sample_datetime_start is not None:\n",
    "            self._fast_forward_env(env, time=self._training_param.random_sample_datetime_start)\n",
    "\n",
    "        self._curr_iter_env = 0\n",
    "        obs = [env.current_obs]\n",
    "        if epoch_num % len(env.chronics_handler.real_data.subpaths) == 0:\n",
    "            # re shuffle the data\n",
    "            env.chronics_handler.shuffle(lambda x: x[np.random.choice(len(x), size=len(x), replace=False)])\n",
    "        return obs\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"create and initialized the replay buffer\"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(self._training_param.buffer_size)\n",
    "\n",
    "    def _store_new_state(self, initial_state, predict_movement_int, reward, done, new_state):\n",
    "        \"\"\"store the new state in the replay buffer\"\"\"\n",
    "        # vectorized version of the previous code\n",
    "        for i_s, pm_i, reward, done, ns in zip(initial_state, predict_movement_int, reward, done, new_state):\n",
    "            self.replay_buffer.add(i_s,\n",
    "                                   pm_i,\n",
    "                                   reward,\n",
    "                                   done,\n",
    "                                   ns)\n",
    "\n",
    "    def _max_iter_env(self, new_max_iter):\n",
    "        \"\"\"update the number of maximum iteration allowed.\"\"\"\n",
    "        self._max_iter_env_ = new_max_iter\n",
    "\n",
    "    def _next_move(self, curr_state, epsilon, training_step):\n",
    "        # supposes that 0 encodes for do nothing, otherwise it will NOT work (for the observer)\n",
    "        pm_i, pq_v, q_actions = self.deep_q.predict_movement(curr_state, epsilon, training=True)\n",
    "        # TODO implement the \"max XXX random action per scenarios\"\n",
    "        pm_i, pq_v = self._short_circuit_actions(training_step, pm_i, pq_v, q_actions)\n",
    "        act = self._convert_all_act(pm_i)\n",
    "        return pm_i, pq_v, act\n",
    "\n",
    "    def _short_circuit_actions(self, training_step, pm_i, pq_v, q_actions):\n",
    "        if self._training_param.min_observe is not None and \\\n",
    "                training_step < self._training_param.min_observe:\n",
    "            # action is replaced by do nothing due to the \"observe only\" specification\n",
    "            pm_i[:] = 0\n",
    "            pq_v[:] = q_actions[:, 0]\n",
    "        return pm_i, pq_v\n",
    "\n",
    "    def _init_global_train_loop(self):\n",
    "        alive_frame = np.zeros(self.__nb_env, dtype=np.int)\n",
    "        total_reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        return alive_frame, total_reward\n",
    "\n",
    "    def _update_loop(self, done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num):\n",
    "        if self.__nb_env == 1:\n",
    "            # force end of episode at early stage of learning\n",
    "            if self._curr_iter_env >= self._max_iter_env_:\n",
    "                temp_done[0] = True\n",
    "                temp_reward[0] = self._max_reward\n",
    "                self._total_sucesses += 1\n",
    "\n",
    "        done = temp_done\n",
    "        alive_frame[done] = 0\n",
    "        total_reward[done] = 0.\n",
    "        self._reset_num += np.sum(done)\n",
    "        if self._reset_num >= self.__nb_env:\n",
    "            # increase the \"global epoch num\" represented by \"epoch_num\" only when on average\n",
    "            # all environments are \"done\"\n",
    "            epoch_num += 1\n",
    "            self._reset_num = 0\n",
    "\n",
    "        total_reward[~done] += temp_reward[~done]\n",
    "        alive_frame[~done] += 1\n",
    "        return done, temp_reward, total_reward, alive_frame, epoch_num\n",
    "\n",
    "    def _init_local_train_loop(self):\n",
    "        # reward, done = np.zeros(self.nb_process), np.full(self.nb_process, fill_value=False, dtype=np.bool)\n",
    "        reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        done = np.full(self.__nb_env, fill_value=False, dtype=np.bool)\n",
    "        return reward, done\n",
    "\n",
    "    def _init_deep_q(self, training_param, env):\n",
    "        \"\"\"\n",
    "        This function serves as initializin the neural network.\n",
    "        \"\"\"\n",
    "        if self.deep_q is None:\n",
    "            self.deep_q = self._nn_archi.make_nn(training_param)\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "    def _save_tensorboard(self, step, epoch_num, UPDATE_FREQ, epoch_rewards, epoch_alive):\n",
    "        \"\"\"save all the informations needed in tensorboard.\"\"\"\n",
    "        if self._tf_writer is None:\n",
    "            return\n",
    "\n",
    "        # Log some useful metrics every even updates\n",
    "        if step % UPDATE_FREQ == 0 and epoch_num > 0:\n",
    "            if step % (10 * UPDATE_FREQ) == 0:\n",
    "                # print the top k scenarios the \"hardest\" (ie chosen the most number of times\n",
    "                if self.verbose:\n",
    "                    top_k = 10\n",
    "                    if self._nb_chosen is not None:\n",
    "                        array_ = np.argsort(self._nb_chosen)[-top_k:][::-1]\n",
    "                        print(\"hardest scenarios\\n{}\".format(array_))\n",
    "                        print(\"They have been chosen respectively\\n{}\".format(self._nb_chosen[array_]))\n",
    "                        # print(\"Associated proba are\\n{}\".format(self._proba[array_]))\n",
    "                        print(\"The number of timesteps played is\\n{}\".format(self._time_step_lived[array_]))\n",
    "                        print(\"avg (accross all scenarios) number of timsteps played {}\"\n",
    "                              \"\".format(np.mean(self._time_step_lived)))\n",
    "                        print(\"Time alive: {}\".format(self._time_step_lived[array_] / (self._nb_chosen[array_] + 1)))\n",
    "                        print(\"Avg time alive: {}\".format(np.mean(self._time_step_lived / (self._nb_chosen + 1 ))))\n",
    "\n",
    "            with self._tf_writer.as_default():\n",
    "                last_alive = epoch_alive[(epoch_num-1)]\n",
    "                last_reward = epoch_rewards[(epoch_num-1)]\n",
    "\n",
    "                mean_reward = np.nanmean(epoch_rewards[:epoch_num])\n",
    "                mean_alive = np.nanmean(epoch_alive[:epoch_num])\n",
    "\n",
    "                mean_reward_30 = mean_reward\n",
    "                mean_alive_30 = mean_alive\n",
    "                mean_reward_100 = mean_reward\n",
    "                mean_alive_100 = mean_alive\n",
    "\n",
    "                tmp = self._actions_per_ksteps > 0\n",
    "                tmp = tmp.sum(axis=0)\n",
    "                nb_action_taken_last_kstep = np.sum(tmp > 0)\n",
    "\n",
    "                nb_illegal_act = np.sum(self._illegal_actions_per_ksteps)\n",
    "                nb_ambiguous_act = np.sum(self._ambiguous_actions_per_ksteps)\n",
    "\n",
    "                if epoch_num >= 100:\n",
    "                    mean_reward_100 = np.nanmean(epoch_rewards[(epoch_num-100):epoch_num])\n",
    "                    mean_alive_100 = np.nanmean(epoch_alive[(epoch_num-100):epoch_num])\n",
    "\n",
    "                if epoch_num >= 30:\n",
    "                    mean_reward_30 = np.nanmean(epoch_rewards[(epoch_num-30):epoch_num])\n",
    "                    mean_alive_30 = np.nanmean(epoch_alive[(epoch_num-30):epoch_num])\n",
    "\n",
    "                # to ensure \"fair\" comparison between single env and multi env\n",
    "                step_tb = step  # * self.__nb_env\n",
    "                # if multiply by the number of env we have \"trouble\" with random exploration at the beginning\n",
    "                # because it lasts the same number of \"real\" steps\n",
    "\n",
    "                # show first the Mean reward and mine time alive (hence the upper case)\n",
    "                tf.summary.scalar(\"Mean_alive_30\", mean_alive_30, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 30 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"Mean_reward_30\", mean_reward_30, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 30 completed episodes\")\n",
    "\n",
    "                # then it's alpha numerical order, hence the \"z_\" in front of some information\n",
    "                tf.summary.scalar(\"loss\", self._losses[step], step_tb,\n",
    "                                  description=\"Training loss (for the last training batch)\")\n",
    "\n",
    "                tf.summary.scalar(\"last_alive\", last_alive, step_tb,\n",
    "                                  description=\"Final number of steps for the last complete episode\")\n",
    "                tf.summary.scalar(\"last_reward\", last_reward, step_tb,\n",
    "                                  description=\"Final reward over the last complete episode\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward\", mean_reward, step_tb,\n",
    "                                  description=\"Average reward over the whole episodes played\")\n",
    "                tf.summary.scalar(\"mean_alive\", mean_alive, step_tb,\n",
    "                                  description=\"Average time alive over the whole episodes played\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward_100\", mean_reward_100, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 100 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"mean_alive_100\", mean_alive_100, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 100 completed episodes\")\n",
    "\n",
    "                tf.summary.scalar(\"nb_different_action_taken\", nb_action_taken_last_kstep, step_tb,\n",
    "                                  description=\"Number of different actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_illegal_act\", nb_illegal_act, step_tb,\n",
    "                                  description=\"Number of illegal actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_ambiguous_act\", nb_ambiguous_act, step_tb,\n",
    "                                  description=\"Number of ambiguous actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_total_success\", self._total_sucesses, step_tb,\n",
    "                                  description=\"Number of times the episode was completed entirely \"\n",
    "                                              \"(no game over)\")\n",
    "\n",
    "                tf.summary.scalar(\"z_lr\", self._train_lr, step_tb,\n",
    "                                  description=\"Current learning rate\")\n",
    "                tf.summary.scalar(\"z_epsilon\", self.epsilon, step_tb,\n",
    "                                  description=\"Current epsilon (from the epsilon greedy)\")\n",
    "                tf.summary.scalar(\"z_max_iter\", self._max_iter_env_, step_tb,\n",
    "                                  description=\"Maximum number of time steps before deciding a scenario \"\n",
    "                                              \"is over (=win)\")\n",
    "                tf.summary.scalar(\"z_total_episode\", epoch_num, step_tb,\n",
    "                                  description=\"Total number of episode played (number of \\\"reset\\\")\")\n",
    "\n",
    "                self.deep_q.save_tensorboard(step_tb)\n",
    "\n",
    "                if self.store_action:\n",
    "                    self._store_frequency_action_type(UPDATE_FREQ, step_tb)\n",
    "\n",
    "                if self._time_step_lived is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        \"timestep_lived\", self._time_step_lived, step=step_tb, buckets=None,\n",
    "                        description=\"Number of time steps lived for all scenarios\"\n",
    "                    )\n",
    "                if self._nb_chosen is not None:\n",
    "                    tf.summary.histogram(\n",
    "                        \"nb_chosen\", self._nb_chosen, step=step_tb, buckets=None,\n",
    "                        description=\"Number of times this scenarios has been played\"\n",
    "                    )\n",
    "\n",
    "    def _store_frequency_action_type(self, UPDATE_FREQ, step_tb):\n",
    "        self.current_ += 1\n",
    "        self.current_ %= self.nb_\n",
    "        nb_inj, nb_volt, nb_topo, nb_line, nb_redisp, nb_storage, nb_dn = self._nb_this_time[self.current_, :]\n",
    "        self._nb_this_time[self.current_, :] = [self.nb_injection,\n",
    "                                                self.nb_voltage,\n",
    "                                                self.nb_topology,\n",
    "                                                self.nb_line,\n",
    "                                                self.nb_redispatching,\n",
    "                                                self.nb_storage,\n",
    "                                                self.nb_do_nothing]\n",
    "\n",
    "        curr_inj = self.nb_injection - nb_inj\n",
    "        curr_volt = self.nb_voltage - nb_volt\n",
    "        curr_topo = self.nb_topology - nb_topo\n",
    "        curr_line = self.nb_line - nb_line\n",
    "        curr_redisp = self.nb_redispatching - nb_redisp\n",
    "        curr_storage = self.nb_storage - nb_storage\n",
    "        curr_dn = self.nb_do_nothing - nb_dn\n",
    "\n",
    "        total_act_num = curr_inj + curr_volt + curr_topo + curr_line + curr_redisp + curr_dn + curr_storage\n",
    "        tf.summary.scalar(\"zz_freq_inj\",\n",
    "                          curr_inj / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"injection\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"zz_freq_voltage\",\n",
    "                          curr_volt / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"voltage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_topo\",\n",
    "                          curr_topo / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"topo\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_line_status\",\n",
    "                          curr_line / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"line status\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_redisp\",\n",
    "                          curr_redisp / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"redispatching\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_do_nothing\",\n",
    "                          curr_dn / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"do nothing\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_storage\",\n",
    "                          curr_storage / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"storage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6419c8f",
   "metadata": {},
   "source": [
    "# DeepQSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaf9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NAME = \"DeepQSimple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838429d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQSimple(DeepQAgent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66f978",
   "metadata": {},
   "source": [
    "# BaseDeepQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557c01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras.optimizers as tfko\n",
    "    _CAN_USE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _CAN_USE_TENSORFLOW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dee90dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepQ(ABC):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None,\n",
    "                 verbose=False):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        self._action_size = nn_params.action_size\n",
    "        self._observation_size = nn_params.observation_size\n",
    "        self._nn_archi = nn_params\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if training_param is None:\n",
    "            self._training_param = TrainingParam()\n",
    "        else:\n",
    "            self._training_param = training_param\n",
    "\n",
    "        self._lr = training_param.lr\n",
    "        self._lr_decay_steps = training_param.lr_decay_steps\n",
    "        self._lr_decay_rate = training_param.lr_decay_rate\n",
    "\n",
    "        self._model = None\n",
    "        self._target_model = None\n",
    "        self._schedule_model = None\n",
    "        self._optimizer_model = None\n",
    "        self._custom_objects = None  # to be able to load other keras layers type\n",
    "\n",
    "    def make_optimiser(self):\n",
    "        schedule = tfko.schedules.InverseTimeDecay(self._lr, self._lr_decay_steps, self._lr_decay_rate)\n",
    "        return schedule, tfko.Adam(learning_rate=schedule)\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_q_network(self):\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def predict_movement(self, data, epsilon, batch_size=None, training=False):\n",
    "        if batch_size is None:\n",
    "            batch_size = data.shape[0]\n",
    "\n",
    "        # q_actions = self._model.predict(data, batch_size=batch_size)  # q value of each action\n",
    "        q_actions = self._model(data, training=training).numpy()\n",
    "        opt_policy = np.argmax(q_actions, axis=-1)\n",
    "        if epsilon > 0.:\n",
    "            rand_val = np.random.random(batch_size)\n",
    "            opt_policy[rand_val < epsilon] = np.random.randint(0, self._action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        return opt_policy, q_actions[np.arange(batch_size), opt_policy], q_actions\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, tf_writer=None, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = s_batch.shape[0]\n",
    "\n",
    "        # Save the graph just the first time\n",
    "        if tf_writer is not None:\n",
    "            tf.summary.trace_on()\n",
    "        target = self._model(s_batch, training=True).numpy()\n",
    "        fut_action = self._model(s2_batch, training=True).numpy()\n",
    "        if tf_writer is not None:\n",
    "            with tf_writer.as_default():\n",
    "                tf.summary.trace_export(\"model-graph\", 0)\n",
    "            tf.summary.trace_off()\n",
    "        target_next = self._target_model(s2_batch, training=True).numpy()\n",
    "\n",
    "        idx = np.arange(batch_size)\n",
    "        target[idx, a_batch] = r_batch\n",
    "        # update the value for not done episode\n",
    "        nd_batch = ~d_batch  # update with this rule only batch that did not game over\n",
    "        next_a = np.argmax(fut_action, axis=-1)  # compute the future action i will take in the next state\n",
    "        fut_Q = target_next[idx, next_a]  # get its Q value\n",
    "        target[nd_batch, a_batch[nd_batch]] += self._training_param.discount_factor * fut_Q[nd_batch]\n",
    "        loss = self.train_on_batch(self._model, self._optimizer_model, s_batch, target)\n",
    "        return loss\n",
    "\n",
    "    def train_on_batch(self, model, optimizer_model, x, y_true):\n",
    "        \"\"\"train the model on a batch of example. This can be overide\"\"\"\n",
    "        loss = model.train_on_batch(x, y_true)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_model(path, name=None):\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        self._model.save('{}.{}'.format(path_model, ext))\n",
    "        self._target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "\n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        # fix for issue https://github.com/keras-team/keras/issues/7440\n",
    "        self.construct_q_network()\n",
    "\n",
    "        self._model.load_weights('{}.{}'.format(path_model, ext))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            self._target_model.load_weights('{}.{}'.format(path_target_model, ext))\n",
    "        if self.verbose:\n",
    "            print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self._training_param.tau\n",
    "        tau_inv = 1.0 - tau\n",
    "\n",
    "        target_params = self._target_model.trainable_variables\n",
    "        source_params = self._model.trainable_variables\n",
    "        for src, dest in zip(source_params, target_params):\n",
    "            # Polyak averaging\n",
    "            var_update = src.value() * tau\n",
    "            var_persist = dest.value() * tau_inv\n",
    "            dest.assign(var_update + var_persist)\n",
    "\n",
    "    def save_tensorboard(self, current_step):\n",
    "        \"\"\"function used to save other information to tensorboard\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d73f04",
   "metadata": {},
   "source": [
    "# NNParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8470af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNParam(object):\n",
    "\n",
    "    _int_attr = [\"action_size\", \"observation_size\"]\n",
    "    _float_attr = []\n",
    "    _str_attr = []\n",
    "    _list_float = []\n",
    "    _list_str = [\"activs\", \"list_attr_obs\"]\n",
    "    _list_int = [\"sizes\"]\n",
    "    nn_class = BaseDeepQ\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs,\n",
    "                 ):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.sizes = [int(el) for el in sizes]\n",
    "        self.activs = [str(el) for el in activs]\n",
    "        if len(self.sizes) != len(self.activs):\n",
    "            raise RuntimeError(\"\\\"sizes\\\" and \\\"activs\\\" lists have not the same size. It's not clear how many layers \"\n",
    "                               \"you want your neural network to have.\")\n",
    "        self.list_attr_obs = [str(el) for el in list_attr_obs]\n",
    "\n",
    "    @classmethod\n",
    "    def get_path_model(cls, path, name=None):\n",
    "        \"\"\"get the path at which the model will be saved\"\"\"\n",
    "        return cls.nn_class.get_path_model(path, name=name)\n",
    "\n",
    "    def make_nn(self, training_param):\n",
    "        \"\"\"build the appropriate BaseDeepQ\"\"\"\n",
    "        res = self.nn_class(self, training_param)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_obs_size(env, list_attr_name):\n",
    "        \"\"\"get the size of the flatten observation\"\"\"\n",
    "        res = 0\n",
    "        for obs_attr_name in list_attr_name:\n",
    "            beg_, end_, dtype_ = env.observation_space.get_indx_extract(obs_attr_name)\n",
    "            res += end_ - beg_  # no \"+1\" needed because \"end_\" is exclude by python convention\n",
    "        return res\n",
    "\n",
    "    def get_obs_attr(self):\n",
    "        \"\"\"get the names of the observation attributes that will be extracted \"\"\"\n",
    "        return self.list_attr_obs\n",
    "\n",
    "    # utilitaries, do not change\n",
    "    def to_dict(self):\n",
    "        \"\"\"convert this instance to a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = {}\n",
    "        for attr_nm in self._int_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = int(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._float_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = float(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._str_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = str(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "\n",
    "        for attr_nm in self._list_float:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, float)\n",
    "        for attr_nm in self._list_int:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, int)\n",
    "        for attr_nm in self._list_str:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, str)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_list_to_json(cls, obj, type_):\n",
    "        if isinstance(obj, type_):\n",
    "            res = obj\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            if len(obj.shape) == 1:\n",
    "                res = [type_(el) for el in obj]\n",
    "            else:\n",
    "                res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        elif isinstance(obj, Iterable):\n",
    "            res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        else:\n",
    "            res = type_(obj)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _attr_from_json(cls, json, type_):\n",
    "        if isinstance(json, type_):\n",
    "            res = json\n",
    "        elif isinstance(json, list):\n",
    "            res = [cls._convert_list_to_json(obj=el, type_=type_) for el in json]\n",
    "        else:\n",
    "            res = type_(json)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, tmp):\n",
    "        \"\"\"load from a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam (more or less)\n",
    "        cls_as_dict = {}\n",
    "        for attr_nm in cls._int_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = int(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._float_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = float(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._str_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = str(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._list_float:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], float)\n",
    "        for attr_nm in cls._list_int:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], int)\n",
    "        for attr_nm in cls._list_str:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], str)\n",
    "\n",
    "        res = cls(**cls_as_dict)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path):\n",
    "        \"\"\"load from a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"No path are located at \\\"{}\\\"\".format(json_path))\n",
    "        with open(json_path, \"r\") as f:\n",
    "            dict_ = json.load(f)\n",
    "        return cls.from_dict(dict_)\n",
    "\n",
    "    def save_as_json(self, path, name=None):\n",
    "        \"\"\"save as a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = self.to_dict()\n",
    "        if name is None:\n",
    "            name = \"neural_net_parameters.json\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"Directory \\\"{}\\\" not found to save the NN parameters\".format(path))\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(\"\\\"{}\\\" should be a directory\".format(path))\n",
    "        path_out = os.path.join(path, name)\n",
    "        with open(path_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, fp=f, indent=4, sort_keys=True)\n",
    "\n",
    "    def center_reduce(self, env):\n",
    "        \"\"\"currently not implemented for this class, \"coming soon\" as we might say\"\"\"\n",
    "        # TODO see TestLeapNet for this feature\n",
    "        self._center_reduce_vect(env.get_obs(), \"x\")\n",
    "\n",
    "    def _get_adds_mults_from_name(self, obs, attr_nm):\n",
    "        if attr_nm in [\"prod_p\"]:\n",
    "            add_tmp = np.array([-0.5 * (pmax + pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "            mult_tmp = np.array([1. / max((pmax - pmin), 0.) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"prod_q\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / max(abs(val), 1.0) for val in obs.prod_q])\n",
    "        elif attr_nm in [\"load_p\", \"load_q\"]:\n",
    "            add_tmp = np.array([-val for val in getattr(obs, attr_nm)])\n",
    "            mult_tmp = 0.5\n",
    "        elif attr_nm in [\"load_v\", \"prod_v\", \"v_or\", \"v_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / val for val in getattr(obs, attr_nm)])\n",
    "        elif attr_nm == \"hour_of_day\":\n",
    "            add_tmp = -12.\n",
    "            mult_tmp = 1.0 / 12\n",
    "        elif attr_nm == \"minute_of_hour\":\n",
    "            add_tmp = -30.\n",
    "            mult_tmp = 1.0 / 30\n",
    "        elif attr_nm == \"day_of_week\":\n",
    "            add_tmp = -4.\n",
    "            mult_tmp = 1.0 / 4\n",
    "        elif attr_nm == \"day\":\n",
    "            add_tmp = -15.\n",
    "            mult_tmp = 1.0 / 15.\n",
    "        elif attr_nm in [\"target_dispatch\", \"actual_dispatch\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / (pmax - pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_or\", \"q_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1.0 / max(val, 1.0) for val in getattr(obs, attr_nm)])\n",
    "        else:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = 1.0\n",
    "        return add_tmp, mult_tmp\n",
    "\n",
    "    def _center_reduce_vect(self, obs, nn_part):\n",
    "        \"\"\"\n",
    "        compute the xxxx_adds and xxxx_mults for one part of the neural network called nn_part,\n",
    "        depending on what attribute of the observation is extracted\n",
    "        \"\"\"\n",
    "        if not isinstance(obs, grid2op.Observation.BaseObservation):\n",
    "            # in multi processing i receive a set of observation there so i might need\n",
    "            # to extract only the first one\n",
    "            obs = obs[0]\n",
    "\n",
    "        li_attr_obs = getattr(self, \"list_attr_obs_{}\".format(nn_part))\n",
    "        adds = []\n",
    "        mults = []\n",
    "        for attr_nm in li_attr_obs:\n",
    "            add_tmp, mult_tmp = self._get_adds_mults_from_name(obs, attr_nm)\n",
    "            mults.append(mult_tmp)\n",
    "            adds.append(add_tmp)\n",
    "        setattr(self, \"{}_adds\".format(nn_part), adds)\n",
    "        setattr(self, \"{}_mults\".format(nn_part), mults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6fc2",
   "metadata": {},
   "source": [
    "# DeepQ_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f77b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NN(BaseDeepQ):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "        BaseDeepQ.__init__(self,\n",
    "                           nn_params,\n",
    "                           training_param)\n",
    "        self.schedule_lr_model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "        This function will make 2 identical models, one will serve as a target model, the other one will be trained\n",
    "        regurlarly.\n",
    "        \"\"\"\n",
    "        self._model = Sequential()\n",
    "        input_layer = Input(shape=(self._nn_archi.observation_size,),\n",
    "                            name=\"state\")\n",
    "        lay = input_layer\n",
    "        for lay_num, (size, act) in enumerate(zip(self._nn_archi.sizes, self._nn_archi.activs)):\n",
    "            lay = Dense(size, name=\"layer_{}\".format(lay_num))(lay)  # put at self.action_size\n",
    "            lay = Activation(act)(lay)\n",
    "\n",
    "        output = Dense(self._action_size, name=\"output\")(lay)\n",
    "\n",
    "        self._model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._schedule_lr_model, self._optimizer_model = self.make_optimiser()\n",
    "        self._model.compile(loss='mse', optimizer=self._optimizer_model)\n",
    "\n",
    "        self._target_model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._target_model.set_weights(self._model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63347ebd",
   "metadata": {},
   "source": [
    "# DeepQ_NNParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ea4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NNParam(NNParam):\n",
    "    \n",
    "    _int_attr = copy.deepcopy(NNParam._int_attr)\n",
    "    _float_attr = copy.deepcopy(NNParam._float_attr)\n",
    "    _str_attr = copy.deepcopy(NNParam._str_attr)\n",
    "    _list_float = copy.deepcopy(NNParam._list_float)\n",
    "    _list_str = copy.deepcopy(NNParam._list_str)\n",
    "    _list_int = copy.deepcopy(NNParam._list_int)\n",
    "\n",
    "    nn_class = DeepQ_NN\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,  # TODO this might not be usefull\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs\n",
    "                 ):\n",
    "        NNParam.__init__(self,\n",
    "                         action_size,\n",
    "                         observation_size,  # TODO this might not be usefull\n",
    "                         sizes,\n",
    "                         activs,\n",
    "                         list_attr_obs\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a0c37",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb3ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LOGS_DIR = \"./logs-eval/deep-q-learning\"\n",
    "DEFAULT_SAVE_DIR = \"./train-result\"\n",
    "DEFAULT_NB_EPISODE = 1\n",
    "DEFAULT_NB_PROCESS = 1\n",
    "DEFAULT_MAX_STEPS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa4e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,\n",
    "          name=DEFAULT_NAME,\n",
    "          iterations=1,\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          logs_dir=None,\n",
    "          training_param=None,\n",
    "          filter_action_fun=None,\n",
    "          kwargs_converters={},\n",
    "          kwargs_archi={},\n",
    "          verbose=True):\n",
    "\n",
    "\n",
    "    # Limit gpu usage\n",
    "    try:\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if len(physical_devices) > 0:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except AttributeError:\n",
    "         # issue of https://stackoverflow.com/questions/59266150/attributeerror-module-tensorflow-core-api-v2-config-has-no-attribute-list-p\n",
    "        try:\n",
    "            physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "            if len(physical_devices) > 0:\n",
    "                tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        except Exception:\n",
    "            warnings.warn(_WARN_GPU_MEMORY)\n",
    "    except Exception:\n",
    "        warnings.warn(_WARN_GPU_MEMORY)\n",
    "\n",
    "    if training_param is None:\n",
    "        training_param = TrainingParam()\n",
    "\n",
    "    # compute the proper size for the converter\n",
    "    kwargs_archi[\"action_size\"] = DeepQSimple.get_action_size(env.action_space, filter_action_fun, kwargs_converters)\n",
    "\n",
    "    if load_path is not None:\n",
    "        path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, the architecture parameters will be ignored\")\n",
    "        nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "    else:\n",
    "        nn_archi = DeepQ_NNParam(**kwargs_archi)\n",
    "\n",
    "    baseline = DeepQSimple(action_space=env.action_space,\n",
    "                           nn_archi=nn_archi,\n",
    "                           name=name,\n",
    "                           istraining=True,\n",
    "                           verbose=verbose,\n",
    "                            **kwargs_converters\n",
    "                            )\n",
    "\n",
    "    if load_path is not None:\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, training parameters will be ignored\")\n",
    "        baseline.load(load_path)\n",
    "        training_param = baseline._training_param\n",
    "\n",
    "    baseline.train(env,\n",
    "                   iterations,\n",
    "                   save_path=save_path,\n",
    "                   logdir=logs_dir,\n",
    "                   training_param=training_param)\n",
    "    # as in our example (and in our explanation) we recommend to save the mode regurlarly in the \"train\" function\n",
    "    # it is not necessary to save it again here. But if you chose not to follow these advice, it is more than\n",
    "    # recommended to save the \"baseline\" at the end of this function with:\n",
    "    # baseline.save(path_save)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab4953",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a08c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env,\n",
    "             name=DEFAULT_NAME,\n",
    "             load_path=None,\n",
    "             logs_path=DEFAULT_LOGS_DIR,\n",
    "             nb_episode=DEFAULT_NB_EPISODE,\n",
    "             nb_process=DEFAULT_NB_PROCESS,\n",
    "             max_steps=DEFAULT_MAX_STEPS,\n",
    "             verbose=False,\n",
    "             save_gif=False):\n",
    "\n",
    "    # Limit gpu usage\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    runner_params = env.get_params_for_runner()\n",
    "    runner_params[\"verbose\"] = verbose\n",
    "\n",
    "    if load_path is None:\n",
    "        raise RuntimeError(\"Cannot evaluate a model if there is nothing to be loaded.\")\n",
    "    path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "    nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "\n",
    "    # Run\n",
    "    # Create agent\n",
    "    agent = DeepQSimple(action_space=env.action_space,\n",
    "                        name=name,\n",
    "                        store_action=nb_process == 1,\n",
    "                        nn_archi=nn_archi,\n",
    "                        observation_space=env.observation_space)\n",
    "\n",
    "    # Load weights from file\n",
    "    agent.load(load_path)\n",
    "\n",
    "    # Build runner\n",
    "    runner = Runner(**runner_params,\n",
    "                    agentClass=None,\n",
    "                    agentInstance=agent)\n",
    "\n",
    "    # Print model summary\n",
    "    stringlist = []\n",
    "    agent.deep_q._model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    if verbose:\n",
    "        print(short_model_summary)\n",
    "\n",
    "    # Run\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "    res = runner.run(path_save=logs_path,\n",
    "                     nb_episode=nb_episode,\n",
    "                     nb_process=nb_process,\n",
    "                     max_iter=max_steps,\n",
    "                     pbar=verbose)\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"Evaluation summary:\")\n",
    "        for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "            msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "            msg_tmp += \"\\ttotal score: {:.6f}\".format(cum_reward)\n",
    "            msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step, max_ts)\n",
    "            print(msg_tmp)\n",
    "\n",
    "        if len(agent.dict_action):\n",
    "            # I output some of the actions played\n",
    "            print(\"The agent played {} different action\".format(len(agent.dict_action)))\n",
    "            for id_, (nb, act, types) in agent.dict_action.items():\n",
    "                print(\"Action with ID {} was played {} times\".format(id_, nb))\n",
    "                print(\"{}\".format(act))\n",
    "                print(\"-----------\")\n",
    "\n",
    "    if save_gif:\n",
    "        if verbose:\n",
    "            print(\"Saving the gif of the episodes\")\n",
    "        save_log_gif(logs_path, res)\n",
    "\n",
    "    return agent, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e914526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)\n",
    "gym_env = GymEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bda7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_attr_obs_X = [\"gen_p\", \"gen_v\", \"load_p\", \"load_q\"]\n",
    "observation_size = NNParam.get_obs_size(env, li_attr_obs_X)\n",
    "sizes = [300, 300, 300]\n",
    "activs =  [\"relu\" for _ in sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5cec260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 10:20:49.724731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/4v/jh4jpq995m984n_88mvkztj00000gn/T/ipykernel_11304/1520152098.py:86: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._actions_per_ksteps = np.zeros((self._vector_size, self.action_space.size()), dtype=np.int)\n",
      "/var/folders/4v/jh4jpq995m984n_88mvkztj00000gn/T/ipykernel_11304/1520152098.py:632: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  done = np.full(self.__nb_env, fill_value=False, dtype=np.bool)\n",
      "  0%|                                                                                                                                                       | 0/1 [00:00<?, ?it/s]/var/folders/4v/jh4jpq995m984n_88mvkztj00000gn/T/ipykernel_11304/1520152098.py:632: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  done = np.full(self.__nb_env, fill_value=False, dtype=np.bool)\n",
      "/var/folders/4v/jh4jpq995m984n_88mvkztj00000gn/T/ipykernel_11304/1520152098.py:294: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  temp_done = np.array([temp_done], dtype=np.bool)\n",
      "  0%|                                                                                                                                                       | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_action_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs_archi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobservation_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msizes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist_attr_obs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mli_attr_obs_X\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, name, iterations, save_path, load_path, logs_dir, training_param, filter_action_fun, kwargs_converters, kwargs_archi, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m     baseline\u001b[38;5;241m.\u001b[39mload(load_path)\n\u001b[1;32m     56\u001b[0m     training_param \u001b[38;5;241m=\u001b[39m baseline\u001b[38;5;241m.\u001b[39m_training_param\n\u001b[0;32m---> 58\u001b[0m \u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m               \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m               \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtraining_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# as in our example (and in our explanation) we recommend to save the mode regurlarly in the \"train\" function\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# it is not necessary to save it again here. But if you chose not to follow these advice, it is more than\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# recommended to save the \"baseline\" at the end of this function with:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# baseline.save(path_save)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m baseline\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mDeepQAgent.train\u001b[0;34m(self, env, iterations, save_path, logdir, training_param)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# save some information to tensorboard\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m alive_frames[epoch_num] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(alive_frame)\n\u001b[1;32m    316\u001b[0m total_rewards[epoch_num] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(total_reward)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_action_played_train(training_step, pm_i)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "res = train(\n",
    "    env,\n",
    "    name=DEFAULT_NAME,\n",
    "    iterations=1,\n",
    "    save_path=None,\n",
    "    load_path=None,\n",
    "    logs_dir=None,\n",
    "    training_param=None,\n",
    "    filter_action_fun=None,\n",
    "    kwargs_converters={},\n",
    "    kwargs_archi={\n",
    "        'observation_size': observation_size,\n",
    "        'sizes': sizes,\n",
    "        'activs': activs,\n",
    "        \"list_attr_obs\": li_attr_obs_X\n",
    "    },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be990cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51904682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
