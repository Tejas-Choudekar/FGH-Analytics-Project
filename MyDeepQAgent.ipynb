{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97910269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import grid2op\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Converter import IdToAct\n",
    "from grid2op.Agent.agentWithConverter import AgentWithConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85e7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Dense, subtract, add\n",
    "from tensorflow.keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65dc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17db5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParam(object):\n",
    "    \"\"\"\n",
    "    A class to store the training parameters of the models. It was hard coded in the notebook 3.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 DECAY_RATE=0.9,\n",
    "                 BUFFER_SIZE=40000,\n",
    "                 MINIBATCH_SIZE=64,\n",
    "                 TOT_FRAME=3000000,\n",
    "                 EPSILON_DECAY=10000,\n",
    "                 MIN_OBSERVATION=50, #5000\n",
    "                 FINAL_EPSILON=1/300,  # have on average 1 random action per scenario of approx 287 time steps\n",
    "                 INITIAL_EPSILON=0.1,\n",
    "                 TAU=0.01,\n",
    "                 ALPHA=1,\n",
    "                 NUM_FRAMES=1,\n",
    "    ):\n",
    "        print('TrainingParam __init__')\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.MINIBATCH_SIZE = MINIBATCH_SIZE\n",
    "        self.TOT_FRAME = TOT_FRAME\n",
    "        self.EPSILON_DECAY = EPSILON_DECAY\n",
    "        self.MIN_OBSERVATION = MIN_OBSERVATION   # 5000\n",
    "        self.FINAL_EPSILON = FINAL_EPSILON  # have on average 1 random action per scenario of approx 287 time steps\n",
    "        self.INITIAL_EPSILON = INITIAL_EPSILON\n",
    "        self.TAU = TAU\n",
    "        self.NUM_FRAMES = NUM_FRAMES\n",
    "        self.ALPHA = ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fbb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "\n",
    "    '''\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        print('ReplayBuffer add')\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        if np.any(~np.isfinite(s)) or np.any(~np.isfinite(s2)):\n",
    "            # TODO proper handling of infinite values somewhere !!!!\n",
    "            return\n",
    "\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        print('ReplayBuffer size')\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        print('ReplayBuffer sample')\n",
    "\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        print('ReplayBuffer clear')\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcdbba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class RLQvalue(object):\n",
    "    \"\"\"\n",
    "    This class aims at representing the Q value (or more in case of SAC) parametrization by\n",
    "    a neural network.\n",
    "\n",
    "    It is composed of 2 different networks:\n",
    "    - model: which is the main model\n",
    "    - target_model: which has the same architecture and same initial weights as \"model\" but is updated less frequently\n",
    "      to stabilize training\n",
    "\n",
    "    It has basic methods to make predictions, to train the model, and train the target model.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=1e-5,\n",
    "                 training_param=TrainingParam()):\n",
    "        # TODO add more flexibilities when building the deep Q networks, with a \"NNParam\" for example.\n",
    "        self.action_size = action_size\n",
    "        self.observation_size = observation_size\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.qvalue_evolution = np.zeros((0,))\n",
    "        self.training_param = training_param\n",
    "\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "    \n",
    "    '''\n",
    "    def construct_q_network(self):\n",
    "        print('RLQvalue construct_q_network')\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "    '''\n",
    "\n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        rand_val = np.random.random(data.shape[0])\n",
    "        print(f'>> rand_val = {rand_val}')\n",
    "        q_actions = self.model.predict(data)\n",
    "        print(f'>> q_actions = {q_actions}')\n",
    "        opt_policy = np.argmax(np.abs(q_actions), axis=-1)\n",
    "        print(f'>> argmax = {opt_policy}')\n",
    "        opt_policy[rand_val < epsilon] = np.random.randint(0, self.action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        \n",
    "        self.qvalue_evolution = np.concatenate((self.qvalue_evolution, q_actions[0, opt_policy]))\n",
    "        print(f'>> qvalue_evolution = {self.qvalue_evolution}')\n",
    "        #print(f'>> opt_policy = {opt_policy}')\n",
    "        #print(f'>> q_actions = {q_actions[0, opt_policy]}')\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "    \n",
    "    '''\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        print('RLQvalue train')\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        targets = self.model.predict(s_batch)\n",
    "        fut_action = self.target_model.predict(s2_batch)\n",
    "        targets[:, a_batch] = r_batch\n",
    "        targets[d_batch, a_batch[d_batch]] += self.training_param.DECAY_RATE * np.max(fut_action[d_batch], axis=-1)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "        return np.all(np.isfinite(loss))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_path_model(path, name=None):\n",
    "        print('RLQvalue _get_path_model')\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue save_network')\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model.save('{}.{}'.format(path_model, ext))\n",
    "        self.target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    \n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue load_network')\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model = load_model('{}.{}'.format(path_model, ext))\n",
    "        self.target_model = load_model('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        print('RLQvalue target_train')\n",
    "        # nothing has changed from the original implementation\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = self.training_param.TAU * model_weights[i] + (1 - self.training_param.TAU) * \\\n",
    "                                      target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0abbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class DuelQ(RLQvalue):\n",
    "    \"\"\"Constructs the desired duelling deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=0.00001,\n",
    "                 training_param=TrainingParam()):\n",
    "        ## print('DuelQ __init__')\n",
    "        RLQvalue.__init__(self, action_size, observation_size, learning_rate, training_param)\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        # The inputs and outputs size have changed, as well as replacing the convolution by dense layers.\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape=(self.observation_size*self.training_param.NUM_FRAMES,))\n",
    "        \n",
    "        lay1 = Dense(self.observation_size*self.training_param.NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(self.observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*self.action_size)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(self.action_size)(lay3)\n",
    "        advantage = Dense(self.action_size)(fc1)\n",
    "        fc2 = Dense(self.action_size)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1) )\n",
    "        mn_ = meaner(advantage)\n",
    "        tmp = subtract([advantage, mn_])\n",
    "        policy = add([tmp, value])\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "        print(\"Successfully constructed networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bad922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class MyDeepQAgent(AgentWithConverter):\n",
    "    \n",
    "    ## 1*0^-5 = 0.00001\n",
    "    def __init__(self, action_space, mode=\"DDQN\", learning_rate=1e-5, training_param=TrainingParam()):     \n",
    "        \n",
    "        ## Handle only vectors, and the type of action_space is GridObjects\n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(training_param.BUFFER_SIZE)\n",
    "\n",
    "        # compare to original implementation, i don't know the observation space size.\n",
    "        # Because it depends on the component of the observation we want to look at. So these neural network will\n",
    "        # be initialized the first time an observation is observe.\n",
    "        self.deep_q = None\n",
    "        self.mode = mode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_param = training_param\n",
    "    \n",
    "    def convert_obs(self, observation):\n",
    "        ## print(f'>> convert_obs = {np.concatenate((observation.rho, observation.line_status, observation.topo_vect))}')\n",
    "        return np.concatenate((observation.rho, observation.line_status, observation.topo_vect))\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        ## print(f'>> transformed_observation = {transformed_observation}')\n",
    "        if self.deep_q is None:\n",
    "            self.init_deep_q(transformed_observation)\n",
    "        \n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation.reshape(1, -1), epsilon=0.0)\n",
    "        print(f'>> predict_movement_int = {predict_movement_int}')\n",
    "        print(*_)\n",
    "        return int(predict_movement_int)\n",
    "\n",
    "    def init_deep_q(self, transformed_observation):\n",
    "        if self.deep_q is None:\n",
    "            # the first time an observation is observed, I set up the neural network with the proper dimensions.\n",
    "            if self.mode == \"DQN\":\n",
    "                cls = DeepQ\n",
    "            elif self.mode == \"DDQN\":\n",
    "                cls = DuelQ\n",
    "            elif self.mode == \"SAC\":\n",
    "                cls = SAC\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown neural network named \\\"{}\\\". Supported types are \\\"DQN\\\", \\\"DDQN\\\" and \"\n",
    "                                   \"\\\"SAC\\\"\".format(self.mode))\n",
    "            self.deep_q = cls(self.action_space.size(), observation_size=transformed_observation.shape[-1], learning_rate=self.learning_rate)\n",
    "            print(f'>> action_size = {self.deep_q.action_size}, observation_size = {self.deep_q.observation_size}, learning_rate_ = {self.deep_q.learning_rate_}, qvalue_evolution = {self.deep_q.qvalue_evolution}, training_param = {self.deep_q.training_param}, model = {self.deep_q.model}, target_model = {self.deep_q.target_model}')\n",
    "            \n",
    "    '''\n",
    "    def load_network(self, path):\n",
    "        print('MyDeepQAgent load_network')\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473fe712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully constructed networks.\n",
      ">> action_size = 451, observation_size = 96, learning_rate_ = 1e-05, qvalue_evolution = [], training_param = <__main__.TrainingParam object at 0x7feec9ae79d0>, model = <keras.engine.functional.Functional object at 0x7feec9a7fe80>, target_model = <keras.engine.functional.Functional object at 0x7feec963b610>\n",
      ">> rand_val = [0.50701507]\n",
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 21:00:56.082991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> q_actions = [[ 1.0334537   0.53934324  0.23217481  0.425765    0.97880244  0.40077382\n",
      "   0.5049721   0.39055574  0.38654286  0.41548347  0.85216475  0.59191334\n",
      "   0.42569402  0.3437204   0.44283527  0.35330802  0.5534356   0.4845095\n",
      "   0.22676344  0.6526748   0.59936655  0.4174549   0.42644998  0.37036374\n",
      "   0.57697076  0.4471568   0.8464261   0.45388442  0.67742354 -0.06016797\n",
      "   0.41905218  0.18587962  0.2699697   0.32057947  0.31662163  0.39133847\n",
      "   0.5227047   0.4634252   0.50978214  0.18868291  0.44343215  0.54863477\n",
      "   0.7011525   0.6938271   0.5504229   0.10351646  0.11946505  0.3524045\n",
      "   0.61729     0.88002706  0.8204594   0.0349828   0.5714311   0.47080973\n",
      "   0.5145987   0.32574853  0.36516696  0.5218014   0.9612078   0.4437476\n",
      "   0.3981024   0.5619908   0.23989211  0.3719298   0.37932605  0.3337577\n",
      "   0.6225888   0.26147625  0.65464616  0.57242984  0.7092445   0.9077025\n",
      "   0.2774457   0.28036773  0.37359765  0.43656918  0.2888719   0.41803193\n",
      "   0.39361766  0.50345767  0.19930923  0.34569255  0.46515682  0.20826596\n",
      "   0.72082186  0.5078966   0.45872697  0.93371475  0.5100976   0.45380268\n",
      "   0.5405756   0.74057853  0.34611896  0.7476021   0.4078436   0.63661647\n",
      "   0.5823853   0.49112326  0.6275907   0.24340229  0.17655972  0.48944062\n",
      "   0.5275506   0.3762567   0.67247283  0.3662406   0.41241565  0.74161196\n",
      "   0.47043267  0.7458903   0.2676564   0.3974526   0.37823647  0.8472574\n",
      "   0.24361138  0.08448485  0.67471665  0.41808918  0.20520261  0.41035464\n",
      "   0.2330097   0.23305558  0.49773192  0.3747325   0.4669473   0.6741049\n",
      "   0.7859293   0.3836239   0.51793444  0.70184547  0.750921    0.21471336\n",
      "   0.74340767  0.30237442  0.51193535 -0.02898446  0.56372917  0.45906577\n",
      "   0.8909235   0.38774127  0.6517717   0.6851614   0.76655877  0.20916232\n",
      "   0.5724238   0.5647465   0.41138896  0.7008982   0.41537064  0.2977494\n",
      "   0.09480736  0.25369984  0.7161828   0.5545248   0.62097836  0.4013271\n",
      "   0.16540065  0.69488317  0.12239239  0.19390246  0.27967393  0.06190601\n",
      "   0.5014882   0.44994444  0.4780113   0.74324614  0.59694386  0.7628722\n",
      "   0.61940134  0.07897079  0.5563838   0.6617398   0.34040496  0.19830653\n",
      "   0.61258405  0.33854982  0.3504553   0.471687    0.31575322  0.29888168\n",
      "   0.49783206  0.4316804   0.5377573   0.32704484  0.34940547  0.69876456\n",
      "   0.48050833  0.29895487  0.61108935  0.3996373   0.12265399  0.41169205\n",
      "   0.38151026  0.49561554  0.12167826  0.7313925   0.33543834  0.40099883\n",
      "   0.5645712   0.7618545   0.44594726  0.4904705   0.07705507  0.77357596\n",
      "   0.709944    0.41708338  0.81715274  0.20167476  0.456763    0.34479854\n",
      "   0.11863509  0.28692698  0.9598931   0.62423307  0.12462762  0.53155196\n",
      "   0.3826128   0.65451527  0.20368558  0.839975    0.7178807   0.6088268\n",
      "   0.17185545  0.7089091   0.74866533  0.5722133   0.25961536 -0.00457069\n",
      "   0.32772386  0.5266725   0.8808347   0.50114155  0.43042168  0.58232814\n",
      "   0.44976813  0.4588196   0.46040136  0.4471695   0.5869267   0.3361566\n",
      "   0.43339068  0.33095568  0.3103486   0.56619436  0.27263874  0.09679884\n",
      "   0.26656437  0.70080656  0.4110141   0.21191755  0.10936162  0.13273415\n",
      "   0.6532676   0.13700569  0.33413088  0.59278804  0.6977803   0.5598485\n",
      "   0.18647209  0.83258617  0.08046666  0.25528038  0.32246354  0.24843293\n",
      "   0.23081337  0.43563715  0.4431295   0.26744205  0.45842305  0.48055285\n",
      "   0.7784574   0.60485643  0.62038994  0.76078105  0.07310939  0.19911376\n",
      "   0.8376764   0.6388206   0.59372807  0.537597    0.5947976   0.5512724\n",
      "   0.34575963  0.64940345  0.692583    0.23785232  0.19843605  0.47793838\n",
      "   0.449834    0.69320536  0.19736859  0.5475864   0.7894552   0.27205312\n",
      "   0.38184437  0.6201942   0.7219197   0.66985685  0.40774363  0.6659795\n",
      "   0.76637995  0.27290857  0.37639037  0.48792052  0.12182519  0.45045924\n",
      "   0.20644942  0.03806859  0.31376782  0.55083907  0.58029556  1.0248954\n",
      "   0.638084    0.62279814  0.39403152  0.41671798  0.19389862  0.4995884\n",
      "   0.3190597   0.39002928  0.6624851   0.61073816  0.5841978   0.18265471\n",
      "   0.8493526   0.73459285  0.6308081   0.53101146  0.22862285  0.507053\n",
      "   0.41172713  0.46780813  0.4873749   0.6067226   0.37918425  0.21326521\n",
      "   0.74797773  0.2573459   0.73536694  0.52057993  0.8899926   0.65605485\n",
      "   0.6443323   0.44020638  0.68534905  0.27925706  0.16310436  0.16067764\n",
      "   0.53254116  0.27881593  0.48068938  0.11491585  0.37799674  0.2547243\n",
      "  -0.11490917  0.44080946  0.65478134  0.61513674  0.3295043   0.4772266\n",
      "   0.17542136  0.65703595  0.41959167  0.29173735  0.4782463   0.3827314\n",
      "   0.3773313   0.16669548  0.5522075   0.39828107  0.6381266   0.247136\n",
      "   0.10313776  0.3885233   0.38334018  0.47841543  0.296684    0.39568996\n",
      "   0.04487494  0.36768726  0.48662612  0.1190584   0.27663964  0.4563209\n",
      "   0.14553836  0.6323333   0.6272292   0.73944217 -0.04955798  0.862254\n",
      "   0.39620528  0.16843599  0.4036476   0.549109    0.6713807   0.5381069\n",
      "   0.2586516   0.43727148  0.21662542  0.39825857  0.38630795  0.74780977\n",
      "   0.5402427   0.5225883   0.885203    0.5097576   0.47134104  0.6279136\n",
      "   0.4157684   0.15622526  0.50059885  0.5534926   0.46576512  0.72328234\n",
      "   0.71861804  0.67578554  0.17075118  0.8806283   0.37276286  0.50546986\n",
      "   0.2181077   0.23256332  0.69552016  0.5479702   0.29673046  0.47955588\n",
      "   0.36034867  0.58993053  0.43445042  0.38840744  0.04948232  0.46138647\n",
      "   0.57440746  0.38236162  0.05841076  0.4212098   0.70054895  0.36542875\n",
      "   0.31785077  0.669023    0.25415105  0.19919905  0.34924376  0.568769\n",
      "   0.528903    0.5257759   0.36012062  0.66429985  0.54910696  0.86991405\n",
      "   0.30047876]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0334537]\n",
      ">> rand_val = [0.56756575]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      ">> q_actions = [[ 1.033253    0.54019326  0.23400658  0.42598537  0.98003393  0.40030557\n",
      "   0.50521874  0.3913275   0.38772315  0.41614303  0.85312164  0.59327954\n",
      "   0.42524937  0.3448274   0.44374463  0.35501269  0.5533183   0.4853873\n",
      "   0.22807078  0.6529459   0.5990325   0.41835907  0.42634046  0.3700593\n",
      "   0.57695836  0.44715172  0.8458922   0.45511818  0.6781987  -0.05911964\n",
      "   0.42002657  0.18603128  0.27063298  0.32126528  0.3176071   0.39189833\n",
      "   0.52426153  0.46286762  0.51006633  0.18975475  0.44365048  0.54922616\n",
      "   0.70148766  0.6934749   0.54997975  0.10258707  0.12000898  0.35190222\n",
      "   0.6172594   0.8802657   0.8199526   0.03482589  0.57237333  0.4720453\n",
      "   0.51527035  0.32699883  0.36576387  0.52254707  0.9600791   0.44282368\n",
      "   0.39720482  0.5614381   0.24070701  0.37289363  0.3790292   0.33314586\n",
      "   0.6224828   0.26160076  0.6543752   0.57306653  0.7093123   0.9070819\n",
      "   0.27784747  0.28191775  0.37214413  0.43774664  0.28890294  0.4188083\n",
      "   0.3952396   0.5036505   0.20059085  0.34593964  0.46513286  0.20868354\n",
      "   0.7209593   0.50939333  0.45933464  0.9337287   0.51059556  0.45498902\n",
      "   0.54152215  0.7392895   0.34632927  0.74762905  0.4072876   0.63516986\n",
      "   0.5826378   0.49056306  0.6279461   0.24455777  0.17705831  0.49076286\n",
      "   0.52790016  0.37711978  0.6725639   0.36761436  0.41247925  0.7413808\n",
      "   0.47110474  0.7458436   0.26788092  0.39826882  0.37845537  0.8470607\n",
      "   0.24334443  0.08559895  0.67384064  0.4190268   0.20580518  0.41062763\n",
      "   0.23236132  0.23336026  0.4979215   0.37515897  0.4664175   0.67430186\n",
      "   0.7862454   0.38429683  0.5183455   0.70242953  0.751335    0.2158794\n",
      "   0.74334735  0.30268055  0.5121477  -0.02767405  0.56324255  0.45947686\n",
      "   0.8920318   0.3882238   0.6524553   0.68525314  0.76832235  0.21023886\n",
      "   0.5726798   0.5656895   0.41151494  0.7006173   0.41587514  0.29811412\n",
      "   0.09513193  0.25409168  0.71747243  0.5552986   0.6200511   0.40150434\n",
      "   0.16611281  0.69547224  0.1235593   0.19517389  0.28004187  0.06155959\n",
      "   0.50192755  0.4500927   0.4778673   0.7438338   0.5972395   0.7632981\n",
      "   0.6200519   0.07932484  0.5563563   0.6605543   0.340425    0.20001674\n",
      "   0.6123567   0.33882132  0.35084844  0.4723281   0.3159242   0.2998572\n",
      "   0.4973482   0.43172434  0.5382338   0.3272003   0.35167465  0.69985497\n",
      "   0.4805443   0.29983824  0.61091983  0.3993501   0.12316009  0.41172364\n",
      "   0.38129464  0.4957422   0.1214487   0.7312909   0.33601534  0.40096205\n",
      "   0.5651125   0.7627145   0.44641283  0.48978016  0.07746813  0.77370137\n",
      "   0.7097368   0.417315    0.8162409   0.20230272  0.4578992   0.3452096\n",
      "   0.11934918  0.2885568   0.96029603  0.62392795  0.1260958   0.5318151\n",
      "   0.3834868   0.6537697   0.2038362   0.8399141   0.71933246  0.61029875\n",
      "   0.17289308  0.70814925  0.7473251   0.5738041   0.25922617 -0.00411916\n",
      "   0.32828155  0.52640384  0.880233    0.50095373  0.43090826  0.58257616\n",
      "   0.44979686  0.45786953  0.4597028   0.44781643  0.58696604  0.3359025\n",
      "   0.43364748  0.3308281   0.3116964   0.56488633  0.27399427  0.09674135\n",
      "   0.2670068   0.7005569   0.4112388   0.21122538  0.10960299  0.13187543\n",
      "   0.65282786  0.1372323   0.33503598  0.5933399   0.6979172   0.5596032\n",
      "   0.18693328  0.8320392   0.08193266  0.25503722  0.3228222   0.24903417\n",
      "   0.23261268  0.43625376  0.44225013  0.26752758  0.45740867  0.4804803\n",
      "   0.7786871   0.6040225   0.61998785  0.76106715  0.07366976  0.19937682\n",
      "   0.8369088   0.6385732   0.5940559   0.53748393  0.59388804  0.55176085\n",
      "   0.346067    0.64863664  0.6928997   0.23891424  0.19837102  0.47926325\n",
      "   0.4503892   0.6927119   0.19802484  0.54920405  0.7894106   0.27316138\n",
      "   0.38126743  0.61997664  0.7213832   0.6704506   0.40908054  0.6658714\n",
      "   0.767545    0.27444315  0.37579143  0.48829743  0.12217614  0.45182884\n",
      "   0.20809425  0.03956866  0.31475693  0.5510712   0.58116907  1.0240693\n",
      "   0.63812757  0.6234488   0.39439806  0.41721907  0.19401434  0.5002199\n",
      "   0.31904453  0.38987276  0.6630068   0.6103401   0.58446467  0.18296531\n",
      "   0.8482307   0.7330605   0.6302575   0.53125346  0.22862817  0.50700206\n",
      "   0.4127842   0.46746084  0.4880654   0.6058899   0.37800997  0.21342318\n",
      "   0.7477877   0.2588396   0.73538727  0.5212289   0.8907243   0.6549095\n",
      "   0.64450496  0.439884    0.68576455  0.27939826  0.16456568  0.16198996\n",
      "   0.5321846   0.27881247  0.48093635  0.11620206  0.37836215  0.25488114\n",
      "  -0.11427963  0.4403082   0.65487444  0.6149019   0.33011574  0.47819892\n",
      "   0.17567962  0.6576674   0.4196627   0.29284585  0.47840747  0.384426\n",
      "   0.37834755  0.16735515  0.55278414  0.39893454  0.6377977   0.24702853\n",
      "   0.10323679  0.38852537  0.38364604  0.4785706   0.2967107   0.39515606\n",
      "   0.04538628  0.3687653   0.48663518  0.11960712  0.27813765  0.45548764\n",
      "   0.14633656  0.63302904  0.6265297   0.7399     -0.04734993  0.86165\n",
      "   0.39560878  0.16843241  0.40511364  0.5483029   0.67146426  0.5374872\n",
      "   0.2585606   0.43762258  0.21604069  0.39874306  0.38689774  0.74938506\n",
      "   0.5402743   0.5222257   0.88589406  0.5105122   0.47171932  0.6288816\n",
      "   0.4161741   0.15658227  0.5003349   0.55315405  0.465747    0.7234001\n",
      "   0.71841025  0.6754124   0.17019826  0.8803729   0.3724442   0.50614804\n",
      "   0.21879886  0.2341768   0.6953104   0.54755956  0.29791605  0.47932208\n",
      "   0.36071974  0.59042674  0.4337743   0.38837206  0.05129156  0.46227404\n",
      "   0.57458305  0.38206768  0.0589847   0.42247486  0.7010766   0.3652051\n",
      "   0.31894535  0.6696448   0.2550937   0.20068428  0.3495503   0.5689522\n",
      "   0.5291083   0.5262907   0.36033922  0.66433275  0.5487224   0.86903167\n",
      "   0.30117384]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295]\n",
      ">> predict_movement_int = [0]\n",
      "[1.033253]\n",
      ">> rand_val = [0.89002113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> q_actions = [[ 1.0343856   0.53983784  0.23283307  0.42580104  0.979175    0.40030083\n",
      "   0.5050656   0.39094937  0.38677245  0.415687    0.85322964  0.5925084\n",
      "   0.42607942  0.34429392  0.44271842  0.354137    0.5538438   0.48477563\n",
      "   0.2278692   0.65265226  0.5993315   0.41772884  0.42665398  0.37020487\n",
      "   0.5773311   0.4477593   0.84639776  0.45454326  0.6785457  -0.0595355\n",
      "   0.41991535  0.18668953  0.27021378  0.3219762   0.3168522   0.39210573\n",
      "   0.5233918   0.46388173  0.5107117   0.18920058  0.4436778   0.54913425\n",
      "   0.70145744  0.6946087   0.55034524  0.10379753  0.12006745  0.35290405\n",
      "   0.6169507   0.88034856  0.820996    0.03494757  0.57216525  0.47149512\n",
      "   0.5148511   0.3264911   0.36550397  0.522081    0.96127     0.4441097\n",
      "   0.39831454  0.56111276  0.2407978   0.3727237   0.38005725  0.33365577\n",
      "   0.6228098   0.26213217  0.6547288   0.5731628   0.70984566  0.9076718\n",
      "   0.27838504  0.28079796  0.37326634  0.43707103  0.28915     0.4183536\n",
      "   0.39374244  0.5040873   0.20023766  0.3464787   0.46563685  0.2085524\n",
      "   0.72067595  0.5081057   0.45842582  0.93434346  0.51110935  0.45456555\n",
      "   0.5412398   0.74113244  0.34634835  0.7477757   0.4079764   0.63701016\n",
      "   0.5830453   0.4903427   0.6284841   0.24356091  0.17769077  0.48993897\n",
      "   0.52886677  0.37758505  0.67325824  0.36687472  0.4129505   0.74180865\n",
      "   0.47069985  0.7461091   0.26827824  0.39839953  0.37916332  0.84755313\n",
      "   0.2442284   0.08549279  0.67452186  0.41816628  0.20546123  0.41090536\n",
      "   0.2326368   0.23350142  0.4989431   0.3750096   0.4668352   0.6744858\n",
      "   0.7861675   0.3848893   0.51834476  0.70184773  0.75120187  0.21471347\n",
      "   0.7431352   0.30220816  0.5122856  -0.02858183  0.56379974  0.4599868\n",
      "   0.8920522   0.38797122  0.6524596   0.6859113   0.7667898   0.20921071\n",
      "   0.5726671   0.5653411   0.41185087  0.7008541   0.41532084  0.29851145\n",
      "   0.09556514  0.25427416  0.71682084  0.5554888   0.6213982   0.4020512\n",
      "   0.16596743  0.6948488   0.12313706  0.19475886  0.2804076   0.06257311\n",
      "   0.50217205  0.44986492  0.47825238  0.74381566  0.5966446   0.7640396\n",
      "   0.6202989   0.07967979  0.5571104   0.66191494  0.34089464  0.19962165\n",
      "   0.61321557  0.33870065  0.351028    0.4719178   0.31644255  0.29988855\n",
      "   0.49861348  0.43242833  0.5381541   0.3280534   0.34992233  0.6989722\n",
      "   0.48089027  0.2992697   0.6116643   0.4000062   0.12298444  0.41245154\n",
      "   0.38180798  0.49572903  0.12183437  0.7313353   0.33525634  0.40156373\n",
      "   0.56519526  0.7624098   0.44568866  0.49023423  0.0771845   0.773998\n",
      "   0.7093129   0.41673166  0.81720865  0.20241123  0.4569281   0.34483236\n",
      "   0.11946315  0.28781244  0.96019685  0.62441224  0.12505719  0.5319551\n",
      "   0.3831036   0.6548393   0.20415601  0.840017    0.7189257   0.6097555\n",
      "   0.17279664  0.7091565   0.74860424  0.5731398   0.26003012 -0.00396356\n",
      "   0.32798648  0.5272347   0.88109183  0.5008929   0.430869    0.5827622\n",
      "   0.45024833  0.45904937  0.45978138  0.4474405   0.5872859   0.33637786\n",
      "   0.4339451   0.33164525  0.311006    0.5669439   0.27335656  0.09729931\n",
      "   0.26685178  0.70102364  0.41139096  0.21224727  0.11000028  0.13288835\n",
      "   0.65359706  0.13730595  0.33507305  0.5932512   0.6987983   0.55985934\n",
      "   0.18656954  0.8327944   0.08197927  0.25487894  0.32245272  0.2483666\n",
      "   0.23108958  0.43591157  0.44350666  0.26740795  0.45858124  0.48128653\n",
      "   0.7784328   0.60536623  0.62037015  0.76117265  0.0741109   0.19945815\n",
      "   0.83833444  0.6384826   0.59437966  0.53798     0.5945287   0.55174744\n",
      "   0.34654605  0.64987504  0.69287133  0.23828171  0.19848263  0.47824067\n",
      "   0.45054552  0.69316727  0.19814637  0.5493183   0.7899815   0.2730837\n",
      "   0.3816359   0.62023926  0.7218091   0.6704196   0.40776688  0.66595054\n",
      "   0.76723886  0.27381635  0.37655783  0.48787114  0.12246445  0.45125204\n",
      "   0.20776358  0.03904292  0.3139367   0.55165386  0.5804989   1.0247314\n",
      "   0.6376053   0.6232396   0.39495546  0.41701376  0.19420731  0.5003629\n",
      "   0.31951398  0.39063406  0.66269624  0.611223    0.5853766   0.18322098\n",
      "   0.8502464   0.7347095   0.6307081   0.5317731   0.22843447  0.5076274\n",
      "   0.4120211   0.468243    0.48839018  0.6065106   0.3790307   0.21391596\n",
      "   0.7482805   0.25787896  0.73645926  0.520703    0.89049196  0.656505\n",
      "   0.6440356   0.44062045  0.6854565   0.2790386   0.16344729  0.16113117\n",
      "   0.5319156   0.27900675  0.48079443  0.11581662  0.37833062  0.25488576\n",
      "  -0.11389112  0.44105983  0.6554528   0.6154897   0.32995093  0.47817117\n",
      "   0.17553183  0.6576074   0.41988364  0.2926808   0.47846574  0.38438904\n",
      "   0.37806457  0.16698983  0.55263937  0.39880088  0.6383829   0.2474065\n",
      "   0.10339108  0.3888254   0.38371378  0.4784748   0.29682034  0.39584547\n",
      "   0.04575852  0.36775863  0.48655188  0.11976591  0.27752763  0.45624638\n",
      "   0.14595321  0.6328605   0.62718767  0.74005044 -0.04840994  0.8621479\n",
      "   0.39608085  0.16836888  0.40470934  0.5487496   0.67132556  0.53862816\n",
      "   0.2585551   0.43769786  0.21662723  0.3993246   0.3863764   0.74868834\n",
      "   0.54059005  0.5226249   0.8853083   0.5100843   0.4713176   0.6288782\n",
      "   0.4158825   0.15641084  0.501167    0.55344445  0.46600524  0.7233763\n",
      "   0.7189115   0.6758362   0.17130303  0.8812604   0.37282377  0.5059374\n",
      "   0.21928154  0.2333759   0.69581646  0.5479321   0.29704314  0.47941297\n",
      "   0.36104703  0.59081185  0.43561497  0.38899496  0.05034807  0.46229273\n",
      "   0.5751436   0.38297117  0.05884364  0.4218337   0.7010137   0.36551276\n",
      "   0.31906456  0.6689749   0.2547438   0.20014408  0.3497674   0.56914747\n",
      "   0.5291833   0.5260258   0.36022428  0.6644163   0.54890823  0.8697258\n",
      "   0.3011067 ]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0343856]\n",
      ">> rand_val = [0.81091871]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      ">> q_actions = [[ 1.0326493   0.53967285  0.23285908  0.42547044  0.9794259   0.40015113\n",
      "   0.5046088   0.39096567  0.386894    0.41597337  0.85209394  0.59298456\n",
      "   0.4248235   0.34401298  0.4433119   0.35373425  0.5528997   0.48492262\n",
      "   0.2268392   0.6527848   0.5989498   0.41797638  0.4260981   0.36996588\n",
      "   0.5766365   0.44666952  0.84579694  0.45446578  0.6771142  -0.05991808\n",
      "   0.41935438  0.18518397  0.26980782  0.32065135  0.31749427  0.3913041\n",
      "   0.5235774   0.46290946  0.5091633   0.18876928  0.4429191   0.54864633\n",
      "   0.7014617   0.6929778   0.55025566  0.10170263  0.11956444  0.35168618\n",
      "   0.61715     0.88013124  0.8192079   0.03450677  0.5711113   0.4711909\n",
      "   0.51469755  0.32568908  0.36533692  0.52206403  0.95993924  0.44250995\n",
      "   0.3968092   0.5618298   0.23997062  0.3721125   0.3786555   0.3328654\n",
      "   0.62210023  0.26121956  0.6536833   0.57214016  0.7089913   0.9062722\n",
      "   0.2770279   0.2811012   0.37190902  0.4373306   0.28833804  0.4183282\n",
      "   0.39462513  0.50335616  0.20016164  0.34549907  0.4642771   0.20825212\n",
      "   0.7207879   0.5089288   0.4590262   0.9332757   0.5101622   0.4540347\n",
      "   0.54039824  0.73917687  0.3460483   0.7471306   0.4072082   0.63448477\n",
      "   0.58184785  0.49049515  0.62741697  0.2440352   0.17629707  0.49034572\n",
      "   0.52678365  0.37590995  0.6717236   0.36708194  0.4122842   0.7414069\n",
      "   0.4707355   0.74536836  0.26721066  0.39746797  0.3775546   0.84670067\n",
      "   0.24255589  0.08484757  0.67395234  0.41842568  0.20569801  0.41037935\n",
      "   0.23204318  0.23268543  0.49699923  0.37470502  0.46643853  0.673909\n",
      "   0.7855543   0.38328183  0.517841    0.70225286  0.75077987  0.21551801\n",
      "   0.7431781   0.30237538  0.5117732  -0.02860278  0.5631752   0.45885095\n",
      "   0.89117765  0.3875211   0.651901    0.68470657  0.767511    0.20972057\n",
      "   0.57233214  0.56499934  0.41100162  0.7000214   0.4153682   0.2973482\n",
      "   0.0945994   0.2535007   0.7165842   0.554243    0.6194752   0.4007017\n",
      "   0.16572955  0.6948082   0.12252983  0.19422954  0.2794994   0.0608879\n",
      "   0.50098574  0.44976562  0.47711757  0.7429776   0.59706235  0.762557\n",
      "   0.61911047  0.07840925  0.5558371   0.6601234   0.3397658   0.1986984\n",
      "   0.61154366  0.33819336  0.35055858  0.47224084  0.31548434  0.29897302\n",
      "   0.49680978  0.4310788   0.5376061   0.3261257   0.35092044  0.69901204\n",
      "   0.48028556  0.29980266  0.6102768   0.3990149   0.12242514  0.41085908\n",
      "   0.38037777  0.49515805  0.12109566  0.7307676   0.3356697   0.4005512\n",
      "   0.56476855  0.76229846  0.44670784  0.48974425  0.07719961  0.7734711\n",
      "   0.7093884   0.41731375  0.8157507   0.2015489   0.45713407  0.3449573\n",
      "   0.11879215  0.2874545   0.96031916  0.6238628   0.12532815  0.53143334\n",
      "   0.3829372   0.65346336  0.2034114   0.83961344  0.71810985  0.6092098\n",
      "   0.17206013  0.70799744  0.7470567   0.5724233   0.2587055  -0.00415751\n",
      "   0.32784975  0.52583784  0.8796028   0.50071996  0.4305089   0.5820716\n",
      "   0.44951695  0.45782235  0.46001706  0.44718426  0.5864898   0.3354311\n",
      "   0.4326502   0.33021033  0.31113106  0.5640076   0.27300164  0.09589463\n",
      "   0.26685435  0.7001055   0.41098526  0.21126716  0.109045    0.13194066\n",
      "   0.6530563   0.13682383  0.33398443  0.5931064   0.6973752   0.55942345\n",
      "   0.18650392  0.831742    0.08072671  0.2544819   0.32274088  0.24839516\n",
      "   0.23231196  0.43571153  0.44180673  0.2674809   0.45676622  0.47977638\n",
      "   0.778535    0.6037588   0.61974764  0.7605992   0.07308587  0.19921836\n",
      "   0.83621913  0.638338    0.59344345  0.53714585  0.59390634  0.55128\n",
      "   0.34558624  0.64836866  0.69299847  0.23879685  0.19832963  0.47880146\n",
      "   0.4495276   0.69274557  0.196899    0.5478154   0.7886662   0.27209342\n",
      "   0.38086766  0.61986077  0.7215221   0.6702117   0.40868253  0.6656133\n",
      "   0.7666925   0.2734513   0.3755255   0.48818445  0.1214419   0.451438\n",
      "   0.20667362  0.03875273  0.31434554  0.5498987   0.5809969   1.0239078\n",
      "   0.63792664  0.62277424  0.3934846   0.41664153  0.19351578  0.49983934\n",
      "   0.31866288  0.38920254  0.66253585  0.6098118   0.58368266  0.1828672\n",
      "   0.8480003   0.7325841   0.63038164  0.5304153   0.22851098  0.5065029\n",
      "   0.4123848   0.46678567  0.4873966   0.6057539   0.37790504  0.2128108\n",
      "   0.74722505  0.2583682   0.7342715   0.52072203  0.89002407  0.6545319\n",
      "   0.64419776  0.43939942  0.6852342   0.2793882   0.16407168  0.16121122\n",
      "   0.5322764   0.27853167  0.48065698  0.1151661   0.37803066  0.25405595\n",
      "  -0.11534205  0.43970758  0.65460676  0.61432916  0.32964754  0.47744668\n",
      "   0.17582071  0.6573801   0.4189152   0.2922483   0.4776782   0.38310188\n",
      "   0.37760296  0.16632384  0.5522761   0.39850497  0.6372411   0.2466263\n",
      "   0.10274583  0.3878659   0.38352707  0.47792935  0.29651815  0.39436534\n",
      "   0.04477665  0.36839393  0.4865053   0.11909854  0.2776238   0.45496336\n",
      "   0.14584234  0.6327519   0.62643486  0.73933756 -0.04869267  0.8617908\n",
      "   0.39503255  0.16848567  0.4042555   0.54831094  0.6712157   0.53719974\n",
      "   0.25865033  0.436768    0.21565674  0.3977057   0.3868357   0.7488247\n",
      "   0.539799    0.5217287   0.8855405   0.510256    0.47182167  0.62795806\n",
      "   0.41577834  0.15594429  0.49921262  0.5527344   0.4655104   0.7226793\n",
      "   0.71828973  0.6748538   0.16950798  0.8796221   0.3719641   0.50564116\n",
      "   0.21796216  0.23360105  0.69537616  0.547311    0.29720002  0.47946534\n",
      "   0.3597358   0.58948404  0.4329391   0.38769227  0.05045927  0.46185645\n",
      "   0.57411945  0.38143206  0.05860659  0.4219752   0.70072746  0.36462963\n",
      "   0.31829482  0.669883    0.25436664  0.20004693  0.34882575  0.56864476\n",
      "   0.52832484  0.5256473   0.36010903  0.6642239   0.5487028   0.8688145\n",
      "   0.300591  ]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0326493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rand_val = [0.06319756]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> q_actions = [[ 1.0332      0.54032195  0.23438618  0.42613703  0.97991043  0.40003562\n",
      "   0.5050729   0.39083096  0.38786507  0.41734686  0.85315526  0.59304535\n",
      "   0.4260107   0.3445149   0.44320425  0.35527328  0.5544902   0.48554564\n",
      "   0.22823966  0.652991    0.59908587  0.41835546  0.42688492  0.37044182\n",
      "   0.57729447  0.44749448  0.845793    0.45533425  0.67874646 -0.05841815\n",
      "   0.42060757  0.1871112   0.27083516  0.3218955   0.31781727  0.3921726\n",
      "   0.5243617   0.46358478  0.51115286  0.19014454  0.4432906   0.54872763\n",
      "   0.7010337   0.69432616  0.550404    0.10349321  0.1202262   0.35235092\n",
      "   0.6170762   0.88084567  0.82100344  0.03501624  0.57265097  0.4718973\n",
      "   0.5156196   0.32715017  0.36640725  0.5230271   0.9603107   0.44408134\n",
      "   0.39804938  0.5607373   0.24127387  0.37349308  0.37935993  0.33354676\n",
      "   0.622687    0.2626596   0.654096    0.5735184   0.7097076   0.9068835\n",
      "   0.2790861   0.2817543   0.37280875  0.4383124   0.28940776  0.41885936\n",
      "   0.39434215  0.5043438   0.20140791  0.34699118  0.4656758   0.20897265\n",
      "   0.7207458   0.5091766   0.45910865  0.9337113   0.5116615   0.45538163\n",
      "   0.5419772   0.74003774  0.34588897  0.74752617  0.40749907  0.63610005\n",
      "   0.58356833  0.4906313   0.62802315  0.24472703  0.1779336   0.49067187\n",
      "   0.5291234   0.37781128  0.672418    0.36753047  0.4121612   0.7404221\n",
      "   0.4717469   0.7456542   0.268946    0.39906594  0.37897414  0.84743524\n",
      "   0.24382953  0.08659667  0.67410976  0.4186921   0.205834    0.4115232\n",
      "   0.23283403  0.23440452  0.49894136  0.375306    0.46654314  0.6749688\n",
      "   0.7860106   0.38542852  0.5185533   0.70203495  0.7515291   0.21616925\n",
      "   0.7430551   0.30323696  0.5123804  -0.02756843  0.5634322   0.46019754\n",
      "   0.8928472   0.38844812  0.6527985   0.68650603  0.76760304  0.21017261\n",
      "   0.57233965  0.56593466  0.4117052   0.7005689   0.41570926  0.29917994\n",
      "   0.0963209   0.25497913  0.7178872   0.55658424  0.62071824  0.40143424\n",
      "   0.16534173  0.69520545  0.12395868  0.195618    0.2804339   0.06260014\n",
      "   0.5025209   0.45019442  0.47782388  0.7438007   0.597076    0.76432085\n",
      "   0.6204729   0.07956702  0.5567603   0.66143006  0.34121254  0.20062873\n",
      "   0.6133037   0.33935356  0.35187736  0.47264826  0.31644478  0.3008837\n",
      "   0.49761435  0.43226296  0.53805226  0.32775024  0.35101116  0.6996435\n",
      "   0.4811397   0.3002618   0.6114279   0.39983705  0.12309414  0.41234446\n",
      "   0.3816846   0.49592364  0.12217918  0.73154914  0.3357215   0.40243536\n",
      "   0.5659309   0.7628882   0.44557247  0.49041316  0.07740569  0.774138\n",
      "   0.70939326  0.416903    0.81622875  0.20222917  0.4580081   0.3449642\n",
      "   0.12063175  0.28982344  0.9600694   0.624107    0.1266703   0.5315953\n",
      "   0.38424656  0.65445226  0.20488861  0.8396894   0.71984196  0.61086464\n",
      "   0.17327446  0.70826876  0.7478196   0.5735923   0.25946864 -0.00373253\n",
      "   0.32879794  0.5272551   0.88049346  0.50055826  0.43157208  0.5830428\n",
      "   0.45017123  0.45851466  0.45973957  0.44752365  0.58767813  0.33629188\n",
      "   0.4343612   0.3312227   0.3121354   0.566061    0.27476877  0.09687838\n",
      "   0.26774216  0.70140445  0.41182125  0.21272458  0.11076331  0.13239065\n",
      "   0.6531969   0.13771397  0.3351819   0.5934822   0.6994265   0.55997807\n",
      "   0.18711367  0.8326936   0.0827021   0.25523233  0.3232451   0.24863887\n",
      "   0.23256892  0.43662885  0.44263542  0.26798928  0.45801383  0.480591\n",
      "   0.7781961   0.6045821   0.6206049   0.76171964  0.07445663  0.20057252\n",
      "   0.8375087   0.6377932   0.59475386  0.5379221   0.5942019   0.5528352\n",
      "   0.3471561   0.64956504  0.69347167  0.23904328  0.19896102  0.4794705\n",
      "   0.4512894   0.6929681   0.19887072  0.55029416  0.78913486  0.27358657\n",
      "   0.3814904   0.6202958   0.72125125  0.6711259   0.40973136  0.66541165\n",
      "   0.76707274  0.2748925   0.37624717  0.48820642  0.1226632   0.4525246\n",
      "   0.2085803   0.04011682  0.314721    0.5515101   0.58130234  1.0231972\n",
      "   0.6375943   0.62355834  0.3944677   0.41717508  0.19437319  0.50059336\n",
      "   0.31966895  0.39087278  0.66334975  0.6104821   0.5855409   0.18361923\n",
      "   0.84942615  0.73396045  0.6303422   0.53235286  0.22940448  0.50786436\n",
      "   0.41305047  0.46757773  0.4889169   0.60648346  0.37846303  0.21395142\n",
      "   0.7480384   0.25807753  0.7360716   0.52129734  0.8905381   0.65583587\n",
      "   0.6443396   0.44003555  0.6859019   0.2798493   0.16481256  0.16232315\n",
      "   0.53218806  0.27930737  0.481232    0.11739081  0.37863794  0.25577843\n",
      "  -0.11277759  0.439951    0.65496373  0.615243    0.3306625   0.47938088\n",
      "   0.17599234  0.65747166  0.42014164  0.29337904  0.47869572  0.3850645\n",
      "   0.37859732  0.16775385  0.55257964  0.39956123  0.63814867  0.24705112\n",
      "   0.1036272   0.38904837  0.38442358  0.47862232  0.29728863  0.39670014\n",
      "   0.04681841  0.3688724   0.486098    0.1208891   0.27853447  0.45560068\n",
      "   0.14642137  0.63342834  0.6269955   0.740222   -0.04696399  0.8619187\n",
      "   0.39535722  0.16949046  0.40566865  0.5493109   0.67107767  0.53840816\n",
      "   0.2584062   0.4379641   0.21638654  0.39959094  0.38662028  0.7498134\n",
      "   0.5405814   0.5224875   0.88565123  0.5106716   0.4709017   0.62832063\n",
      "   0.4162232   0.15701088  0.5007098   0.553402    0.46623105  0.72393453\n",
      "   0.71916354  0.67595696  0.1711131   0.8808303   0.37297392  0.5063609\n",
      "   0.21990784  0.23443219  0.695222    0.54829353  0.29850164  0.47991183\n",
      "   0.36119133  0.59098035  0.43508434  0.38948783  0.05150855  0.46211654\n",
      "   0.5750171   0.3831266   0.05955872  0.42294398  0.70136374  0.3649321\n",
      "   0.31992865  0.6692496   0.25551757  0.20109072  0.35049772  0.5689508\n",
      "   0.5290655   0.5268497   0.36074674  0.6639887   0.54889786  0.8690398\n",
      "   0.30155784]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0332]\n",
      ">> rand_val = [0.27102838]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      ">> q_actions = [[ 1.0323524   0.5397689   0.23353855  0.42559916  0.9796501   0.39961374\n",
      "   0.5045421   0.3906003   0.38736868  0.41715202  0.8522285   0.59280884\n",
      "   0.42560852  0.34346643  0.4429591   0.35422227  0.5539325   0.4847386\n",
      "   0.22709906  0.6528803   0.598886    0.41797012  0.42629975  0.37025398\n",
      "   0.57682717  0.4471963   0.8452123   0.45442152  0.6781056  -0.05921143\n",
      "   0.41996956  0.18670046  0.27006048  0.3211352   0.31750855  0.39172712\n",
      "   0.5235189   0.46307704  0.510027    0.18919775  0.44237006  0.5482956\n",
      "   0.70041776  0.6937306   0.55011284  0.10254982  0.11944097  0.35215884\n",
      "   0.61688733  0.8804085   0.82005215  0.03475082  0.57128537  0.47111452\n",
      "   0.51478523  0.3263414   0.36583936  0.52222735  0.9599359   0.44353423\n",
      "   0.3972857   0.56093824  0.24034981  0.37260464  0.37898225  0.33299112\n",
      "   0.6221795   0.26205707  0.65356994  0.57288057  0.7091435   0.9059546\n",
      "   0.27816284  0.28118464  0.3724128   0.4376679   0.28871042  0.4185175\n",
      "   0.393847    0.5038273   0.200811    0.3460476   0.4648664   0.20864898\n",
      "   0.72038007  0.50847894  0.4587165   0.9330804   0.5109517   0.45462745\n",
      "   0.5409331   0.7395698   0.34541553  0.7469683   0.407039    0.63528216\n",
      "   0.5828774   0.49040592  0.62747204  0.24409518  0.17720363  0.48996407\n",
      "   0.52777755  0.3771049   0.6717013   0.36716807  0.4116192   0.7401948\n",
      "   0.47130835  0.7452066   0.26829332  0.39821637  0.3780862   0.8466749\n",
      "   0.24335857  0.08599031  0.67381     0.4182762   0.20546988  0.411104\n",
      "   0.2325498   0.23359458  0.49816406  0.37473398  0.46670583  0.67451614\n",
      "   0.7857415   0.3845604   0.5179572   0.70178324  0.75120515  0.2156567\n",
      "   0.7422122   0.3029211   0.5122523  -0.02823004  0.56310797  0.45958173\n",
      "   0.8918772   0.3877616   0.6523114   0.6855583   0.7666975   0.20981339\n",
      "   0.5719504   0.5651885   0.4113173   0.69996816  0.41489524  0.29875714\n",
      "   0.09563309  0.25437012  0.7168046   0.55558014  0.6196618   0.40062058\n",
      "   0.16515476  0.6946043   0.12328389  0.19454631  0.2797957   0.06217378\n",
      "   0.5014699   0.44952917  0.47712833  0.7428224   0.5968369   0.7633743\n",
      "   0.6196223   0.07895142  0.556213    0.660648    0.34078363  0.19971228\n",
      "   0.6125026   0.33886304  0.35133383  0.47193623  0.315667    0.30004764\n",
      "   0.49684274  0.43153358  0.5371713   0.32709038  0.35013515  0.69867295\n",
      "   0.4805616   0.2997396   0.6106406   0.39899245  0.12254357  0.4111393\n",
      "   0.38097078  0.49523965  0.12181288  0.73123074  0.33547044  0.40189874\n",
      "   0.5654442   0.7623743   0.44565874  0.48988315  0.07713178  0.7735293\n",
      "   0.70918095  0.41695347  0.81580514  0.2013587   0.45756838  0.3450863\n",
      "   0.11988324  0.28838724  0.9596726   0.6237536   0.12620705  0.5312393\n",
      "   0.3836149   0.65419805  0.20438129  0.8388941   0.7190666   0.60950816\n",
      "   0.17227635  0.70764446  0.7469523   0.57272667  0.25915453 -0.00422221\n",
      "   0.32799926  0.526412    0.8798804   0.50060034  0.43101224  0.58203566\n",
      "   0.44986317  0.45804423  0.4595438   0.4471024   0.5867357   0.33597603\n",
      "   0.4335609   0.33073914  0.3116121   0.5650358   0.27399236  0.09629595\n",
      "   0.26748142  0.7007927   0.41131803  0.21219984  0.11011857  0.13253689\n",
      "   0.65317035  0.1372247   0.3343832   0.5931205   0.6987971   0.5594674\n",
      "   0.18638131  0.83178866  0.08183196  0.25466406  0.323182    0.24794695\n",
      "   0.23245946  0.43580788  0.44175148  0.26792675  0.45716837  0.47990772\n",
      "   0.7774314   0.60418385  0.62013954  0.7611469   0.07380575  0.20000488\n",
      "   0.8370025   0.63732034  0.59384936  0.53724885  0.59399104  0.552461\n",
      "   0.34663218  0.6489428   0.6932789   0.2385801   0.19883472  0.47891468\n",
      "   0.45043567  0.6926793   0.19809419  0.5492875   0.78822446  0.27276075\n",
      "   0.3808987   0.61992747  0.7211355   0.67083406  0.40936607  0.6652051\n",
      "   0.7662605   0.27410108  0.375933    0.48806715  0.12193966  0.45201194\n",
      "   0.20724592  0.03952253  0.31431752  0.55078524  0.5806463   1.0230324\n",
      "   0.63738585  0.623011    0.39395422  0.4163397   0.1940602   0.49999782\n",
      "   0.31913775  0.39047793  0.6627195   0.6098211   0.58465767  0.18323106\n",
      "   0.8487799   0.7331009   0.6303289   0.53182644  0.22902408  0.5069524\n",
      "   0.41278964  0.46705538  0.4879352   0.6061271   0.37799138  0.21353388\n",
      "   0.7474246   0.2576135   0.7349351   0.5208183   0.8897377   0.6551532\n",
      "   0.64404607  0.43927848  0.6855104   0.27974588  0.16462085  0.16153565\n",
      "   0.5321556   0.2787934   0.4812031   0.11671588  0.37800068  0.25513\n",
      "  -0.11352819  0.43933195  0.6541228   0.61480343  0.3300435   0.47882974\n",
      "   0.17610425  0.65711844  0.4195019   0.29296952  0.477845    0.38408333\n",
      "   0.3776846   0.16706476  0.55247015  0.39917612  0.6373224   0.24677037\n",
      "   0.10329503  0.38866538  0.38398203  0.47835937  0.2967633   0.39583516\n",
      "   0.04616138  0.36876777  0.48597538  0.12015909  0.27804822  0.4549595\n",
      "   0.1458469   0.6330143   0.6265177   0.73961824 -0.04793882  0.8610525\n",
      "   0.3946498   0.16922465  0.40468594  0.54893076  0.6706358   0.537831\n",
      "   0.2581845   0.4374241   0.21616527  0.3986988   0.3864103   0.7491677\n",
      "   0.54031813  0.5219843   0.88508606  0.50952053  0.47057143  0.6276326\n",
      "   0.41572982  0.15625265  0.49951366  0.55293167  0.46561936  0.72315526\n",
      "   0.7187493   0.67549574  0.17035922  0.8802872   0.3728035   0.5056832\n",
      "   0.21917456  0.23377314  0.69497126  0.5478099   0.29774022  0.4797305\n",
      "   0.3602742   0.5901943   0.4343996   0.38870713  0.05084535  0.46188563\n",
      "   0.5743759   0.3823367   0.05909923  0.42257774  0.7009686   0.36449733\n",
      "   0.319148    0.6693761   0.25453097  0.20067152  0.34951633  0.5684992\n",
      "   0.5284223   0.52608544  0.36052677  0.66378665  0.54846996  0.86878073\n",
      "   0.30096442]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003 1.03235245]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0323524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rand_val = [0.27211775]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> q_actions = [[ 1.0314754   0.5386282   0.23242399  0.425113    0.9780458   0.39939794\n",
      "   0.50390863  0.3898308   0.38629594  0.41679204  0.85097605  0.59157544\n",
      "   0.42543733  0.342958    0.44182804  0.35310853  0.55388844  0.48384467\n",
      "   0.22621597  0.65229505  0.59873194  0.41704983  0.4262228   0.369833\n",
      "   0.57603115  0.44660386  0.844701    0.45339364  0.67675734 -0.06003174\n",
      "   0.4191775   0.18629321  0.26932853  0.32031888  0.3168208   0.3912407\n",
      "   0.5220419   0.46303174  0.50936973  0.18803161  0.44167042  0.5471765\n",
      "   0.69941306  0.6933905   0.5501502   0.10254386  0.11896378  0.35167873\n",
      "   0.6162703   0.88013375  0.8195696   0.0343602   0.5703634   0.470136\n",
      "   0.51412314  0.32500184  0.36521524  0.52171314  0.9601171   0.44363362\n",
      "   0.39751834  0.56071293  0.23976988  0.37204957  0.37848338  0.3331963\n",
      "   0.6217066   0.2614528   0.65289235  0.5723607   0.70829016  0.90547\n",
      "   0.27778763  0.27981895  0.3729637   0.43694523  0.28838548  0.4180505\n",
      "   0.39272094  0.50320506  0.20010287  0.34566447  0.46465757  0.20811911\n",
      "   0.72003055  0.5075285   0.45811185  0.9320148   0.51042265  0.45408872\n",
      "   0.540096    0.73970294  0.34523407  0.74604553  0.40700278  0.6352947\n",
      "   0.5822446   0.49046057  0.62662214  0.24351974  0.17657244  0.48902524\n",
      "   0.5271081   0.3759899   0.67097896  0.36583918  0.41112667  0.7395644\n",
      "   0.47052842  0.744426    0.26792336  0.39744377  0.3774916   0.84639096\n",
      "   0.24249792  0.08554682  0.6739287   0.41773587  0.20505708  0.41074687\n",
      "   0.2325649   0.23330073  0.49753672  0.3741605   0.46645346  0.6739365\n",
      "   0.78475726  0.3836749   0.5171767   0.7011961   0.7503904   0.21484567\n",
      "   0.7418487   0.30263728  0.5116825  -0.02918631  0.5627678   0.45887005\n",
      "   0.89092135  0.38744465  0.65144587  0.68500197  0.76516384  0.20900862\n",
      "   0.5712078   0.5642251   0.41074327  0.69948816  0.4145644   0.29812258\n",
      "   0.09529659  0.25368243  0.71579516  0.55479217  0.61972904  0.40004802\n",
      "   0.16398022  0.69354886  0.12263933  0.19358686  0.27921757  0.06199288\n",
      "   0.50076985  0.44915825  0.47635332  0.7422819   0.5962699   0.7623935\n",
      "   0.61882097  0.07835305  0.55551034  0.66095746  0.34042165  0.19849741\n",
      "   0.61253184  0.3386573   0.35079333  0.4712088   0.3149442   0.29935563\n",
      "   0.4963718   0.4312875   0.53628623  0.32629108  0.34845355  0.69789356\n",
      "   0.4805098   0.29929465  0.6103334   0.39905602  0.12203461  0.41072038\n",
      "   0.3804846   0.4947686   0.12160009  0.7305965   0.33462548  0.4015364\n",
      "   0.56491375  0.76154923  0.44495302  0.49004704  0.07677492  0.77315295\n",
      "   0.7086723   0.41616032  0.81559116  0.20083314  0.45647636  0.34443316\n",
      "   0.11958179  0.28755057  0.9588976   0.6233342   0.12521425  0.5307227\n",
      "   0.3831048   0.6541513   0.20412928  0.83884525  0.7176227   0.6082471\n",
      "   0.17147279  0.70733374  0.74718225  0.57117337  0.25891358 -0.0045442\n",
      "   0.32740873  0.5264755   0.8796109   0.5000912   0.430618    0.58141273\n",
      "   0.4491555   0.45824772  0.45985416  0.4462623   0.58663005  0.3358884\n",
      "   0.43302965  0.33010334  0.31074172  0.5649309   0.2732373   0.09604558\n",
      "   0.2666989   0.70048565  0.41080797  0.21264671  0.10984987  0.13267043\n",
      "   0.6525651   0.1368548   0.33357713  0.5925136   0.6982252   0.5590314\n",
      "   0.18617547  0.83174336  0.08032519  0.25449342  0.32281822  0.2469964\n",
      "   0.23160097  0.43505642  0.4417943   0.26761666  0.45729193  0.47946587\n",
      "   0.77705586  0.6038337   0.6200772   0.7606631   0.07342586  0.19966581\n",
      "   0.8362524   0.6369822   0.5933765   0.5368932   0.5939393   0.55164415\n",
      "   0.34620517  0.64891756  0.6924349   0.237873    0.19852573  0.47813898\n",
      "   0.44981128  0.6922594   0.19756857  0.54771876  0.7877463   0.27192706\n",
      "   0.38068628  0.61979055  0.72101116  0.6699704   0.40880758  0.6645457\n",
      "   0.764925    0.27307314  0.37555414  0.48761424  0.12174448  0.45083869\n",
      "   0.20619825  0.03868574  0.31348014  0.550404    0.57990015  1.022606\n",
      "   0.63671947  0.6221476   0.3930063   0.41575867  0.19371909  0.49916452\n",
      "   0.31881553  0.3901188   0.66201574  0.60940903  0.5834814   0.1829108\n",
      "   0.8486511   0.73354495  0.630129    0.53130245  0.22898453  0.5066852\n",
      "   0.4117359   0.4665968   0.48695397  0.60632586  0.3780679   0.2133558\n",
      "   0.7468142   0.25634652  0.7339721   0.52011013  0.8888054   0.6550867\n",
      "   0.64364964  0.43900043  0.68453705  0.27972886  0.16338193  0.16024905\n",
      "   0.5318462   0.27865613  0.48069817  0.11564985  0.3771377   0.25498202\n",
      "  -0.11405197  0.43890736  0.6534101   0.6144298   0.3295138   0.47774982\n",
      "   0.17561719  0.65644073  0.41887185  0.2919571   0.47737032  0.38265538\n",
      "   0.3766702   0.16621509  0.55138177  0.39861628  0.637457    0.24624656\n",
      "   0.10278565  0.38837707  0.38350496  0.47776937  0.29623932  0.39606053\n",
      "   0.04562151  0.36772585  0.48549175  0.11980614  0.27676505  0.45481688\n",
      "   0.14518213  0.6322562   0.6265228   0.73885167 -0.04967234  0.8610076\n",
      "   0.3945212   0.16927192  0.40334556  0.5492969   0.6701808   0.5374548\n",
      "   0.2578449   0.43681267  0.21624635  0.39806172  0.38573357  0.74765676\n",
      "   0.53965795  0.5215783   0.88412714  0.50903237  0.4699667   0.6262388\n",
      "   0.41523364  0.15590978  0.49922392  0.5526831   0.4651031   0.72280943\n",
      "   0.7183316   0.67516315  0.17030078  0.87966824  0.37253326  0.5048565\n",
      "   0.21856214  0.23260054  0.6941489   0.5478128   0.29705274  0.47964075\n",
      "   0.3596983   0.5894127   0.4343031   0.38833317  0.05001196  0.46111012\n",
      "   0.57371455  0.38240764  0.05868626  0.42157272  0.7003713   0.36419198\n",
      "   0.3180647   0.668411    0.25395197  0.19943091  0.34901637  0.56778836\n",
      "   0.52775604  0.5254069   0.36017448  0.6631637   0.5483387   0.8688227\n",
      "   0.30038846]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003 1.03235245\n",
      " 1.03147542]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0314754]\n",
      ">> rand_val = [0.94057602]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      ">> q_actions = [[ 1.029795    0.5383941   0.23318318  0.42481428  0.9785485   0.39807117\n",
      "   0.5034082   0.38994324  0.38684487  0.4166477   0.85048646  0.59161204\n",
      "   0.42439732  0.34203354  0.4423002   0.35345194  0.55262595  0.48335615\n",
      "   0.22573768  0.65219575  0.597935    0.41684243  0.42477065  0.36931688\n",
      "   0.57522666  0.44607025  0.84291506  0.452398    0.6765299  -0.06029353\n",
      "   0.4189823   0.18667567  0.26919466  0.3194417   0.3162002   0.3910421\n",
      "   0.521256    0.46133354  0.50854146  0.18790007  0.44057897  0.5466821\n",
      "   0.69819826  0.692082    0.5485788   0.10148379  0.11822808  0.35080665\n",
      "   0.61559445  0.8788361   0.8180042   0.03379494  0.5692775   0.46994397\n",
      "   0.5131433   0.3257903   0.36494654  0.52138484  0.9583702   0.44264206\n",
      "   0.39586213  0.5598447   0.23849949  0.37123168  0.3779076   0.33202615\n",
      "   0.6205627   0.26073056  0.6521948   0.5724059   0.706977    0.90415204\n",
      "   0.2769411   0.280328    0.37203383  0.43615544  0.28750333  0.41751617\n",
      "   0.3925814   0.5018745   0.19985649  0.34370434  0.46388188  0.20784004\n",
      "   0.71890974  0.50725627  0.45777047  0.9305695   0.5094798   0.4538579\n",
      "   0.54005194  0.73767424  0.34403104  0.74566567  0.40542442  0.6339613\n",
      "   0.5816645   0.49008003  0.62549406  0.24345045  0.17604953  0.48856282\n",
      "   0.5254674   0.3757863   0.6699744   0.3659324   0.40966535  0.7380786\n",
      "   0.46998155  0.7435067   0.26765883  0.3971088   0.37660587  0.84509623\n",
      "   0.24168971  0.08595341  0.6724489   0.41783282  0.203771    0.40981558\n",
      "   0.23177184  0.2325464   0.49670535  0.3736542   0.46615607  0.67330194\n",
      "   0.7852239   0.38319927  0.5163491   0.7007574   0.7504739   0.21521308\n",
      "   0.73996544  0.3028771   0.5114421  -0.02880609  0.56129193  0.45798528\n",
      "   0.88970935  0.38679537  0.6508012   0.68375665  0.765188    0.20933755\n",
      "   0.57086474  0.56365097  0.41004515  0.69846797  0.41367668  0.29865628\n",
      "   0.09477228  0.2536001   0.71485126  0.55444235  0.6183444   0.39920816\n",
      "   0.16366014  0.6928697   0.12304342  0.19313279  0.2782053   0.06144431\n",
      "   0.49994263  0.44816744  0.47513494  0.7412651   0.59599173  0.7612089\n",
      "   0.61821616  0.07802266  0.55461884  0.6593214   0.34042776  0.19921076\n",
      "   0.61133546  0.33866563  0.35028064  0.47005552  0.31404942  0.29886198\n",
      "   0.49484617  0.4303005   0.5351106   0.32565057  0.3482368   0.6975322\n",
      "   0.47988516  0.29823017  0.6095229   0.39733756  0.12209681  0.40886813\n",
      "   0.38000956  0.49376476  0.12100157  0.7304651   0.33493087  0.40127575\n",
      "   0.5643617   0.76120615  0.4443744   0.48892838  0.07633522  0.7719867\n",
      "   0.708363    0.41615397  0.81451     0.19997594  0.4568702   0.34484896\n",
      "   0.1194185   0.28716287  0.95712185  0.6221638   0.12617975  0.53018665\n",
      "   0.3829511   0.65351015  0.2036446   0.8373784   0.71835005  0.6069944\n",
      "   0.17116585  0.70529824  0.7445792   0.57160425  0.2580908  -0.00550395\n",
      "   0.32664773  0.5249909   0.8788738   0.4999473   0.42986348  0.5794459\n",
      "   0.4485165   0.45723796  0.4583594   0.44634223  0.58517754  0.33530843\n",
      "   0.43294492  0.32992518  0.31115878  0.5631505   0.2730875   0.0958311\n",
      "   0.2663278   0.69925106  0.4094219   0.21122737  0.10930002  0.13205975\n",
      "   0.6511755   0.13634858  0.3335203   0.5922526   0.6977952   0.5577191\n",
      "   0.18508288  0.82975805  0.08059803  0.25401682  0.32268286  0.24662194\n",
      "   0.23226571  0.43440345  0.4398548   0.26761433  0.45537773  0.47871217\n",
      "   0.77503777  0.6028196   0.6193882   0.76034594  0.07246694  0.19911057\n",
      "   0.83519423  0.6356987   0.5926312   0.53572345  0.59291774  0.5514844\n",
      "   0.34580755  0.6472804   0.691926    0.23720993  0.19798118  0.4777402\n",
      "   0.4494592   0.69088006  0.19735575  0.54786545  0.7866193   0.27177104\n",
      "   0.37967995  0.6188714   0.7198128   0.6690542   0.40910834  0.6636301\n",
      "   0.76390207  0.27323654  0.37448275  0.487158    0.12136871  0.4506849\n",
      "   0.20591554  0.03895658  0.31352982  0.55011463  0.5789852   1.0209949\n",
      "   0.63642246  0.6219337   0.39267957  0.41475165  0.19342569  0.49839026\n",
      "   0.31799245  0.3901692   0.6612301   0.608344    0.58227956  0.18241301\n",
      "   0.8467402   0.73174083  0.62942326  0.53137845  0.228261    0.5052611\n",
      "   0.41177994  0.46546313  0.48583692  0.60487044  0.37632436  0.21344243\n",
      "   0.74595267  0.25626892  0.73308444  0.5198577   0.8884145   0.65355265\n",
      "   0.64325994  0.4376128   0.6842519   0.2793851   0.16454536  0.16008943\n",
      "   0.53094137  0.2780081   0.4806199   0.1162506   0.37608713  0.2547819\n",
      "  -0.11368576  0.43766457  0.65143824  0.6137744   0.32897234  0.477947\n",
      "   0.1757772   0.6556202   0.41823173  0.29204488  0.4768222   0.38298655\n",
      "   0.37630242  0.16675001  0.5514939   0.39802265  0.6360415   0.24542554\n",
      "   0.10252628  0.38803312  0.38248473  0.4781023   0.2955589   0.39499348\n",
      "   0.04517749  0.36818248  0.484676    0.11937916  0.27651927  0.45372704\n",
      "   0.14471346  0.631993    0.62478286  0.73797923 -0.04914287  0.857961\n",
      "   0.3932472   0.16856518  0.40292364  0.5478229   0.66922605  0.53622735\n",
      "   0.2567752   0.43640876  0.21547358  0.39757395  0.38549393  0.74770445\n",
      "   0.53983957  0.5207954   0.8830365   0.5069208   0.46856833  0.6261804\n",
      "   0.41487485  0.15516078  0.49834782  0.55246073  0.46401444  0.72192895\n",
      "   0.71727735  0.67479306  0.1690273   0.8790786   0.37232098  0.50340813\n",
      "   0.21869184  0.23245521  0.69289815  0.54663867  0.29688898  0.4790062\n",
      "   0.3590715   0.58895105  0.43315798  0.38765022  0.05029082  0.461087\n",
      "   0.5728582   0.3812894   0.05806595  0.42208755  0.7001753   0.36356857\n",
      "   0.31751454  0.66778475  0.25348216  0.20021513  0.34814626  0.5659751\n",
      "   0.5270262   0.52522206  0.35987294  0.6619507   0.54715884  0.8677703\n",
      "   0.30003512]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003 1.03235245\n",
      " 1.03147542 1.02979505]\n",
      ">> predict_movement_int = [0]\n",
      "[1.029795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> rand_val = [0.43916573]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      ">> q_actions = [[ 1.0270212   0.5371916   0.23286505  0.42401072  0.9768768   0.39637554\n",
      "   0.50209606  0.38883036  0.38633496  0.4174232   0.8487364   0.5906585\n",
      "   0.42396858  0.34089732  0.44123515  0.35275793  0.55274564  0.4822075\n",
      "   0.22453284  0.651526    0.597199    0.41594428  0.42432827  0.36862323\n",
      "   0.5736899   0.44528905  0.84087837  0.450906    0.6749896  -0.06054831\n",
      "   0.41877335  0.18697101  0.2679981   0.31844187  0.3157667   0.3905592\n",
      "   0.51977396  0.4605315   0.5076014   0.18679696  0.4383633   0.5446011\n",
      "   0.695724    0.6908462   0.5481628   0.10036287  0.11734435  0.3497985\n",
      "   0.6140709   0.8784783   0.8166759   0.03317761  0.5676746   0.46845922\n",
      "   0.51232946  0.32433057  0.36473203  0.5211562   0.95726305  0.44223848\n",
      "   0.39511847  0.5586991   0.23771888  0.37087202  0.37650567  0.33174798\n",
      "   0.6192596   0.26015502  0.6500063   0.5721457   0.70525354  0.9019056\n",
      "   0.27679512  0.2793281   0.37189484  0.43596578  0.28673398  0.41702434\n",
      "   0.39130664  0.5010767   0.19996822  0.34328252  0.46326837  0.20729664\n",
      "   0.71794313  0.5063661   0.45694774  0.9284033   0.5092299   0.45400134\n",
      "   0.5389682   0.7364197   0.34318164  0.7438768   0.40433747  0.63259953\n",
      "   0.5809271   0.48963666  0.6238677   0.2435066   0.1755707   0.48767632\n",
      "   0.52424663  0.37490028  0.66795313  0.36512023  0.40761065  0.73575413\n",
      "   0.46976632  0.74172235  0.26782167  0.3965126   0.37552     0.8439746\n",
      "   0.23987122  0.08656192  0.6714823   0.4170828   0.2033757   0.40978634\n",
      "   0.23135611  0.2324009   0.4959531   0.37265784  0.46574855  0.6727974\n",
      "   0.7839137   0.3825014   0.5150732   0.6996168   0.7497914   0.21493751\n",
      "   0.73801696  0.30300814  0.51081324 -0.02943709  0.56002074  0.45717055\n",
      "   0.8886862   0.3862551   0.6500535   0.68287194  0.7633025   0.20884696\n",
      "   0.5692048   0.562812    0.40916207  0.6969204   0.4127176   0.29867125\n",
      "   0.09484911  0.25341034  0.7137606   0.5543378   0.61693066  0.39757803\n",
      "   0.16148308  0.69122475  0.12284577  0.19215173  0.27731496  0.061405\n",
      "   0.49890655  0.447388    0.47324416  0.7399908   0.5953362   0.75997293\n",
      "   0.6171191   0.07749134  0.55317926  0.65863705  0.3404382   0.19877055\n",
      "   0.6111174   0.33872864  0.3503055   0.468835    0.31253535  0.29871416\n",
      "   0.49288267  0.42950746  0.5328666   0.32400036  0.34668896  0.69623363\n",
      "   0.4795779   0.2978722   0.6085926   0.39655706  0.12154371  0.4074641\n",
      "   0.37912092  0.49256775  0.12067169  0.72960806  0.33393893  0.4014641\n",
      "   0.56440276  0.760402    0.44295144  0.48853487  0.07571098  0.7710592\n",
      "   0.70724213  0.41492915  0.81271565  0.19859886  0.45668682  0.34401262\n",
      "   0.1198779   0.28711134  0.9554253   0.6210858   0.12642846  0.5289948\n",
      "   0.38336238  0.65288854  0.20405087  0.8361981   0.71762514  0.60553426\n",
      "   0.17014062  0.7031795   0.7430974   0.5698322   0.25720608 -0.00597036\n",
      "   0.32604846  0.5248675   0.87753415  0.49861205  0.42959496  0.5780702\n",
      "   0.44736248  0.45658237  0.45814815  0.44498184  0.5845891   0.33497563\n",
      "   0.43256506  0.3286249   0.3112281   0.5618903   0.27319258  0.09514764\n",
      "   0.26577258  0.6986522   0.40867344  0.21206766  0.1094012   0.13191009\n",
      "   0.6497134   0.13586599  0.33260614  0.59178257  0.69747144  0.5565961\n",
      "   0.18456286  0.8284838   0.07956374  0.25370198  0.32300946  0.2448652\n",
      "   0.23275812  0.43377066  0.4383686   0.26759148  0.45414796  0.47739968\n",
      "   0.77330244  0.60110337  0.61876196  0.7597001   0.07226938  0.1995756\n",
      "   0.83341026  0.6339845   0.592059    0.53461516  0.5920595   0.551547\n",
      "   0.3456276   0.6463139   0.69119096  0.23669742  0.19771919  0.47745252\n",
      "   0.44902486  0.68952554  0.1974133   0.5469017   0.7849126   0.27119982\n",
      "   0.37878132  0.6186464   0.71870244  0.66859055  0.4098236   0.6618761\n",
      "   0.7617921   0.27277505  0.37355813  0.48664913  0.12091896  0.45009825\n",
      "   0.20517874  0.03898522  0.3127324   0.54921854  0.5781108   1.018567\n",
      "   0.6350248   0.6209234   0.39085627  0.41351902  0.19284245  0.49717385\n",
      "   0.31740403  0.39011696  0.66042733  0.60657287  0.58065     0.1821962\n",
      "   0.8453295   0.73100317  0.6283728   0.5316264   0.22840221  0.5043572\n",
      "   0.41125417  0.46421114  0.48448762  0.6044403   0.37506455  0.21342576\n",
      "   0.74444926  0.2543689   0.73120254  0.5188942   0.8868338   0.6522887\n",
      "   0.6425586   0.43613395  0.6833139   0.2799515   0.16456077  0.15903434\n",
      "   0.53025305  0.27758104  0.4803365   0.11643279  0.37458032  0.2552395\n",
      "  -0.1131506   0.43538266  0.64903414  0.61264384  0.3287186   0.47778973\n",
      "   0.175455    0.65437114  0.4172402   0.29140717  0.47592694  0.38180646\n",
      "   0.37524337  0.16621628  0.5502611   0.3979353   0.63530076  0.24405363\n",
      "   0.10210526  0.38768315  0.38203004  0.47715598  0.29481465  0.3953661\n",
      "   0.04547057  0.36798364  0.48350117  0.11985978  0.27572426  0.45239466\n",
      "   0.14406562  0.6311641   0.6239836   0.73693335 -0.05009061  0.8562709\n",
      "   0.39169052  0.1691863   0.4018454   0.54807156  0.6679715   0.53525627\n",
      "   0.25559616  0.4355826   0.21529192  0.39671844  0.38432828  0.74655676\n",
      "   0.538969    0.5194135   0.8814881   0.50557     0.46681598  0.62385124\n",
      "   0.4140846   0.15451756  0.4971058   0.5517057   0.46286356  0.72125196\n",
      "   0.7165946   0.67424244  0.16847974  0.87768805  0.37202018  0.50195\n",
      "   0.21835363  0.23150831  0.69075763  0.54642224  0.29692468  0.47904316\n",
      "   0.35821345  0.5878906   0.43241084  0.38741553  0.05033872  0.46038014\n",
      "   0.5716182   0.38127625  0.05787513  0.4217646   0.69945586  0.36246347\n",
      "   0.31662226  0.6664943   0.25284502  0.19992012  0.3476333   0.5640442\n",
      "   0.525629    0.52478963  0.35963398  0.6604619   0.5461291   0.8669231\n",
      "   0.2997685 ]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003 1.03235245\n",
      " 1.03147542 1.02979505 1.02702117]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0270212]\n",
      ">> rand_val = [0.99090786]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> q_actions = [[ 1.0256393   0.53688365  0.23291227  0.42368057  0.97671497  0.39556235\n",
      "   0.5015418   0.38841146  0.38636404  0.4178782   0.84792817  0.59053004\n",
      "   0.42347342  0.34021252  0.4410422   0.35244733  0.5524895   0.48178193\n",
      "   0.22386536  0.65132487  0.5967988   0.41574508  0.42399126  0.36840695\n",
      "   0.57306135  0.44489333  0.8399359   0.45040664  0.6743275  -0.06070536\n",
      "   0.41857472  0.18684387  0.26752186  0.317837    0.3158587   0.39022052\n",
      "   0.5193082   0.45998275  0.50692534  0.1863448   0.4372812   0.5438356\n",
      "   0.69483423  0.69010496  0.54777056  0.09941104  0.1168367   0.34927323\n",
      "   0.61367667  0.8782362   0.8157239   0.0328556   0.5668214   0.4679362\n",
      "   0.51201934  0.32385296  0.36460185  0.52098405  0.95634097  0.44174582\n",
      "   0.39428222  0.5583333   0.23719788  0.37055576  0.3757682   0.33128136\n",
      "   0.61857975  0.2599122   0.6489815   0.5718493   0.704558    0.9006337\n",
      "   0.2764696   0.27918744  0.3714891   0.4359511   0.2861757   0.41685486\n",
      "   0.39099687  0.5005975   0.20006222  0.34276724  0.4626481   0.20703945\n",
      "   0.717419    0.5062198   0.45674747  0.92731726  0.5089411   0.45386174\n",
      "   0.5384368   0.73540187  0.34256455  0.74306107  0.4036796   0.6314995\n",
      "   0.58038926  0.48943287  0.62299967  0.24354663  0.1751802   0.48746195\n",
      "   0.5233375   0.37433514  0.6667911   0.36505967  0.40657645  0.7347474\n",
      "   0.46971905  0.7409891   0.26767582  0.396221    0.37472498  0.8433136\n",
      "   0.23889412  0.08675128  0.6708432   0.41685313  0.20297438  0.40967852\n",
      "   0.23097481  0.23201779  0.4952401   0.3721301   0.46547395  0.67256206\n",
      "   0.78335905  0.38204002  0.51441526  0.69929236  0.7495739   0.21509796\n",
      "   0.73721313  0.30311322  0.51055294 -0.02968216  0.55943274  0.45661604\n",
      "   0.8881266   0.38586754  0.6497322   0.68235046  0.7626831   0.20884165\n",
      "   0.5685694   0.5623928   0.40862522  0.696124    0.4122011   0.2986734\n",
      "   0.09479761  0.2533551   0.713254    0.55409455  0.61580855  0.39662114\n",
      "   0.16073018  0.69062304  0.12258583  0.19168538  0.27668282  0.06097448\n",
      "   0.49812126  0.44685835  0.47231266  0.7392398   0.59515     0.75937366\n",
      "   0.616461    0.07697216  0.55240095  0.6577925   0.3402941   0.19856259\n",
      "   0.61049116  0.33847052  0.35022694  0.46860197  0.31172982  0.29844737\n",
      "   0.49163622  0.42887217  0.5319554   0.3231808   0.34633526  0.6956393\n",
      "   0.47927982  0.29777616  0.607863    0.39586115  0.12111899  0.4064234\n",
      "   0.37847596  0.4919889   0.12035781  0.72922456  0.33375734  0.40141448\n",
      "   0.56425387  0.760113    0.44266617  0.48813796  0.07542419  0.77064216\n",
      "   0.7067442   0.4147531   0.81173694  0.19777846  0.4567056   0.34390607\n",
      "   0.11995509  0.2870162   0.9547791   0.6205337   0.12670147  0.52846235\n",
      "   0.38350195  0.65233856  0.20406947  0.835358    0.717427    0.6049495\n",
      "   0.16961327  0.7020284   0.7419617   0.569177    0.25650612 -0.00627404\n",
      "   0.32578033  0.52425754  0.87671834  0.4981795   0.42935702  0.5772848\n",
      "   0.44686255  0.45594943  0.45782068  0.44448543  0.58400905  0.33462402\n",
      "   0.43214747  0.32802722  0.31132993  0.5606971   0.27320248  0.09447733\n",
      "   0.2657568   0.6982637   0.4083273   0.2120426   0.10919982  0.13162765\n",
      "   0.6491973   0.13559815  0.3320068   0.5915866   0.69724405  0.5559919\n",
      "   0.18419322  0.8276845   0.07918406  0.2533039   0.3232397   0.2442715\n",
      "   0.23332946  0.43346408  0.43724155  0.26773226  0.45310035  0.4765278\n",
      "   0.77231586  0.6002392   0.6183746   0.75936335  0.07191226  0.1996966\n",
      "   0.8324115   0.63305324  0.5916282   0.5339568   0.59150624  0.551713\n",
      "   0.3454667   0.645633    0.69113696  0.23656602  0.19763273  0.47749403\n",
      "   0.44871685  0.6889366   0.19722313  0.54652095  0.78385204  0.27077478\n",
      "   0.37809744  0.6184258   0.7180793   0.668515    0.41037753  0.66117775\n",
      "   0.7608042   0.2725458   0.37298095  0.48651436  0.12045556  0.45023167\n",
      "   0.20460877  0.03894767  0.31262365  0.5484968   0.5777911   1.0174081\n",
      "   0.6346446   0.62053984  0.39000222  0.41286528  0.19251913  0.49667794\n",
      "   0.31695387  0.38984582  0.6600846   0.6056277   0.57986885  0.18202782\n",
      "   0.84443283  0.73005843  0.6279623   0.53149354  0.22838211  0.5036476\n",
      "   0.41126248  0.46336442  0.4838906   0.6039948   0.37424567  0.21310484\n",
      "   0.7436939   0.25394362  0.730116    0.51856226  0.8860754   0.6513413\n",
      "   0.6422844   0.435197    0.68303907  0.28017563  0.16485864  0.15876108\n",
      "   0.5300078   0.27730268  0.48025632  0.11659873  0.3739147   0.25508815\n",
      "  -0.11319381  0.4341702   0.6479287   0.61199665  0.32848758  0.47785428\n",
      "   0.17566556  0.65384144  0.41669577  0.29122803  0.47533497  0.381313\n",
      "   0.37474522  0.16591018  0.54989475  0.3977587   0.6345792   0.24333796\n",
      "   0.10176694  0.3873786   0.381873    0.47681245  0.29447722  0.39505053\n",
      "   0.04542893  0.36819533  0.4829806   0.11991173  0.27563632  0.45151693\n",
      "   0.14372092  0.6309587   0.62334454  0.73637146 -0.05029702  0.85532784\n",
      "   0.39069372  0.16938168  0.40148062  0.5478878   0.6673152   0.5345726\n",
      "   0.25511375  0.4351016   0.2148028   0.39610684  0.38403404  0.74641454\n",
      "   0.53856874  0.5187086   0.88100076  0.5049692   0.4662121   0.6228963\n",
      "   0.41370255  0.15409464  0.49604657  0.5511957   0.46235287  0.72076434\n",
      "   0.71623415  0.6738611   0.16774675  0.87692046  0.37178084  0.50134504\n",
      "   0.21809474  0.23133726  0.6899862   0.5461621   0.29694188  0.47906506\n",
      "   0.3575649   0.58734614  0.4315892   0.387075    0.05033848  0.4600453\n",
      "   0.5710362   0.38073814  0.05769351  0.42185572  0.6991409   0.3616788\n",
      "   0.3163429   0.6663057   0.25243944  0.19998369  0.347226    0.5632223\n",
      "   0.5248547   0.5245513   0.35959098  0.6598632   0.5456101   0.8662534\n",
      "   0.29952127]]\n",
      ">> argmax = [0]\n",
      ">> qvalue_evolution = [1.0334537  1.03325295 1.03438556 1.03264928 1.03320003 1.03235245\n",
      " 1.03147542 1.02979505 1.02702117 1.0256393 ]\n",
      ">> predict_movement_int = [0]\n",
      "[1.0256393]\n"
     ]
    }
   ],
   "source": [
    "my_agent = MyDeepQAgent(env.action_space)\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(), agentClass=MyDeepQAgent)\n",
    "res = runner.run(nb_episode=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81f7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFor chronics with id 000\n",
      "\t\t - cumulative reward: 923.063965\n",
      "\t\t - number of time steps completed: 10 / 10\n"
     ]
    }
   ],
   "source": [
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
