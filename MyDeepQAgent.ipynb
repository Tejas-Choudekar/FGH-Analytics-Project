{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97910269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import grid2op\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Converter import IdToAct\n",
    "from grid2op.Agent.agentWithConverter import AgentWithConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85e7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Dense, subtract, add\n",
    "from tensorflow.keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65dc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17db5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParam(object):\n",
    "    \"\"\"\n",
    "    A class to store the training parameters of the models. It was hard coded in the notebook 3.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 DECAY_RATE=0.9,\n",
    "                 BUFFER_SIZE=40000,\n",
    "                 MINIBATCH_SIZE=64,\n",
    "                 TOT_FRAME=3000000,\n",
    "                 EPSILON_DECAY=10000,\n",
    "                 MIN_OBSERVATION=50, #5000\n",
    "                 FINAL_EPSILON=1/300,  # have on average 1 random action per scenario of approx 287 time steps\n",
    "                 INITIAL_EPSILON=0.1,\n",
    "                 TAU=0.01,\n",
    "                 ALPHA=1,\n",
    "                 NUM_FRAMES=1,\n",
    "    ):\n",
    "        print('TrainingParam __init__')\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.MINIBATCH_SIZE = MINIBATCH_SIZE\n",
    "        self.TOT_FRAME = TOT_FRAME\n",
    "        self.EPSILON_DECAY = EPSILON_DECAY\n",
    "        self.MIN_OBSERVATION = MIN_OBSERVATION   # 5000\n",
    "        self.FINAL_EPSILON = FINAL_EPSILON  # have on average 1 random action per scenario of approx 287 time steps\n",
    "        self.INITIAL_EPSILON = INITIAL_EPSILON\n",
    "        self.TAU = TAU\n",
    "        self.NUM_FRAMES = NUM_FRAMES\n",
    "        self.ALPHA = ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fbb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "\n",
    "    '''\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        print('ReplayBuffer add')\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        if np.any(~np.isfinite(s)) or np.any(~np.isfinite(s2)):\n",
    "            # TODO proper handling of infinite values somewhere !!!!\n",
    "            return\n",
    "\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        print('ReplayBuffer size')\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        print('ReplayBuffer sample')\n",
    "\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        print('ReplayBuffer clear')\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcdbba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class RLQvalue(object):\n",
    "    \"\"\"\n",
    "    This class aims at representing the Q value (or more in case of SAC) parametrization by\n",
    "    a neural network.\n",
    "\n",
    "    It is composed of 2 different networks:\n",
    "    - model: which is the main model\n",
    "    - target_model: which has the same architecture and same initial weights as \"model\" but is updated less frequently\n",
    "      to stabilize training\n",
    "\n",
    "    It has basic methods to make predictions, to train the model, and train the target model.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=1e-5,\n",
    "                 training_param=TrainingParam()):\n",
    "        # TODO add more flexibilities when building the deep Q networks, with a \"NNParam\" for example.\n",
    "        self.action_size = action_size\n",
    "        self.observation_size = observation_size\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.qvalue_evolution = np.zeros((0,))\n",
    "        self.training_param = training_param\n",
    "\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "    \n",
    "    '''\n",
    "    def construct_q_network(self):\n",
    "        print('RLQvalue construct_q_network')\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "    '''\n",
    "\n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        rand_val = np.random.random(data.shape[0])\n",
    "        print(f'>> rand_val = {rand_val}')\n",
    "        q_actions = self.model.predict(data)\n",
    "        print(f'>> q_actions = {q_actions}')\n",
    "        opt_policy = np.argmax(np.abs(q_actions), axis=-1)\n",
    "        print(f'>> argmax = {opt_policy}')\n",
    "        opt_policy[rand_val < epsilon] = np.random.randint(0, self.action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        \n",
    "        self.qvalue_evolution = np.concatenate((self.qvalue_evolution, q_actions[0, opt_policy]))\n",
    "        print(f'>> qvalue_evolution = {self.qvalue_evolution}')\n",
    "        #print(f'>> opt_policy = {opt_policy}')\n",
    "        #print(f'>> q_actions = {q_actions[0, opt_policy]}')\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "    \n",
    "    '''\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        print('RLQvalue train')\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        targets = self.model.predict(s_batch)\n",
    "        fut_action = self.target_model.predict(s2_batch)\n",
    "        targets[:, a_batch] = r_batch\n",
    "        targets[d_batch, a_batch[d_batch]] += self.training_param.DECAY_RATE * np.max(fut_action[d_batch], axis=-1)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "        return np.all(np.isfinite(loss))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_path_model(path, name=None):\n",
    "        print('RLQvalue _get_path_model')\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue save_network')\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model.save('{}.{}'.format(path_model, ext))\n",
    "        self.target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    \n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue load_network')\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model = load_model('{}.{}'.format(path_model, ext))\n",
    "        self.target_model = load_model('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        print('RLQvalue target_train')\n",
    "        # nothing has changed from the original implementation\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = self.training_param.TAU * model_weights[i] + (1 - self.training_param.TAU) * \\\n",
    "                                      target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0abbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class DuelQ(RLQvalue):\n",
    "    \"\"\"Constructs the desired duelling deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=0.00001,\n",
    "                 training_param=TrainingParam()):\n",
    "        ## print('DuelQ __init__')\n",
    "        RLQvalue.__init__(self, action_size, observation_size, learning_rate, training_param)\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        # The inputs and outputs size have changed, as well as replacing the convolution by dense layers.\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape=(self.observation_size*self.training_param.NUM_FRAMES,))\n",
    "        \n",
    "        lay1 = Dense(self.observation_size*self.training_param.NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(self.observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*self.action_size)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(self.action_size)(lay3)\n",
    "        advantage = Dense(self.action_size)(fc1)\n",
    "        fc2 = Dense(self.action_size)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1) )\n",
    "        mn_ = meaner(advantage)\n",
    "        tmp = subtract([advantage, mn_])\n",
    "        policy = add([tmp, value])\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "        print(\"Successfully constructed networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bad922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingParam __init__\n"
     ]
    }
   ],
   "source": [
    "class MyDeepQAgent(AgentWithConverter):\n",
    "    \n",
    "    ## 1*0^-5 = 0.00001\n",
    "    def __init__(self, action_space, mode=\"DDQN\", learning_rate=1e-5, training_param=TrainingParam()):     \n",
    "        \n",
    "        ## Handle only vectors, and the type of action_space is GridObjects\n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(training_param.BUFFER_SIZE)\n",
    "\n",
    "        # compare to original implementation, i don't know the observation space size.\n",
    "        # Because it depends on the component of the observation we want to look at. So these neural network will\n",
    "        # be initialized the first time an observation is observe.\n",
    "        self.deep_q = None\n",
    "        self.mode = mode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_param = training_param\n",
    "    \n",
    "    def convert_obs(self, observation):\n",
    "        ## print(f'>> convert_obs = {np.concatenate((observation.rho, observation.line_status, observation.topo_vect))}')\n",
    "        return np.concatenate((observation.rho, observation.line_status, observation.topo_vect))\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        ## print(f'>> transformed_observation = {transformed_observation}')\n",
    "        if self.deep_q is None:\n",
    "            self.init_deep_q(transformed_observation)\n",
    "        \n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation.reshape(1, -1), epsilon=0.0)\n",
    "        print(f'>> predict_movement_int = {predict_movement_int}')\n",
    "        print(*_)\n",
    "        return int(predict_movement_int)\n",
    "\n",
    "    def init_deep_q(self, transformed_observation):\n",
    "        if self.deep_q is None:\n",
    "            # the first time an observation is observed, I set up the neural network with the proper dimensions.\n",
    "            if self.mode == \"DQN\":\n",
    "                cls = DeepQ\n",
    "            elif self.mode == \"DDQN\":\n",
    "                cls = DuelQ\n",
    "            elif self.mode == \"SAC\":\n",
    "                cls = SAC\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown neural network named \\\"{}\\\". Supported types are \\\"DQN\\\", \\\"DDQN\\\" and \"\n",
    "                                   \"\\\"SAC\\\"\".format(self.mode))\n",
    "            self.deep_q = cls(self.action_space.size(), observation_size=transformed_observation.shape[-1], learning_rate=self.learning_rate)\n",
    "            print(f'>> action_size = {self.deep_q.action_size}, observation_size = {self.deep_q.observation_size}, learning_rate_ = {self.deep_q.learning_rate_}, qvalue_evolution = {self.deep_q.qvalue_evolution}, training_param = {self.deep_q.training_param}, model = {self.deep_q.model}, target_model = {self.deep_q.target_model}')\n",
    "            print(f'>> self.deep_q = {self.deep_q}')\n",
    "            \n",
    "    '''\n",
    "    def load_network(self, path):\n",
    "        print('MyDeepQAgent load_network')\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473fe712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully constructed networks.\n",
      ">> action_size = 451, observation_size = 96, learning_rate_ = 1e-05, qvalue_evolution = [], training_param = <__main__.TrainingParam object at 0x7fb9ee66fe20>, model = <keras.engine.functional.Functional object at 0x7fb9ef391b80>, target_model = <keras.engine.functional.Functional object at 0x7fb9eeff2850>\n",
      ">> self.deep_q = <__main__.DuelQ object at 0x7fb9eefde5e0>\n",
      ">> rand_val = [0.31292105]\n",
      "1/1 [==============================] - 0s 85ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 14:51:18.591893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> q_actions = [[-0.21707192 -0.3266574  -0.33051664 -0.0687148  -0.21444835 -0.22947392\n",
      "  -0.42168558 -0.29451928 -0.55095696 -0.23684058 -0.14569393 -0.56864923\n",
      "  -0.35897627 -0.08650753 -0.40262848 -0.3339302  -0.26194698 -0.35521877\n",
      "  -0.43239993 -0.3855791  -0.38429844 -0.22488365 -0.2646321  -0.35977912\n",
      "  -0.24205174 -0.39300185 -0.27291593 -0.33653688 -0.06619361 -0.38505754\n",
      "  -0.38732278 -0.2057418  -0.3259206  -0.16456042 -0.14095753 -0.5245169\n",
      "  -0.386196   -0.2889656  -0.5852301  -0.24179494 -0.4951021  -0.40860963\n",
      "  -0.40560594 -0.2738433  -0.46450445 -0.28493422 -0.4076653  -0.36678803\n",
      "  -0.17506441 -0.41046613 -0.29346982 -0.3597146  -0.3254201  -0.32701325\n",
      "  -0.41525978 -0.3849613  -0.33242655 -0.4870559  -0.2728156  -0.36278653\n",
      "  -0.3980261  -0.39003712 -0.22230092 -0.23223585 -0.46817344 -0.209116\n",
      "  -0.43943396 -0.31179953 -0.40656173 -0.29915306 -0.31598338 -0.20944278\n",
      "  -0.10257858 -0.20699224 -0.43247244 -0.26575372 -0.3611637  -0.2173027\n",
      "  -0.49998438 -0.49821424 -0.5087255  -0.31842488 -0.29366997 -0.3450462\n",
      "  -0.14009507 -0.43483672 -0.3874737  -0.30658206 -0.2881298  -0.32915786\n",
      "  -0.16685866 -0.40935233 -0.51887316 -0.27473626 -0.29528007 -0.24033798\n",
      "  -0.15869835 -0.3223307  -0.51707524 -0.42365667 -0.40389022 -0.20485935\n",
      "  -0.27259818 -0.2605448  -0.30661216 -0.24285164 -0.3455083  -0.34870797\n",
      "  -0.45669055 -0.38008726 -0.4048983  -0.3199327  -0.3070276  -0.48900157\n",
      "  -0.25690186 -0.5326337  -0.34985346 -0.29396552 -0.47636962 -0.2318909\n",
      "  -0.2426765  -0.09347886 -0.19738373 -0.2290324  -0.2773859  -0.14029907\n",
      "  -0.40421054 -0.4162511  -0.3324543  -0.30121922 -0.4385376  -0.26090482\n",
      "  -0.4042065  -0.24585968 -0.57281137 -0.18195494 -0.37885162 -0.3004811\n",
      "  -0.19212691 -0.3549666  -0.41251493 -0.20035167 -0.0741276  -0.27268708\n",
      "  -0.42027694 -0.3262991  -0.40165687 -0.4675969  -0.5417023  -0.37452933\n",
      "  -0.3104777  -0.5321277  -0.27622676 -0.26819068 -0.2087082  -0.3508293\n",
      "  -0.20715784 -0.32634857 -0.32771185 -0.13929524 -0.43740404 -0.45098966\n",
      "  -0.06405279 -0.64527094 -0.34977168 -0.40127406 -0.30457824 -0.27619177\n",
      "  -0.45674413 -0.3313245  -0.5075673  -0.29828894 -0.44035798 -0.17187469\n",
      "  -0.1738041  -0.07457076 -0.43677974 -0.3215299  -0.4200902  -0.19379728\n",
      "  -0.26808485 -0.39967433 -0.44607148 -0.06240419 -0.09633856 -0.27673447\n",
      "  -0.28009856 -0.55242693 -0.11106333 -0.44502026 -0.4293899  -0.49419928\n",
      "  -0.5562538  -0.43593225 -0.31121373 -0.2878793  -0.42689672 -0.29272547\n",
      "  -0.22971031 -0.33722684 -0.34877264 -0.38398507 -0.40389413 -0.08754267\n",
      "  -0.37426883 -0.4978983  -0.31708357 -0.3255264  -0.20755157 -0.45515186\n",
      "  -0.2186588  -0.50345355 -0.3341001  -0.34749436 -0.39538082 -0.27748263\n",
      "  -0.35492507 -0.46187785 -0.2925295  -0.42348742 -0.31937835 -0.2473906\n",
      "  -0.28253853 -0.06585962 -0.4717213  -0.29839033 -0.39523318 -0.601889\n",
      "  -0.4846562  -0.35843045 -0.42916426 -0.10718507 -0.2967788  -0.3209748\n",
      "  -0.2104012  -0.1979222  -0.2529893  -0.2627141  -0.09716977 -0.41615614\n",
      "  -0.14031851 -0.42072564 -0.3471604  -0.22420156 -0.4729072  -0.24775542\n",
      "  -0.54915476 -0.55233467 -0.20027919 -0.34652013 -0.2936353  -0.5658131\n",
      "  -0.3915611  -0.27001137 -0.2758251  -0.0605911  -0.43807045 -0.16078667\n",
      "  -0.2582292  -0.38358954 -0.5271219  -0.3881967  -0.17357959 -0.35863653\n",
      "  -0.36362782 -0.24625519 -0.4597787  -0.40311193 -0.5043184  -0.41677168\n",
      "  -0.22310287 -0.10827754 -0.24554089 -0.29665563 -0.322488   -0.313843\n",
      "  -0.27691114 -0.4727494  -0.24468996 -0.39004678 -0.46626568 -0.41335362\n",
      "  -0.2509132  -0.1816048  -0.39421886 -0.36089638 -0.28951895 -0.35765755\n",
      "  -0.22676468 -0.32289836 -0.3296577  -0.4943238  -0.17892946 -0.29103377\n",
      "  -0.2815584  -0.17012464 -0.16960399 -0.27784237 -0.13680927 -0.46615255\n",
      "  -0.3669389  -0.48336855 -0.46806878 -0.21358988 -0.22148845 -0.36542436\n",
      "  -0.31461248 -0.338009   -0.2235885  -0.15541495 -0.15129602 -0.30168694\n",
      "  -0.18893924 -0.14470531 -0.32296577 -0.31737578 -0.27860224 -0.27122182\n",
      "  -0.2950229  -0.3389761  -0.4725368  -0.28590384 -0.4550036  -0.25719178\n",
      "  -0.2893426  -0.2630589  -0.15464072 -0.1450898  -0.34914124 -0.47755486\n",
      "  -0.37350693 -0.3882259  -0.3312188  -0.38085997 -0.27434477 -0.50555754\n",
      "  -0.17377242 -0.3139344  -0.33643588 -0.18969685 -0.3391492  -0.2685765\n",
      "  -0.43747532 -0.1790601  -0.26933926 -0.17749721 -0.18587245 -0.48573577\n",
      "  -0.44672132 -0.5586237  -0.2930216  -0.31419855 -0.57655483 -0.4728698\n",
      "  -0.39326316 -0.2649902  -0.42364386 -0.3832761  -0.35352036 -0.3766004\n",
      "  -0.17833824 -0.14707251 -0.43361697 -0.37119305 -0.6049198  -0.23049127\n",
      "  -0.18724947 -0.32103747 -0.36547172 -0.4392194  -0.10497682 -0.17187685\n",
      "  -0.18815048 -0.21007788 -0.5282132  -0.13779004 -0.2909749  -0.44765845\n",
      "  -0.37986988 -0.40167707 -0.24872419 -0.07368864 -0.07505809 -0.19860245\n",
      "  -0.2886496  -0.36846626 -0.12486617 -0.4242066  -0.35270286 -0.45175248\n",
      "  -0.19023912 -0.46636674 -0.05351716 -0.35530066 -0.32018274 -0.39560947\n",
      "  -0.4591536  -0.24208324 -0.28948957 -0.3166353  -0.22467525 -0.3316008\n",
      "  -0.25551334 -0.24220102 -0.41258672 -0.33266464 -0.195134   -0.1338308\n",
      "  -0.26224896 -0.27130508 -0.51358277 -0.3861331  -0.2073816  -0.53287613\n",
      "  -0.31922063 -0.2811049  -0.5616817  -0.35469142 -0.38234133 -0.2682231\n",
      "  -0.18266863 -0.36903358 -0.21567899 -0.21771407 -0.10773994 -0.26146877\n",
      "  -0.382198   -0.24643543 -0.2703209  -0.3235086  -0.22143511 -0.5266976\n",
      "  -0.33358967 -0.24670628 -0.18676077 -0.25606632 -0.38669452 -0.32575604\n",
      "  -0.40703994 -0.3208361  -0.35160694 -0.2256234  -0.3541978  -0.42475742\n",
      "  -0.38938746 -0.34260842 -0.22270921 -0.38781676 -0.2949064  -0.39426824\n",
      "  -0.35730377]]\n",
      ">> argmax = [163]\n",
      ">> qvalue_evolution = [-0.64527094]\n",
      ">> predict_movement_int = [163]\n",
      "[-0.64527094]\n"
     ]
    }
   ],
   "source": [
    "my_agent = MyDeepQAgent(env.action_space)\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(), agentClass=MyDeepQAgent)\n",
    "res = runner.run(nb_episode=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81f7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFor chronics with id 000\n",
      "\t\t - cumulative reward: -10.000000\n",
      "\t\t - number of time steps completed: 1 / 10\n"
     ]
    }
   ],
   "source": [
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
