{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97910269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import grid2op\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.Converter import IdToAct\n",
    "from grid2op.Agent.agentWithConverter import AgentWithConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85e7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Dense, subtract, add\n",
    "from tensorflow.keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65dc28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = 157\n",
      "observation space = 455\n"
     ]
    }
   ],
   "source": [
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)\n",
    "obs = env.reset()\n",
    "\n",
    "print(f'action space = {env.action_space.size()}')\n",
    "print(f'observation space = {obs.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17db5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParam(object):\n",
    "    \"\"\"\n",
    "    A class to store the training parameters of the models. It was hard coded in the notebook 3.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 DECAY_RATE=0.9,\n",
    "                 BUFFER_SIZE=40000,\n",
    "                 MINIBATCH_SIZE=64,\n",
    "                 TOT_FRAME=3000000,\n",
    "                 EPSILON_DECAY=10000,\n",
    "                 MIN_OBSERVATION=50, #5000\n",
    "                 FINAL_EPSILON=1/300,  # have on average 1 random action per scenario of approx 287 time steps\n",
    "                 INITIAL_EPSILON=0.1,\n",
    "                 TAU=0.01,\n",
    "                 ALPHA=1,\n",
    "                 NUM_FRAMES=1,\n",
    "    ):\n",
    "        print(f'## TrainingParam ## __init__ ##')\n",
    "        self.DECAY_RATE = DECAY_RATE\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.MINIBATCH_SIZE = MINIBATCH_SIZE\n",
    "        self.TOT_FRAME = TOT_FRAME\n",
    "        self.EPSILON_DECAY = EPSILON_DECAY\n",
    "        self.MIN_OBSERVATION = MIN_OBSERVATION   # 5000\n",
    "        self.FINAL_EPSILON = FINAL_EPSILON  # have on average 1 random action per scenario of approx 287 time steps\n",
    "        self.INITIAL_EPSILON = INITIAL_EPSILON\n",
    "        self.TAU = TAU\n",
    "        self.NUM_FRAMES = NUM_FRAMES\n",
    "        self.ALPHA = ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8fbb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        print(f'## ReplayBuffer ## __init__ ##')\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "\n",
    "    '''\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        print('ReplayBuffer add')\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        if np.any(~np.isfinite(s)) or np.any(~np.isfinite(s2)):\n",
    "            # TODO proper handling of infinite values somewhere !!!!\n",
    "            return\n",
    "\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        print('ReplayBuffer size')\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        print('ReplayBuffer sample')\n",
    "\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        print('ReplayBuffer clear')\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcdbba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## TrainingParam ## __init__ ##\n"
     ]
    }
   ],
   "source": [
    "class RLQvalue(object):\n",
    "    \"\"\"\n",
    "    This class aims at representing the Q value (or more in case of SAC) parametrization by\n",
    "    a neural network.\n",
    "\n",
    "    It is composed of 2 different networks:\n",
    "    - model: which is the main model\n",
    "    - target_model: which has the same architecture and same initial weights as \"model\" but is updated less frequently\n",
    "      to stabilize training\n",
    "\n",
    "    It has basic methods to make predictions, to train the model, and train the target model.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=1e-5,\n",
    "                 training_param=TrainingParam()):\n",
    "        # TODO add more flexibilities when building the deep Q networks, with a \"NNParam\" for example.\n",
    "        print(f'## RLQvalue ## __init__ ##')\n",
    "        self.action_size = action_size\n",
    "        self.observation_size = observation_size\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.qvalue_evolution = np.zeros((0,))\n",
    "        self.training_param = training_param\n",
    "\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "    \n",
    "    '''\n",
    "    def construct_q_network(self):\n",
    "        print('RLQvalue construct_q_network')\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "    '''\n",
    "\n",
    "    def predict_movement(self, data, epsilon):\n",
    "        print(f'## RLQvalue ## predict_movement ##')\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        rand_val = np.random.random(data.shape[0])\n",
    "        #print(f'>> rand_val = {rand_val}')\n",
    "        q_actions = self.model.predict(data)\n",
    "        #print(f'>> q_actions = {q_actions}')\n",
    "        opt_policy = np.argmax(np.abs(q_actions), axis=-1)\n",
    "        #print(f'>> argmax = {opt_policy}')\n",
    "        opt_policy[rand_val < epsilon] = np.random.randint(0, self.action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        \n",
    "        self.qvalue_evolution = np.concatenate((self.qvalue_evolution, q_actions[0, opt_policy]))\n",
    "        print(f'>> qvalue_evolution = {self.qvalue_evolution}')\n",
    "        #print(f'>> opt_policy = {opt_policy}')\n",
    "        #print(f'>> q_actions = {q_actions[0, opt_policy]}')\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "    \n",
    "    '''\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        print('RLQvalue train')\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        targets = self.model.predict(s_batch)\n",
    "        fut_action = self.target_model.predict(s2_batch)\n",
    "        targets[:, a_batch] = r_batch\n",
    "        targets[d_batch, a_batch[d_batch]] += self.training_param.DECAY_RATE * np.max(fut_action[d_batch], axis=-1)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "        return np.all(np.isfinite(loss))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_path_model(path, name=None):\n",
    "        print('RLQvalue _get_path_model')\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue save_network')\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model.save('{}.{}'.format(path_model, ext))\n",
    "        self.target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    \n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        print('RLQvalue load_network')\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self._get_path_model(path, name)\n",
    "        self.model = load_model('{}.{}'.format(path_model, ext))\n",
    "        self.target_model = load_model('{}.{}'.format(path_target_model, ext))\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        print('RLQvalue target_train')\n",
    "        # nothing has changed from the original implementation\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = self.training_param.TAU * model_weights[i] + (1 - self.training_param.TAU) * \\\n",
    "                                      target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b0abbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## TrainingParam ## __init__ ##\n"
     ]
    }
   ],
   "source": [
    "class DuelQ(RLQvalue):\n",
    "    \"\"\"Constructs the desired duelling deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, observation_size,\n",
    "                 learning_rate=0.00001,\n",
    "                 training_param=TrainingParam()):\n",
    "        print(f'## DuelQ ## __init__ ##')\n",
    "        RLQvalue.__init__(self, action_size, observation_size, learning_rate, training_param)\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        print(f'## DuelQ ## construct_q_network ##')\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        # The inputs and outputs size have changed, as well as replacing the convolution by dense layers.\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        #print(f'observation_size = {self.observation_size}')\n",
    "        #print(f'NUM_FRAMES = {self.training_param.NUM_FRAMES}')\n",
    "        input_layer = Input(shape=(self.observation_size*self.training_param.NUM_FRAMES,))\n",
    "        #print(f'input_layer = {input_layer}')\n",
    "        \n",
    "        lay1 = Dense(self.observation_size*self.training_param.NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(self.observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*self.action_size)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(self.action_size)(lay3)\n",
    "        advantage = Dense(self.action_size)(fc1)\n",
    "        fc2 = Dense(self.action_size)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1) )\n",
    "        mn_ = meaner(advantage)\n",
    "        tmp = subtract([advantage, mn_])\n",
    "        policy = add([tmp, value])\n",
    "        \n",
    "        print(f'input_layer = {input_layer}')\n",
    "        print(f'policy = {policy}')\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate_))\n",
    "        print(\"Successfully constructed networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6bad922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## TrainingParam ## __init__ ##\n"
     ]
    }
   ],
   "source": [
    "class MyDeepQAgent(AgentWithConverter):\n",
    "    \n",
    "    ## 1*0^-5 = 0.00001\n",
    "    def __init__(self, action_space, mode=\"DDQN\", learning_rate=1e-5, training_param=TrainingParam()):     \n",
    "        print(f'## MyDeepQAgent ## __init__ ##')\n",
    "        \n",
    "        ## Handle only vectors, and the type of action_space is GridObjects\n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(training_param.BUFFER_SIZE)\n",
    "\n",
    "        # compare to original implementation, i don't know the observation space size.\n",
    "        # Because it depends on the component of the observation we want to look at. So these neural network will\n",
    "        # be initialized the first time an observation is observe.\n",
    "        self.deep_q = None\n",
    "        self.mode = mode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_param = training_param\n",
    "    \n",
    "    def convert_obs(self, observation):\n",
    "        print(f'## MyDeepQAgent ## convert_obs ##')\n",
    "        #print(f'rho = {observation.rho}')\n",
    "        #print(f'line_status = {observation.line_status}')\n",
    "        #print(f'topo_vect = {observation.topo_vect}')\n",
    "        convert_obs = np.concatenate((observation.rho, observation.line_status, observation.topo_vect))\n",
    "        \n",
    "        #print(f'>> convert_obs = {convert_obs}')\n",
    "        return np.concatenate((observation.rho, observation.line_status, observation.topo_vect))\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        print(f'## MyDeepQAgent ## my_act ##')\n",
    "        #print(f'>> transformed_observation = {transformed_observation}')\n",
    "        if self.deep_q is None:\n",
    "            self.init_deep_q(transformed_observation)\n",
    "        \n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation.reshape(1, -1), epsilon=0.0)\n",
    "        print(f'>> predict_movement_int = {predict_movement_int}')\n",
    "        #print(f'### {transformed_observation.reshape(1, -1)}')\n",
    "        return int(predict_movement_int)\n",
    "\n",
    "    def init_deep_q(self, transformed_observation):\n",
    "        print(f'## MyDeepQAgent ## init_deep_q ##')\n",
    "        if self.deep_q is None:\n",
    "            # the first time an observation is observed, I set up the neural network with the proper dimensions.\n",
    "            if self.mode == \"DQN\":\n",
    "                cls = DeepQ\n",
    "            elif self.mode == \"DDQN\":\n",
    "                cls = DuelQ\n",
    "            elif self.mode == \"SAC\":\n",
    "                cls = SAC\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown neural network named \\\"{}\\\". Supported types are \\\"DQN\\\", \\\"DDQN\\\" and \"\n",
    "                                   \"\\\"SAC\\\"\".format(self.mode))\n",
    "            \n",
    "            #print(f'action_space = {self.action_space.size()}')\n",
    "            #print(f'transformed_observation = {transformed_observation}')\n",
    "            #print(f'transformed_observation_size = {transformed_observation.shape[-1]}')\n",
    "            self.deep_q = cls(self.action_space.size(), observation_size=transformed_observation.shape[-1], learning_rate=self.learning_rate)\n",
    "            #print(f'>> action_size = {self.deep_q.action_size}, observation_size = {self.deep_q.observation_size}, learning_rate_ = {self.deep_q.learning_rate_}, qvalue_evolution = {self.deep_q.qvalue_evolution}, training_param = {self.deep_q.training_param}, model = {self.deep_q.model}, target_model = {self.deep_q.target_model}')\n",
    "            #print(f'>> self.deep_q = {self.deep_q}')\n",
    "            \n",
    "    '''\n",
    "    def load_network(self, path):\n",
    "        print('MyDeepQAgent load_network')\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "473fe712",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## MyDeepQAgent ## __init__ ##\n",
      "## ReplayBuffer ## __init__ ##\n",
      "## MyDeepQAgent ## __init__ ##\n",
      "## ReplayBuffer ## __init__ ##\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## MyDeepQAgent ## init_deep_q ##\n",
      "## DuelQ ## __init__ ##\n",
      "## RLQvalue ## __init__ ##\n",
      "## DuelQ ## construct_q_network ##\n",
      "input_layer = KerasTensor(type_spec=TensorSpec(shape=(None, 96), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\")\n",
      "policy = KerasTensor(type_spec=TensorSpec(shape=(None, 451), dtype=tf.float32, name=None), name='add_11/add:0', description=\"created by layer 'add_11'\")\n",
      "Successfully constructed networks.\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      ">> qvalue_evolution = [-1.104846]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936 -1.07553542]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936 -1.07553542\n",
      " -1.10344577]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936 -1.07553542\n",
      " -1.10344577 -1.07594597]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936 -1.07553542\n",
      " -1.10344577 -1.07594597 -1.10133815]\n",
      ">> predict_movement_int = [331]\n",
      "## MyDeepQAgent ## convert_obs ##\n",
      "## MyDeepQAgent ## my_act ##\n",
      "## RLQvalue ## predict_movement ##\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      ">> qvalue_evolution = [-1.104846   -1.07757115 -1.10298038 -1.07771957 -1.10143936 -1.07553542\n",
      " -1.10344577 -1.07594597 -1.10133815 -1.07465935]\n",
      ">> predict_movement_int = [331]\n"
     ]
    }
   ],
   "source": [
    "my_agent = MyDeepQAgent(env.action_space)\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(), agentClass=MyDeepQAgent)\n",
    "res = runner.run(nb_episode=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b81f7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFor chronics with id 000\n",
      "\t\t - cumulative reward: 923.046936\n",
      "\t\t - number of time steps completed: 10 / 10\n"
     ]
    }
   ],
   "source": [
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbe70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
