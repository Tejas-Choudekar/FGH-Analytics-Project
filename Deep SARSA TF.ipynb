{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9662984",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this notebook we will use a dedicated environment called \"l2rpn_neurips_2020_track1\", that has 36 substations. Grid2op comes with many different environments, with different problems etc. In this notebook, we will only mention and explain this specific environment.\n",
    "\n",
    "The approach in this notebook is a loss minimization approach. We have used SARSA algorithm with Q-learning network and this new approach is called Deep SARSA Algorithm.\n",
    "\n",
    "The algoritm takes current action $a$ with $\\epsilon$-greedy algorithm on current state $S$. The reward $r$ and next state $S'$ is observed.\n",
    "\n",
    "Later the current state $S$, current action $a$, reward $r$ and next state $S'$ is stored in buffer replay memory. The main idea behind the buffer replay memory is to train q-network on the experiences. The q-network will calculate q-values based on these experiences $Q(S,a)$.\n",
    "\n",
    "A copy of q-network is used as target network to calculate the q-values of next action $a'$ on next state $S'$ where the $a'$ is selected using $\\epsilon$-greedy algorithm. The idea behind this network is that it will predict next q-values $\\hat{Q}(S',a')$ based on next actions $a'$ predicted by $\\epsilon$-greedy algorithm and the next states $S'$. These q-values are then used to calculate next state-action values as $r+\\gamma.\\hat{Q}(S',a')$ which are then used to calculate loss function.\n",
    "\n",
    "The loss function that we used is mean square error loss (MSE) which is calculated as $L = \\frac{1}{|K|}\\sum_{i=1}^{|K|}[(r+\\gamma.\\hat{Q}(S',a'))-Q(S,a)]^2$ Deep SARSA Algorithm will try to minimise this loss by adjusting the weights of the q-network accordingly.\n",
    "\n",
    "The Structure of this notebook is as follows:\n",
    "1. Importing necessary Libraries\n",
    "2. Preprocess environment\n",
    "3. Process replay memory\n",
    "4. Create Deep SARSA Agent\n",
    "5. Evaluate Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1f49d",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import grid2op\n",
    "from grid2op.Exceptions import Grid2OpException\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "from grid2op.Converter import IdToAct\n",
    "from grid2op.MakeEnv import make\n",
    "from grid2op.Runner import Runner\n",
    "\n",
    "from l2rpn_baselines.utils.replayBuffer import ReplayBuffer\n",
    "from l2rpn_baselines.utils.trainingParam import TrainingParam\n",
    "from l2rpn_baselines.utils.save_log_gif import save_log_gif\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from l2rpn_baselines.utils.waring_msgs import _WARN_GPU_MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec778acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from grid2op.Chronics import MultifolderWithCache\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = True\n",
    "except ImportError:\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = False\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        import tensorflow as tf\n",
    "        import tensorflow.keras.optimizers as tfko\n",
    "        from tensorflow.keras.models import Sequential, Model\n",
    "        from tensorflow.keras.layers import Activation, Dense\n",
    "        from tensorflow.keras.layers import Input\n",
    "    _CAN_USE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _CAN_USE_TENSORFLOW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2baff10",
   "metadata": {},
   "source": [
    "## Setting up defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a12248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LOGS_DIR = \"./logs-eval/dsarsa_baseline\"\n",
    "DEFAULT_NB_EPISODE = 1\n",
    "DEFAULT_NB_PROCESS = 1\n",
    "DEFAULT_MAX_STEPS = -1\n",
    "DEFAULT_NAME = \"Deep_SARSA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cd5c4",
   "metadata": {},
   "source": [
    "## Below is the base class for the deep SARSA Agent \n",
    "This class allows to train and log the training of Deep SARSA algorithm.\n",
    "\n",
    "It derives from :class:`grid2op.Agent.AgentWithConverter` and as such implements the :func:`DeepSARSAAgent.convert_obs` and :func:`DeepSARSAAgent.my_act`\n",
    "\n",
    "It is suppose to be a Baseline, so it implements also the\n",
    "\n",
    "- :func:`DeepSARSAAgent.load`: to load the agent\n",
    "- :func:`DeepSARSAAgent.save`: to save the agent\n",
    "- :func:`DeepSARSAAgent.train`: to train the agent\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "filter_action_fun: ``callable``\n",
    "The function used to filter the action of the action space. See the documentation of grid2op:\n",
    ":class:`grid2op.Converter.IdToAct`\n",
    "`here <https://grid2op.readthedocs.io/en/v0.9.3/converter.html#grid2op.Converter.IdToAct>`_ for more information.\n",
    "\n",
    "replay_buffer: The experience replay buffer\n",
    "\n",
    "deep_sarsa: :class:`BaseDeepSARSA` \n",
    "        The neural network, represented as a :class:`BaseDeepSARSA` object.\n",
    "\n",
    "name: ``str``\n",
    "        The name of the Agent\n",
    "\n",
    "store_action: ``bool``\n",
    "        Whether you want to register which action your agent took or not. Saving the action can slow down a bit\n",
    "        the computation (less than 1%) but can help understand what your agent is doing during its learning process.\n",
    "\n",
    "dict_action: ``str``\n",
    "        The action taken by the agent, represented as a dictionnary. This can be useful to know which type of actions\n",
    "        is taken by your agent. Only filled if :attr:DeepSARSAAgent.store_action` is ``True`\n",
    "\n",
    "istraining: ``bool``\n",
    "        Whether you are training this agent or not. No more really used. Mainly used for backward compatibility.\n",
    "\n",
    "epsilon: ``float``\n",
    "        The epsilon greedy exploration parameter.\n",
    "\n",
    "nb_injection: ``int``\n",
    "        Number of action tagged as \"injection\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_voltage: ``int``\n",
    "        Number of action tagged as \"voltage\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_topology: ``int``\n",
    "        Number of action tagged as \"topology\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_redispatching: ``int``\n",
    "        Number of action tagged as \"redispatching\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_storage: ``int``\n",
    "        Number of action tagged as \"storage\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "        \n",
    "nb_curtail: ``int``\n",
    "        Number of action tagged as \"curtailment\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_do_nothing: ``int``\n",
    "        Number of action tagged as \"do_nothing\", *ie* when an action is not modifiying the state of the grid. See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "verbose: ``bool``\n",
    "        An effort will be made on the logging (outside of trensorboard) of the training. For now: verbose=True will\n",
    "        allow some printing on the command prompt, and verbose=False will drastically reduce the amount of information\n",
    "        printed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSARSAAgent(AgentWithConverter):\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 nn_archi,\n",
    "                 name=\"DeepSARSAAgent\",\n",
    "                 store_action=True,\n",
    "                 istraining=False,\n",
    "                 filter_action_fun=None,\n",
    "                 verbose=False,\n",
    "                 observation_space=None,\n",
    "                 **kwargs_converters):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct, **kwargs_converters)\n",
    "        self.filter_action_fun = filter_action_fun\n",
    "        if self.filter_action_fun is not None:\n",
    "            self.action_space.filter_action(self.filter_action_fun)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = None\n",
    "        self.__nb_env = None\n",
    "\n",
    "        self.deep_sarsa = None\n",
    "        self._training_param = None\n",
    "        self._tf_writer = None\n",
    "        self.name = name\n",
    "        self._losses = None\n",
    "        self.__graph_saved = False\n",
    "        self.store_action = store_action\n",
    "        self.dict_action = {}\n",
    "        self.istraining = istraining\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        # for tensorbaord\n",
    "        self._train_lr = None\n",
    "\n",
    "        self._reset_num = None\n",
    "\n",
    "        self._max_iter_env_ = 1000000\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = 0.\n",
    "\n",
    "        # action type\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_curtail = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for over sampling the hard scenarios\n",
    "        self._prev_obs_num = 0\n",
    "        self._time_step_lived = None\n",
    "        self._nb_chosen = None\n",
    "        self._proba = None\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        # neural network architecture\n",
    "        self._nn_archi = nn_archi\n",
    "\n",
    "        # observation tranformers\n",
    "        self._obs_as_vect = None\n",
    "        self._tmp_obs = None\n",
    "        self._indx_obs = None\n",
    "        self.verbose = verbose\n",
    "        if observation_space is None:\n",
    "            pass\n",
    "        else:\n",
    "            self.init_obs_extraction(observation_space)\n",
    "\n",
    "        # for the frequency of action type\n",
    "        self.current_ = 0\n",
    "        self.nb_ = 10\n",
    "        self._nb_this_time = np.zeros((self.nb_, 8), dtype=int)\n",
    "\n",
    "        #\n",
    "        self._vector_size = None\n",
    "        self._actions_per_ksteps = None\n",
    "        self._illegal_actions_per_ksteps = None\n",
    "        self._ambiguous_actions_per_ksteps = None\n",
    "\n",
    "    def _fill_vectors(self, training_param):\n",
    "        self._vector_size  = self.nb_ * training_param.update_tensorboard_freq\n",
    "        self._actions_per_ksteps = np.zeros((self._vector_size, self.action_space.size()), dtype=np.int)\n",
    "        self._illegal_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "        self._ambiguous_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "\n",
    "    # grid2op.Agent interface\n",
    "    def convert_obs(self, observation):\n",
    "        \"\"\"\n",
    "        Generic way to convert an observation. This transform it to a vector and the select the attributes that were\n",
    "        selected in :attr:`l2rpn_baselines.utils.NNParams.list_attr_obs` (that have been extracted once and for all\n",
    "        in the :attr:`DeepSARSAAgent._indx_obs` vector).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation: :class:`grid2op.Observation.BaseObservation`\n",
    "            The current observation sent by the environment\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _tmp_obs: ``numpy.ndarray``\n",
    "            The observation as vector with only the proper attribute selected (TODO scaling will be available\n",
    "            in future version)\n",
    "\n",
    "        \"\"\"\n",
    "        obs_as_vect = observation.to_vect()\n",
    "        self._tmp_obs[:] = obs_as_vect[self._indx_obs]\n",
    "        return self._tmp_obs\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        \"\"\"\n",
    "        This function will return the action (its id) selected by the underlying :attr:`DeepSARSAAgent.deep_sarsa` network.\n",
    "\n",
    "        Before being used, this method require that the :attr:`DeepSARSAAgent.deep_sarsa` is created. To that end a call\n",
    "        to :func:`DeepSARSAAgent.init_deep_sarsa` needs to have been performed (this is automatically done if you use\n",
    "        baseline we provide and their `evaluate` and `train` scripts).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transformed_observation: ``numpy.ndarray``\n",
    "            The observation, as transformed after :func:`DeepSARSAAgent.convert_obs`\n",
    "\n",
    "        reward: ``float``\n",
    "            The reward of the last time step. Ignored by this method. Here for retro compatibility with openAI\n",
    "            gym interface.\n",
    "\n",
    "        done: ``bool``\n",
    "            Whether the episode is over or not. This is not used, and is only present to be compliant with\n",
    "            open AI gym interface\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res: ``int``\n",
    "            The id the action taken.\n",
    "\n",
    "        \"\"\"\n",
    "        predict_movement_int, *_ = self.deep_sarsa.predict_movement(transformed_observation,\n",
    "                                                                epsilon=0.0,\n",
    "                                                                training=False)\n",
    "        res = int(predict_movement_int)\n",
    "        self._store_action_played(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_size(action_space, filter_fun, kwargs_converters):\n",
    "        \"\"\"\n",
    "        This function allows to get the size of the action space if we were to built a :class:`DeepSARSAAgent`\n",
    "        with this parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action_space: :class:`grid2op.ActionSpace`\n",
    "            The grid2op action space used.\n",
    "\n",
    "        filter_fun: ``callable``\n",
    "            see :attr:`DeepSARSAAgent.filter_fun` for more information\n",
    "\n",
    "        kwargs_converters: ``dict``\n",
    "            see the documentation of grid2op for more information:\n",
    "            `here <https://grid2op.readthedocs.io/en/v0.9.3/converter.html?highlight=idToAct#grid2op.Converter.IdToAct.init_converter>`_\n",
    "\n",
    "        \"\"\"\n",
    "        converter = IdToAct(action_space)\n",
    "        converter.init_converter(**kwargs_converters)\n",
    "        if filter_fun is not None:\n",
    "            converter.filter_action(filter_fun)\n",
    "        return converter.n\n",
    "\n",
    "    def init_obs_extraction(self, observation_space):\n",
    "        \"\"\"\n",
    "        This method should be called to initialize the observation (feed as a vector in the neural network)\n",
    "        from its description as a list of its attribute names.\n",
    "        \"\"\"\n",
    "        tmp = np.zeros(0, dtype=np.uint)  # TODO platform independant\n",
    "        for obs_attr_name in self._nn_archi.get_obs_attr():\n",
    "            beg_, end_, dtype_ = observation_space.get_indx_extract(obs_attr_name)\n",
    "            tmp = np.concatenate((tmp, np.arange(beg_, end_, dtype=np.uint)))\n",
    "        self._indx_obs = tmp\n",
    "        self._tmp_obs = np.zeros((1, tmp.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # baseline interface\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Part of the l2rpn_baselines interface, this function allows to read back a trained model, to continue the\n",
    "        training or to evaluate its performance for example.\n",
    "\n",
    "        **NB** To reload an agent, it must have exactly the same name and have been saved at the right location.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path where the agent has previously beens saved.\n",
    "\n",
    "        \"\"\"\n",
    "        # not modified compare to original implementation\n",
    "        tmp_me = os.path.join(path, self.name)\n",
    "        if not os.path.exists(tmp_me):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(tmp_me))\n",
    "        self._load_action_space(tmp_me)\n",
    "\n",
    "        # TODO handle case where training param class has been overidden\n",
    "        self._training_param = TrainingParam.from_json(os.path.join(tmp_me, \"training_params.json\".format(self.name)))\n",
    "        self.deep_sarsa = self._nn_archi.make_nn(self._training_param)\n",
    "        try:\n",
    "            self.deep_sarsa.load_network(tmp_me, name=self.name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to load the model located at \\\"{}\\\" with error \\n{}\".format(path, e))\n",
    "\n",
    "        for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "            conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "            if os.path.exists(conv_path):\n",
    "                setattr(self, nm_attr, np.load(file=conv_path))\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Part of the l2rpn_baselines interface, this allows to save a model. Its name is used at saving time. The\n",
    "        same name must be reused when loading it back.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path where to save the agent.\n",
    "\n",
    "        \"\"\"\n",
    "        if path is not None:\n",
    "            tmp_me = os.path.join(path, self.name)\n",
    "            if not os.path.exists(tmp_me):\n",
    "                os.mkdir(tmp_me)\n",
    "            nm_conv = \"action_space.npy\"\n",
    "            conv_path = os.path.join(tmp_me, nm_conv)\n",
    "            if not os.path.exists(conv_path):\n",
    "                self.action_space.save(path=tmp_me, name=nm_conv)\n",
    "\n",
    "            self._training_param.save_as_json(tmp_me, name=\"training_params.json\")\n",
    "            self._nn_archi.save_as_json(tmp_me, \"nn_architecture.json\")\n",
    "            self.deep_sarsa.save_network(tmp_me, name=self.name)\n",
    "\n",
    "            # TODO save the \"oversampling\" part, and all the other info\n",
    "            for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "                conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "                attr_ = getattr(self, nm_attr)\n",
    "                if attr_ is not None:\n",
    "                    np.save(arr=attr_, file=conv_path)\n",
    "\n",
    "    def train(self,\n",
    "              env,\n",
    "              iterations,\n",
    "              save_path,\n",
    "              logdir,\n",
    "              training_param=None):\n",
    "        \"\"\"\n",
    "        This function allows to train the baseline.\n",
    "\n",
    "        If `save_path` is not None, the the model is saved regularly, and also at the end of training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: :class:`grid2op.Environment.Environment` or :class:`grid2op.Environment.MultiEnvironment`\n",
    "            The environment used to train your model.\n",
    "\n",
    "        iterations: ``int``\n",
    "            The number of training iteration. NB when reloading a model, this is **NOT** the training steps that will\n",
    "            be used when re training. Indeed, if `iterations` is 1000 and the model was already trained for 750 time\n",
    "            steps, then when reloaded, the training will occur on 250 (=1000 - 750) time steps only.\n",
    "\n",
    "        save_path: ``str``\n",
    "            Location at which to save the model\n",
    "\n",
    "        logdir: ``str``\n",
    "            Location at which tensorboard related information will be kept.\n",
    "\n",
    "        training_param: :class:`l2rpn_baselines.utils.TrainingParam`\n",
    "            The meta parameters for the training procedure. This is currently ignored if the model is reloaded (in that\n",
    "            case the parameters used when first created will be used)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "\n",
    "        self._train_lr = training_param.lr\n",
    "\n",
    "        if self._training_param is None:\n",
    "            self._training_param = training_param\n",
    "        else:\n",
    "            training_param = self._training_param\n",
    "        self._init_deep_sarsa(self._training_param, env)\n",
    "        self._fill_vectors(self._training_param)\n",
    "\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "        # efficient reading of the data (read them by chunk of roughly 1 day\n",
    "        nb_ts_one_day = 24 * 60 / 5  # number of time steps per day\n",
    "        self._set_chunk(env, nb_ts_one_day)\n",
    "\n",
    "        # Create file system related vars\n",
    "        if save_path is not None:\n",
    "            save_path = os.path.abspath(save_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if logdir is not None:\n",
    "            logpath = os.path.join(logdir, self.name)\n",
    "            self._tf_writer = tf.summary.create_file_writer(logpath, name=self.name)\n",
    "        else:\n",
    "            logpath = None\n",
    "            self._tf_writer = None\n",
    "        UPDATE_FREQ = training_param.update_tensorboard_freq  # update tensorboard every \"UPDATE_FREQ\" steps\n",
    "        SAVING_NUM = training_param.save_model_each\n",
    "\n",
    "        if hasattr(env, \"nb_env\"):\n",
    "            nb_env = env.nb_env\n",
    "            warnings.warn(\"Training using {} environments\".format(nb_env))\n",
    "            self.__nb_env = nb_env\n",
    "        else:\n",
    "            self.__nb_env = 1\n",
    "\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "        training_step = self._training_param.last_step\n",
    "        self.epsilon = self._training_param.initial_epsilon\n",
    "\n",
    "        # now the number of alive frames and total reward depends on the \"underlying environment\". It is vector instead\n",
    "        # of scalar\n",
    "        alive_frame, total_reward = self._init_global_train_loop()\n",
    "        reward, done = self._init_local_train_loop()\n",
    "        epoch_num = 0\n",
    "        self._losses = np.zeros(iterations)\n",
    "        alive_frames = np.zeros(iterations)\n",
    "        total_rewards = np.zeros(iterations)\n",
    "        new_state = None\n",
    "        self._reset_num = 0\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = env.reward_range[1]\n",
    "\n",
    "        # action types\n",
    "        # injection, voltage, topology, line, redispatching = action.get_types()\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_curtail = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for non uniform random sampling of the scenarios\n",
    "        th_size = None\n",
    "        self._prev_obs_num = 0\n",
    "        if self.__nb_env == 1:\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    th_size = env.chronics_handler.real_data.cache_size\n",
    "            if th_size is None:\n",
    "                th_size = len(env.chronics_handler.real_data.subpaths)\n",
    "\n",
    "            # number of time step lived per possible scenarios\n",
    "            if self._time_step_lived is None or self._time_step_lived.shape[0] != th_size:\n",
    "                self._time_step_lived = np.zeros(th_size, dtype=np.uint64)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._nb_chosen is None or self._nb_chosen.shape[0] != th_size:\n",
    "                self._nb_chosen = np.zeros(th_size, dtype=np.uint)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._proba is None or self._proba.shape[0] != th_size:\n",
    "                self._proba = np.ones(th_size, dtype=np.float64)\n",
    "\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        with tqdm(total=iterations - training_step, disable=not self.verbose) as pbar:\n",
    "            while training_step < iterations:\n",
    "                # reset or build the environment\n",
    "                initial_state = self._need_reset(env, training_step, epoch_num, done, new_state)\n",
    "\n",
    "                # Slowly decay the exploration parameter epsilon\n",
    "                # if self.epsilon > training_param.FINAL_EPSILON:\n",
    "                self.epsilon = self._training_param.get_next_epsilon(current_step=training_step)\n",
    "\n",
    "                # then we need to predict the next moves. Agents have been adapted to predict a batch of data\n",
    "                pm_i, pq_v, act = self._next_move(initial_state, self.epsilon, training_step)\n",
    "                EPS = self.epsilon\n",
    "\n",
    "                # todo store the illegal / ambiguous / ... actions\n",
    "                reward, done = self._init_local_train_loop()\n",
    "                if self.__nb_env == 1:\n",
    "                    act = act[0]\n",
    "\n",
    "                temp_observation_obj, temp_reward, temp_done, info = env.step(act)\n",
    "                if self.__nb_env == 1:\n",
    "                    temp_observation_obj = [temp_observation_obj]\n",
    "                    temp_reward = np.array([temp_reward], dtype=np.float32)\n",
    "                    temp_done = np.array([temp_done], dtype=np.bool)\n",
    "                    info = [info]\n",
    "\n",
    "                new_state = self._convert_obs_train(temp_observation_obj)\n",
    "                self._updage_illegal_ambiguous(training_step, info)\n",
    "                done, reward, total_reward, alive_frame, epoch_num \\\n",
    "                    = self._update_loop(done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num)\n",
    "\n",
    "                # update the replay buffer\n",
    "                self._store_new_state(initial_state, pm_i, reward, done, new_state)\n",
    "                \n",
    "                # now train the model\n",
    "                if not self._train_model(training_step):\n",
    "                    # infinite loss in this case\n",
    "                    raise RuntimeError(\"ERROR INFINITE LOSS\")\n",
    "\n",
    "                # Save the network every 1000 iterations\n",
    "                if training_step % SAVING_NUM == 0 or training_step == iterations - 1:\n",
    "                    self.save(save_path)\n",
    "\n",
    "                # save some information to tensorboard\n",
    "                alive_frames[epoch_num] = np.mean(alive_frame)\n",
    "                total_rewards[epoch_num] = np.mean(total_reward)\n",
    "                self._store_action_played_train(training_step, pm_i)\n",
    "                self._save_tensorboard(training_step, epoch_num, UPDATE_FREQ, total_rewards, alive_frames)\n",
    "                training_step += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        self.save(save_path)\n",
    "\n",
    "    # auxiliary functions\n",
    "    # two below function: to train with multiple environments\n",
    "    def _convert_obs_train(self, observations):\n",
    "        \"\"\" create the observations that are used for training.\"\"\"\n",
    "        if self._obs_as_vect is None:\n",
    "            size_obs = self.convert_obs(observations[0]).shape[1]\n",
    "            self._obs_as_vect = np.zeros((self.__nb_env, size_obs), dtype=np.float32)\n",
    "\n",
    "        for i, obs in enumerate(observations):\n",
    "            self._obs_as_vect[i, :] = self.convert_obs(obs).reshape(-1)\n",
    "        return self._obs_as_vect\n",
    "\n",
    "    def _create_action_if_not_registered(self, action_int):\n",
    "        \"\"\"make sure that `action_int` is present in dict_action\"\"\"\n",
    "        if action_int not in self.dict_action:\n",
    "            act = self.action_space.all_actions[action_int]\n",
    "            is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_dn, is_curtail = \\\n",
    "                False, False, False, False, False, False, False, False\n",
    "            try:\n",
    "                # feature unavailble in grid2op <= 0.9.2\n",
    "                try:\n",
    "                    # storage introduced in grid2op 1.5.0 so if below it is not supported\n",
    "                    is_inj, is_volt, is_topo, is_line_status, is_redisp = act.get_types()\n",
    "                except ValueError as exc_:\n",
    "                    try:\n",
    "                        is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage = act.get_types()\n",
    "                    except ValueError as exc_:\n",
    "                        is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail = act.get_types()\n",
    "\n",
    "                is_dn = (not is_inj) and (not is_volt) and (not is_topo) and (not is_line_status) and (not is_redisp)\n",
    "                is_dn = is_dn and (not is_storage)\n",
    "                is_dn = is_dn and (not is_curtail)\n",
    "            except Exception as exc_:\n",
    "                pass\n",
    "\n",
    "            self.dict_action[action_int] = [0, act,\n",
    "                                            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail, is_dn)]\n",
    "\n",
    "    def _store_action_played(self, action_int):\n",
    "        \"\"\"if activated, this function will store the action taken by the agent.\"\"\"\n",
    "        if self.store_action:\n",
    "            self._create_action_if_not_registered(action_int)\n",
    "\n",
    "            self.dict_action[action_int][0] += 1\n",
    "            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail, is_dn) = self.dict_action[action_int][2]\n",
    "            if is_inj:\n",
    "                self.nb_injection += 1\n",
    "            if is_volt:\n",
    "                self.nb_voltage += 1\n",
    "            if is_topo:\n",
    "                self.nb_topology += 1\n",
    "            if is_line_status:\n",
    "                self.nb_line += 1\n",
    "            if is_redisp:\n",
    "                self.nb_redispatching += 1\n",
    "            if is_storage:\n",
    "                self.nb_storage += 1\n",
    "                self.nb_redispatching += 1\n",
    "            if is_curtail:\n",
    "                self.nb_curtail += 1\n",
    "            if is_dn:\n",
    "                self.nb_do_nothing += 1\n",
    "\n",
    "    def _convert_all_act(self, act_as_integer):\n",
    "        \"\"\"this function converts the action given as a list of integer. It ouputs a list of valid grid2op Action\"\"\"\n",
    "        res = []\n",
    "        for act_id in act_as_integer:\n",
    "            res.append(self.convert_act(act_id))\n",
    "            self._store_action_played(act_id)\n",
    "        return res\n",
    "\n",
    "    def _load_action_space(self, path):\n",
    "        \"\"\" load the action space in case the model is reloaded\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(path))\n",
    "        try:\n",
    "            self.action_space.init_converter(\n",
    "                all_actions=os.path.join(path, \"action_space.npy\".format(self.name)))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to reload converter action space with error \\n{}\".format(e))\n",
    "\n",
    "    # utilities for data reading\n",
    "    def _set_chunk(self, env, nb):\n",
    "        \"\"\"\n",
    "        to optimize the data reading process. See the official grid2op documentation for the effect of setting\n",
    "        the chunk size for the environment.\n",
    "        \"\"\"\n",
    "        env.set_chunk_size(int(max(100, nb)))\n",
    "\n",
    "    def _train_model(self, training_step):\n",
    "        \"\"\"train the deep sarsa networks.\"\"\"\n",
    "        self._training_param.tell_step(training_step)\n",
    "        if training_step > max(self._training_param.min_observation, self._training_param.minibatch_size) and \\\n",
    "            self._training_param.do_train():\n",
    "\n",
    "            # train the model\n",
    "            s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(self._training_param.minibatch_size)\n",
    "            tf_writer = None\n",
    "            if self.__graph_saved is False:\n",
    "                tf_writer = self._tf_writer\n",
    "            \n",
    "            loss = self.deep_sarsa.train(s_batch, a_batch, r_batch, d_batch, s2_batch, self.epsilon, \n",
    "                                     tf_writer)\n",
    "            # save learning rate for later\n",
    "            self._train_lr = self.deep_sarsa._optimizer_model._decayed_lr('float32').numpy()\n",
    "            self.__graph_saved = True\n",
    "            if not np.all(np.isfinite(loss)):\n",
    "                # if the loss is not finite i stop the learning\n",
    "                return False\n",
    "            self.deep_sarsa.target_train()\n",
    "            self._losses[training_step:] = np.sum(loss)\n",
    "        return True\n",
    "\n",
    "    def _updage_illegal_ambiguous(self, curr_step, info):\n",
    "        \"\"\"update the conunt of illegal and ambiguous actions\"\"\"\n",
    "        tmp_ = curr_step % self._vector_size\n",
    "        self._illegal_actions_per_ksteps[tmp_] = np.sum([el[\"is_illegal\"] for el in info])\n",
    "        self._ambiguous_actions_per_ksteps[tmp_] = np.sum([el[\"is_ambiguous\"] for el in info])\n",
    "\n",
    "    def _store_action_played_train(self, training_step, action_id):\n",
    "        \"\"\"store which action were played, for tensorboard only.\"\"\"\n",
    "        which_row = training_step % self._vector_size\n",
    "        self._actions_per_ksteps[which_row, :] = 0\n",
    "        self._actions_per_ksteps[which_row, action_id] += 1\n",
    "\n",
    "    def _fast_forward_env(self, env, time=7*24*60/5):\n",
    "        \"\"\"use this functio to skip some time steps when environment is reset.\"\"\"\n",
    "        my_int = np.random.randint(0, min(time, env.chronics_handler.max_timestep()))\n",
    "        env.fast_forward_chronics(my_int)\n",
    "\n",
    "    def _reset_env_clean_state(self, env):\n",
    "        \"\"\"\n",
    "        reset this environment to a proper state. This should rather be integrated in grid2op. And will probably\n",
    "        be integrated partially starting from grid2op 1.0.0\n",
    "        \"\"\"\n",
    "        # /!\\ DO NOT ATTEMPT TO MODIFY OTHERWISE IT WILL PROBABLY CRASH /!\\\n",
    "        # /!\\ THIS WILL BE PART OF THE ENVIRONMENT IN FUTURE GRID2OP RELEASE (>= 1.0.0) /!\\\n",
    "        # AND OF COURSE USING THIS METHOD DURING THE EVALUATION IS COMPLETELY FORBIDDEN\n",
    "        if self.__nb_env > 1:\n",
    "            return\n",
    "        env.current_obs = None\n",
    "        env.env_modification = None\n",
    "        env._reset_maintenance()\n",
    "        env._reset_redispatching()\n",
    "        env._reset_vectors_and_timings()\n",
    "        _backend_action = env._backend_action_class()\n",
    "        _backend_action.all_changed()\n",
    "        env._backend_action =_backend_action\n",
    "        env.backend.apply_action(_backend_action)\n",
    "        _backend_action.reset()\n",
    "        *_, fail_to_start, info = env.step(env.action_space())\n",
    "        if fail_to_start:\n",
    "            # this is happening because not enough care has been taken to handle these problems\n",
    "            # more care will be taken when this feature will be available in grid2op directly.\n",
    "            raise Grid2OpException(\"Impossible to initialize the powergrid, the powerflow diverge at iteration 0. \"\n",
    "                                   \"Available information are: {}\".format(info))\n",
    "        env._reset_vectors_and_timings()\n",
    "\n",
    "    def _need_reset(self, env, observation_num, epoch_num, done, new_state):\n",
    "        \"\"\"perform the proper reset of the environment\"\"\"\n",
    "        if self._training_param.step_increase_nb_iter is not None and \\\n",
    "           self._training_param.step_increase_nb_iter > 0:\n",
    "            self._max_iter_env(min(max(self._training_param.min_iter,\n",
    "                                       self._training_param.max_iter_fun(self._total_sucesses)),\n",
    "                                   self._training_param.max_iter))  # TODO\n",
    "        self._curr_iter_env += 1\n",
    "        if new_state is None:\n",
    "            # it's the first ever loop\n",
    "            obs = env.reset()\n",
    "            if self.__nb_env == 1:\n",
    "                obs = [obs]\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        elif self.__nb_env > 1:\n",
    "            pass\n",
    "        elif done[0]:\n",
    "            nb_ts_one_day = 24*60/5\n",
    "            if False:\n",
    "                # the 3-4 lines below allow to reuse the loaded dataset and continue further up\n",
    "                try:\n",
    "                    self._reset_env_clean_state(env)\n",
    "                    # random fast forward between now and next day\n",
    "                    self._fast_forward_env(env, time=nb_ts_one_day)\n",
    "                except (StopIteration, Grid2OpException):\n",
    "                    env.reset()\n",
    "                    # random fast forward between now and next week\n",
    "                    self._fast_forward_env(env, time=7*nb_ts_one_day)\n",
    "\n",
    "            # update the number of time steps it has live\n",
    "            ts_lived = observation_num - self._prev_obs_num\n",
    "            if self._time_step_lived is not None:\n",
    "                self._time_step_lived[self._prev_id] += ts_lived\n",
    "            self._prev_obs_num = observation_num\n",
    "            if self._training_param.oversampling_rate is not None:\n",
    "                # proba = np.sqrt(1. / (self._time_step_lived +1))\n",
    "                # # over sampling some kind of \"UCB like\" stuff\n",
    "                # # https://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/\n",
    "\n",
    "                # proba = 1. / (self._time_step_lived + 1)\n",
    "                self._proba[:] = 1. / (self._time_step_lived ** self._training_param.oversampling_rate + 1)\n",
    "                self._proba /= np.sum(self._proba)\n",
    "\n",
    "            _prev_id = self._prev_id\n",
    "            self._prev_id = None\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    self._prev_id = env.chronics_handler.real_data.sample_next_chronics(self._proba)\n",
    "            if self._prev_id is None:\n",
    "                self._prev_id = _prev_id + 1\n",
    "                self._prev_id %= self._time_step_lived.shape[0]\n",
    "\n",
    "            obs = self._reset_env(env, epoch_num)\n",
    "            if self._training_param.sample_one_random_action_begin is not None and \\\n",
    "                    observation_num < self._training_param.sample_one_random_action_begin:\n",
    "                done = True\n",
    "                while done:\n",
    "                    act = env.action_space(env.action_space._sample_set_bus())\n",
    "                    obs, reward, done, info = env.step(act)\n",
    "                    if info[\"is_illegal\"] or info[\"is_ambiguous\"]:\n",
    "                        # there are no guarantee that sampled action are legal nor perfectly\n",
    "                        # correct.\n",
    "                        # if that is the case, i \"simply\" restart the process, as if the action\n",
    "                        # broke everything\n",
    "                        done = True\n",
    "\n",
    "                    if done:\n",
    "                        obs = self._reset_env(env, epoch_num)\n",
    "                    else:\n",
    "                        if self.verbose:\n",
    "                            print(\"step {}: {}\".format(observation_num, act))\n",
    "\n",
    "                obs = [obs]\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        return new_state\n",
    "\n",
    "    def _reset_env(self, env, epoch_num):\n",
    "        env.reset()\n",
    "        if self._nb_chosen is not None:\n",
    "            self._nb_chosen[self._prev_id] += 1\n",
    "\n",
    "        # random fast forward between now and next week\n",
    "        if self._training_param.random_sample_datetime_start is not None:\n",
    "            self._fast_forward_env(env, time=self._training_param.random_sample_datetime_start)\n",
    "\n",
    "        self._curr_iter_env = 0\n",
    "        obs = [env.current_obs]\n",
    "        if epoch_num % len(env.chronics_handler.real_data.subpaths) == 0:\n",
    "            # re-shuffle the data\n",
    "            env.chronics_handler.shuffle(lambda x: x[np.random.choice(len(x), size=len(x), replace=False)])\n",
    "        return obs\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"create and initialized the replay buffer\"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(self._training_param.buffer_size)\n",
    "\n",
    "    def _store_new_state(self, initial_state, predict_movement_int, reward, done, new_state):\n",
    "        \"\"\"store the new state in the replay buffer\"\"\"\n",
    "        # vectorized version of the previous code\n",
    "        for i_s, pm_i, reward, done, ns in zip(initial_state, predict_movement_int, reward, done, new_state):\n",
    "            self.replay_buffer.add(i_s,\n",
    "                                   pm_i,\n",
    "                                   reward,\n",
    "                                   done,\n",
    "                                   ns)\n",
    "\n",
    "    def _max_iter_env(self, new_max_iter):\n",
    "        \"\"\"update the number of maximum iteration allowed.\"\"\"\n",
    "        self._max_iter_env_ = new_max_iter\n",
    "\n",
    "    def _next_move(self, curr_state, epsilon, training_step):\n",
    "        # supposes that 0 encodes for do nothing, otherwise it will NOT work (for the observer)\n",
    "        pm_i, pq_v, q_actions = self.deep_sarsa.predict_movement(curr_state, epsilon, training=True)\n",
    "        pm_i, pq_v = self._short_circuit_actions(training_step, pm_i, pq_v, q_actions)\n",
    "        act = self._convert_all_act(pm_i)\n",
    "        return pm_i, pq_v, act\n",
    "\n",
    "    def _short_circuit_actions(self, training_step, pm_i, pq_v, q_actions):\n",
    "        if self._training_param.min_observe is not None and \\\n",
    "                training_step < self._training_param.min_observe:\n",
    "            # action is replaced by do nothing due to the \"observe only\" specification\n",
    "            pm_i[:] = 0\n",
    "            pq_v[:] = q_actions[:, 0]\n",
    "        return pm_i, pq_v\n",
    "\n",
    "    def _init_global_train_loop(self):\n",
    "        alive_frame = np.zeros(self.__nb_env, dtype=np.int)\n",
    "        total_reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        return alive_frame, total_reward\n",
    "\n",
    "    def _update_loop(self, done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num):\n",
    "        if self.__nb_env == 1:\n",
    "            # force end of episode at early stage of learning\n",
    "            if self._curr_iter_env >= self._max_iter_env_:\n",
    "                temp_done[0] = True\n",
    "                temp_reward[0] = self._max_reward\n",
    "                self._total_sucesses += 1\n",
    "\n",
    "        done = temp_done\n",
    "        alive_frame[done] = 0\n",
    "        total_reward[done] = 0.\n",
    "        self._reset_num += np.sum(done)\n",
    "        if self._reset_num >= self.__nb_env:\n",
    "            # increase the \"global epoch num\" represented by \"epoch_num\" only when on average\n",
    "            # all environments are \"done\"\n",
    "            epoch_num += 1\n",
    "            self._reset_num = 0\n",
    "\n",
    "        total_reward[~done] += temp_reward[~done]\n",
    "        alive_frame[~done] += 1\n",
    "        return done, temp_reward, total_reward, alive_frame, epoch_num\n",
    "\n",
    "    def _init_local_train_loop(self):\n",
    "        # reward, done = np.zeros(self.nb_process), np.full(self.nb_process, fill_value=False, dtype=np.bool)\n",
    "        reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        done = np.full(self.__nb_env, fill_value=False, dtype=np.bool)\n",
    "        return reward, done\n",
    "\n",
    "    def _init_deep_sarsa(self, training_param, env):\n",
    "        \"\"\"\n",
    "        This function serves as initializin the neural network.\n",
    "        \"\"\"\n",
    "        if self.deep_sarsa is None:\n",
    "            self.deep_sarsa = self._nn_archi.make_nn(training_param)\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "    def _save_tensorboard(self, step, epoch_num, UPDATE_FREQ, epoch_rewards, epoch_alive):\n",
    "        \"\"\"save all the informations needed in tensorboard.\"\"\"\n",
    "        if self._tf_writer is None:\n",
    "            return\n",
    "\n",
    "        # Log some useful metrics every even updates\n",
    "        if step % UPDATE_FREQ == 0 and epoch_num > 0:\n",
    "            if step % (10 * UPDATE_FREQ) == 0:\n",
    "                # print the top k scenarios the \"hardest\" (ie chosen the most number of times\n",
    "                if self.verbose:\n",
    "                    top_k = 10\n",
    "                    if self._nb_chosen is not None:\n",
    "                        array_ = np.argsort(self._nb_chosen)[-top_k:][::-1]\n",
    "                        print(\"hardest scenarios\\n{}\".format(array_))\n",
    "                        print(\"They have been chosen respectively\\n{}\".format(self._nb_chosen[array_]))\n",
    "                        # print(\"Associated proba are\\n{}\".format(self._proba[array_]))\n",
    "                        print(\"The number of timesteps played is\\n{}\".format(self._time_step_lived[array_]))\n",
    "                        print(\"avg (accross all scenarios) number of timsteps played {}\"\n",
    "                              \"\".format(np.mean(self._time_step_lived)))\n",
    "                        print(\"Time alive: {}\".format(self._time_step_lived[array_] / (self._nb_chosen[array_] + 1)))\n",
    "                        print(\"Avg time alive: {}\".format(np.mean(self._time_step_lived / (self._nb_chosen + 1 ))))\n",
    "\n",
    "            with self._tf_writer.as_default():\n",
    "                last_alive = epoch_alive[(epoch_num-1)]\n",
    "                last_reward = epoch_rewards[(epoch_num-1)]\n",
    "\n",
    "                mean_reward = np.nanmean(epoch_rewards[:epoch_num])\n",
    "                mean_alive = np.nanmean(epoch_alive[:epoch_num])\n",
    "\n",
    "                mean_reward_30 = mean_reward\n",
    "                mean_alive_30 = mean_alive\n",
    "                mean_reward_100 = mean_reward\n",
    "                mean_alive_100 = mean_alive\n",
    "\n",
    "                tmp = self._actions_per_ksteps > 0\n",
    "                tmp = tmp.sum(axis=0)\n",
    "                nb_action_taken_last_kstep = np.sum(tmp > 0)\n",
    "\n",
    "                nb_illegal_act = np.sum(self._illegal_actions_per_ksteps)\n",
    "                nb_ambiguous_act = np.sum(self._ambiguous_actions_per_ksteps)\n",
    "\n",
    "                if epoch_num >= 100:\n",
    "                    mean_reward_100 = np.nanmean(epoch_rewards[(epoch_num-100):epoch_num])\n",
    "                    mean_alive_100 = np.nanmean(epoch_alive[(epoch_num-100):epoch_num])\n",
    "\n",
    "                if epoch_num >= 30:\n",
    "                    mean_reward_30 = np.nanmean(epoch_rewards[(epoch_num-30):epoch_num])\n",
    "                    mean_alive_30 = np.nanmean(epoch_alive[(epoch_num-30):epoch_num])\n",
    "\n",
    "                # to ensure \"fair\" comparison between single env and multi env\n",
    "                step_tb = step  # * self.__nb_env\n",
    "                # if multiply by the number of env we have \"trouble\" with random exploration at the beginning\n",
    "                # because it lasts the same number of \"real\" steps\n",
    "\n",
    "                # show first the Mean reward and mine time alive (hence the upper case)\n",
    "                tf.summary.scalar(\"Mean_alive_30\", mean_alive_30, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 30 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"Mean_reward_30\", mean_reward_30, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 30 completed episodes\")\n",
    "\n",
    "                # then it's alpha numerical order, hence the \"z_\" in front of some information\n",
    "                tf.summary.scalar(\"loss\", self._losses[step], step_tb,\n",
    "                                  description=\"Training loss (for the last training batch)\")\n",
    "\n",
    "                tf.summary.scalar(\"last_alive\", last_alive, step_tb,\n",
    "                                  description=\"Final number of steps for the last complete episode\")\n",
    "                tf.summary.scalar(\"last_reward\", last_reward, step_tb,\n",
    "                                  description=\"Final reward over the last complete episode\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward\", mean_reward, step_tb,\n",
    "                                  description=\"Average reward over the whole episodes played\")\n",
    "                tf.summary.scalar(\"mean_alive\", mean_alive, step_tb,\n",
    "                                  description=\"Average time alive over the whole episodes played\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward_100\", mean_reward_100, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 100 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"mean_alive_100\", mean_alive_100, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 100 completed episodes\")\n",
    "\n",
    "                tf.summary.scalar(\"nb_different_action_taken\", nb_action_taken_last_kstep, step_tb,\n",
    "                                  description=\"Number of different actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_illegal_act\", nb_illegal_act, step_tb,\n",
    "                                  description=\"Number of illegal actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_ambiguous_act\", nb_ambiguous_act, step_tb,\n",
    "                                  description=\"Number of ambiguous actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_total_success\", self._total_sucesses, step_tb,\n",
    "                                  description=\"Number of times the episode was completed entirely \"\n",
    "                                              \"(no game over)\")\n",
    "\n",
    "                tf.summary.scalar(\"z_lr\", self._train_lr, step_tb,\n",
    "                                  description=\"Current learning rate\")\n",
    "                tf.summary.scalar(\"z_epsilon\", self.epsilon, step_tb,\n",
    "                                  description=\"Current epsilon (from the epsilon greedy)\")\n",
    "                tf.summary.scalar(\"z_max_iter\", self._max_iter_env_, step_tb,\n",
    "                                  description=\"Maximum number of time steps before deciding a scenario \"\n",
    "                                              \"is over (=win)\")\n",
    "                tf.summary.scalar(\"z_total_episode\", epoch_num, step_tb,\n",
    "                                  description=\"Total number of episode played (number of \\\"reset\\\")\")\n",
    "\n",
    "                self.deep_sarsa.save_tensorboard(step_tb)\n",
    "\n",
    "                if self.store_action:\n",
    "                    self._store_frequency_action_type(UPDATE_FREQ, step_tb)\n",
    "\n",
    "                \n",
    "\n",
    "    def _store_frequency_action_type(self, UPDATE_FREQ, step_tb):\n",
    "        self.current_ += 1\n",
    "        self.current_ %= self.nb_\n",
    "        nb_inj, nb_volt, nb_topo, nb_line, nb_redisp, nb_storage, nb_curtail, nb_dn = self._nb_this_time[self.current_, :]\n",
    "        self._nb_this_time[self.current_, :] = [self.nb_injection,\n",
    "                                                self.nb_voltage,\n",
    "                                                self.nb_topology,\n",
    "                                                self.nb_line,\n",
    "                                                self.nb_redispatching,\n",
    "                                                self.nb_storage,\n",
    "                                                self.nb_curtail,\n",
    "                                                self.nb_do_nothing]\n",
    "\n",
    "        curr_inj = self.nb_injection - nb_inj\n",
    "        curr_volt = self.nb_voltage - nb_volt\n",
    "        curr_topo = self.nb_topology - nb_topo\n",
    "        curr_line = self.nb_line - nb_line\n",
    "        curr_redisp = self.nb_redispatching - nb_redisp\n",
    "        curr_storage = self.nb_storage - nb_storage\n",
    "        curr_curtail = self.nb_curtail - nb_curtail\n",
    "        curr_dn = self.nb_do_nothing - nb_dn\n",
    "\n",
    "        total_act_num = curr_inj + curr_volt + curr_topo + curr_line + curr_redisp + curr_dn + curr_storage\n",
    "        tf.summary.scalar(\"zz_freq_inj\",\n",
    "                          curr_inj / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"injection\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"zz_freq_voltage\",\n",
    "                          curr_volt / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"voltage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_topo\",\n",
    "                          curr_topo / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"topo\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_line_status\",\n",
    "                          curr_line / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"line status\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_redisp\",\n",
    "                          curr_redisp / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"redispatching\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_do_nothing\",\n",
    "                          curr_dn / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"do nothing\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_storage\",\n",
    "                          curr_storage / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"storage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_curtail\",\n",
    "                          curr_curtail / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"curtailment\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993fa1",
   "metadata": {},
   "source": [
    "## Abstract class to create neural networks\n",
    "\n",
    "This class aims at representing the Q value (or more in case of SAC) parametrization by a neural network.\t\n",
    "        \n",
    "It is composed of 2 different networks:\n",
    "\n",
    "- model: which is the main model\n",
    "- target_model: which has the same architecture and same initial weights as \"model\" but is updated less frequently to stabilize training\n",
    "\n",
    "It has basic methods to make predictions, to train the model, and train the target model.\n",
    "\n",
    "This class is abstraction and need to be overide in order to create object from this class. The only pure virtual function is :func:`BaseDeepSARSA.construct_q_network` that creates the neural network from the nn_params (:class:`NNParam`) provided as input\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "_action_size: ``int``\n",
    "        Total number of actions\n",
    "\n",
    "_observation_size: ``int``\n",
    "        Size of the observation space considered\n",
    "\n",
    "_nn_archi: :class:`NNParam`\n",
    "        The parameters of the neural networks that will be created\n",
    "\n",
    "_training_param: :class:`TrainingParam`\n",
    "        The meta parameters for the training scheme (used especially for learning rate or gradient clipping for example)\n",
    "\n",
    "_lr: ``float``\n",
    "        The  initial learning rate\n",
    "\n",
    "_lr_decay_steps: ``float``\n",
    "        The decay step of the learning rate\n",
    "\n",
    "_lr_decay_rate: ``float``\n",
    "        The rate at which the learning rate will decay\n",
    "\n",
    "_model:\n",
    "        Main neural network model, here a keras Model object.\n",
    "\n",
    "_target_model:\n",
    "        a copy of the main neural network that will be updated less frequently (also known as \"target model\" in RL\n",
    "        community)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0491ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepSARSA(ABC):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None,\n",
    "                 verbose=False):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        self._action_size = nn_params.action_size\n",
    "        self._observation_size = nn_params.observation_size\n",
    "        self._nn_archi = nn_params\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if training_param is None:\n",
    "            self._training_param = TrainingParam()\n",
    "        else:\n",
    "            self._training_param = training_param\n",
    "\n",
    "        self._lr = training_param.lr\n",
    "        self._lr_decay_steps = training_param.lr_decay_steps\n",
    "        self._lr_decay_rate = training_param.lr_decay_rate\n",
    "\n",
    "        self._model = None\n",
    "        self._target_model = None\n",
    "        self._schedule_model = None\n",
    "        self._optimizer_model = None\n",
    "        self._custom_objects = None  # to be able to load other keras layers type\n",
    "\n",
    "    def make_optimiser(self):\n",
    "        \"\"\"\n",
    "        helper function to create the proper optimizer (Adam) with the learning rates and its decay\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        schedule = tfko.schedules.InverseTimeDecay(self._lr, self._lr_decay_steps, self._lr_decay_rate)\n",
    "        return schedule, tfko.Adam(learning_rate=schedule)\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "         Abstract method that need to be overide.\n",
    "\n",
    "         It should create :attr:`BaseDeepSARSA._model` and :attr:`BaseDeepSARSA._target_model`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def predict_movement(self, data, epsilon, batch_size=None, training=False):\n",
    "        \"\"\"\n",
    "        Predict movement of game controler where is epsilon probability randomly move.\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = data.shape[0]\n",
    "\n",
    "        q_actions = self._model(data, training=training).numpy()\n",
    "        opt_policy = np.argmax(q_actions, axis=-1)\n",
    "        if epsilon > 0.:\n",
    "            rand_val = np.random.random(batch_size)\n",
    "            opt_policy[rand_val < epsilon] = np.random.randint(0, self._action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        return opt_policy, q_actions[np.arange(batch_size), opt_policy], q_actions\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, epsilon_tr, tf_writer=None, batch_size=None):\n",
    "        \"\"\"\n",
    "        Trains network to fit given parameters:\n",
    "        \n",
    "        .. seealso::\n",
    "            https://towardsdatascience.com/dueling-double-deep-q-learning-using-tensorflow-2-x-7bbbcec06a2a\n",
    "            for the update rules\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s_batch:\n",
    "            the state vector (before the action is taken)\n",
    "        a_batch:\n",
    "            the action taken\n",
    "        s2_batch:\n",
    "            the state vector (after the action is taken)\n",
    "        d_batch:\n",
    "            says whether or not the episode was over\n",
    "        r_batch:\n",
    "            the reward obtained this step\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = s_batch.shape[0]\n",
    "\n",
    "        # Save the graph just the first time\n",
    "        if tf_writer is not None:\n",
    "            tf.summary.trace_on()\n",
    "        target = self._model(s_batch, training=True).numpy()\n",
    "        # this fut_action should come from epsilon policy\n",
    "        next_a, fut_actions_3, fut_action_2 = self.predict_movement(s2_batch,epsilon=epsilon_tr,training=True)\n",
    "        fut_action = self._model(s2_batch, training=True).numpy()\n",
    "        \n",
    "\n",
    "        if tf_writer is not None:\n",
    "            with tf_writer.as_default():\n",
    "                tf.summary.trace_export(\"model-graph\", 0)\n",
    "            tf.summary.trace_off()\n",
    "        target_next = self._target_model(s2_batch, training=True).numpy()\n",
    "\n",
    "        idx = np.arange(batch_size)\n",
    "        target[idx, a_batch] = r_batch\n",
    "        # update the value for not done episode\n",
    "        nd_batch = ~d_batch  # update with this rule only batch that did not game over\n",
    "        next_action = np.argmax(fut_action, axis=-1)  # compute the future action i will take in the next state\n",
    "        fut_Q = target_next[idx, next_a]  # get its Q value\n",
    "        fut_Q_new = target_next[idx, next_action]  # get its Q value\n",
    "        \n",
    "        target[nd_batch, a_batch[nd_batch]] += self._training_param.discount_factor * fut_Q[nd_batch]\n",
    "        loss = self.train_on_batch(self._model, self._optimizer_model, s_batch, target)\n",
    "        return loss\n",
    "\n",
    "    def train_on_batch(self, model, optimizer_model, x, y_true):\n",
    "        \"\"\"train the model on a batch of example. This can be overide\"\"\"\n",
    "        loss = model.train_on_batch(x, y_true)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_model(path, name=None):\n",
    "        \"\"\"\n",
    "        Get the location at which the neural networks will be saved.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        path_model: ``str``\n",
    "            The path at which the model will be saved (path include both path and name, it is the full path at which\n",
    "            the neural networks are saved)\n",
    "\n",
    "        path_target_model: ``str``\n",
    "            The path at which the target model will be saved\n",
    "        \"\"\"\n",
    "\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        \"\"\"\n",
    "        save the neural networks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path at which the models need to be saved\n",
    "        name: ``str``\n",
    "            The name given to this model\n",
    "\n",
    "        ext: ``str``\n",
    "            The file extension (by default h5)\n",
    "        \"\"\"\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        self._model.save('{}.{}'.format(path_model, ext))\n",
    "        self._target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "\n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        \"\"\"\n",
    "        Load the neural networks.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path at which the models need to be saved\n",
    "        name: ``str``\n",
    "            The name given to this model\n",
    "\n",
    "        ext: ``str``\n",
    "            The file extension (by default h5)\n",
    "        \"\"\"\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        # fix for issue https://github.com/keras-team/keras/issues/7440\n",
    "        self.construct_q_network()\n",
    "\n",
    "        self._model.load_weights('{}.{}'.format(path_model, ext))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            self._target_model.load_weights('{}.{}'.format(path_target_model, ext))\n",
    "        if self.verbose:\n",
    "            print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self, tau=None):\n",
    "        \"\"\"\n",
    "        update the target model with the parameters given in the :attr:`BaseDeepSARSA._training_param`.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self._training_param.tau\n",
    "        tau_inv = 1.0 - tau\n",
    "\n",
    "        target_params = self._target_model.trainable_variables\n",
    "        source_params = self._model.trainable_variables\n",
    "        for src, dest in zip(source_params, target_params):\n",
    "            # Polyak averaging\n",
    "            var_update = src.value() * tau\n",
    "            var_persist = dest.value() * tau_inv\n",
    "            dest.assign(var_update + var_persist)\n",
    "\n",
    "    def save_tensorboard(self, current_step):\n",
    "        \"\"\"function used to save other information to tensorboard\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f5bbb",
   "metadata": {},
   "source": [
    "## Saving of trained neural networks\n",
    "This class provides an easy way to save and restore, as json, the shape of your neural networks (number of layers, non linearities, size of each layers etc.)\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "nn_class: :class:`l2rpn_baselines.BaseDeepSARSA`\n",
    "        The neural network class that will be created with each call of :func:`l2rpn_baselines.make_nn`\n",
    "\n",
    "observation_size: ``int``\n",
    "        The size of the observation space.\n",
    "\n",
    "action_size: ``int``\n",
    "        The size of the action space.\n",
    "\n",
    "sizes: ``list``\n",
    "        A list of integer, each will represent the number of hidden units. The number of hidden layer is given by\n",
    "        the size / length of this list.\n",
    "\n",
    "activs: ``list``\n",
    "        List of activation functions (given as string). It should have the same length as the :attr:`NNParam.sizes`.\n",
    "        This function should be name of keras activation function.\n",
    "\n",
    "list_attr_obs: ``list``\n",
    "        List of the attributes that will be used from the observation and concatenated to be fed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2612e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNParam(object):\n",
    "\n",
    "    _int_attr = [\"action_size\", \"observation_size\"]\n",
    "    _float_attr = []\n",
    "    _str_attr = []\n",
    "    _list_float = []\n",
    "    _list_str = [\"activs\", \"list_attr_obs\"]\n",
    "    _list_int = [\"sizes\"]\n",
    "    nn_class = BaseDeepSARSA\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs,\n",
    "                 ):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.sizes = [int(el) for el in sizes]\n",
    "        self.activs = [str(el) for el in activs]\n",
    "        if len(self.sizes) != len(self.activs):\n",
    "            raise RuntimeError(\"\\\"sizes\\\" and \\\"activs\\\" lists have not the same size. It's not clear how many layers \"\n",
    "                               \"you want your neural network to have.\")\n",
    "        self.list_attr_obs = [str(el) for el in list_attr_obs]\n",
    "\n",
    "    @classmethod\n",
    "    def get_path_model(cls, path, name=None):\n",
    "        \"\"\"get the path at which the model will be saved\"\"\"\n",
    "        return cls.nn_class.get_path_model(path, name=name)\n",
    "\n",
    "    def make_nn(self, training_param):\n",
    "        \"\"\"build the appropriate BaseDeepSARSA\"\"\"\n",
    "        res = self.nn_class(self, training_param)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_obs_size(env, list_attr_name):\n",
    "        \"\"\"get the size of the flatten observation\"\"\"\n",
    "        res = 0\n",
    "        for obs_attr_name in list_attr_name:\n",
    "            beg_, end_, dtype_ = env.observation_space.get_indx_extract(obs_attr_name)\n",
    "            res += end_ - beg_  # no \"+1\" needed because \"end_\" is exclude by python convention\n",
    "        return res\n",
    "\n",
    "    def get_obs_attr(self):\n",
    "        \"\"\"get the names of the observation attributes that will be extracted \"\"\"\n",
    "        return self.list_attr_obs\n",
    "\n",
    "    # utilitaries, do not change\n",
    "    def to_dict(self):\n",
    "        \"\"\"convert this instance to a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = {}\n",
    "        for attr_nm in self._int_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = int(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._float_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = float(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._str_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = str(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "\n",
    "        for attr_nm in self._list_float:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, float)\n",
    "        for attr_nm in self._list_int:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, int)\n",
    "        for attr_nm in self._list_str:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, str)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_list_to_json(cls, obj, type_):\n",
    "        if isinstance(obj, type_):\n",
    "            res = obj\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            if len(obj.shape) == 1:\n",
    "                res = [type_(el) for el in obj]\n",
    "            else:\n",
    "                res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        elif isinstance(obj, Iterable):\n",
    "            res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        else:\n",
    "            res = type_(obj)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _attr_from_json(cls, json, type_):\n",
    "        if isinstance(json, type_):\n",
    "            res = json\n",
    "        elif isinstance(json, list):\n",
    "            res = [cls._convert_list_to_json(obj=el, type_=type_) for el in json]\n",
    "        else:\n",
    "            res = type_(json)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, tmp):\n",
    "        \"\"\"load from a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam (more or less)\n",
    "        cls_as_dict = {}\n",
    "        for attr_nm in cls._int_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = int(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._float_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = float(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._str_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = str(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._list_float:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], float)\n",
    "        for attr_nm in cls._list_int:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], int)\n",
    "        for attr_nm in cls._list_str:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], str)\n",
    "\n",
    "        res = cls(**cls_as_dict)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path):\n",
    "        \"\"\"load from a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"No path are located at \\\"{}\\\"\".format(json_path))\n",
    "        with open(json_path, \"r\") as f:\n",
    "            dict_ = json.load(f)\n",
    "        return cls.from_dict(dict_)\n",
    "\n",
    "    def save_as_json(self, path, name=None):\n",
    "        \"\"\"save as a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = self.to_dict()\n",
    "        if name is None:\n",
    "            name = \"neural_net_parameters.json\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"Directory \\\"{}\\\" not found to save the NN parameters\".format(path))\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(\"\\\"{}\\\" should be a directory\".format(path))\n",
    "        path_out = os.path.join(path, name)\n",
    "        with open(path_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, fp=f, indent=4, sort_keys=True)\n",
    "\n",
    "    def center_reduce(self, env):\n",
    "        \"\"\"currently not implemented for this class, \"coming soon\" as we might say\"\"\"\n",
    "        # TODO see TestLeapNet for this feature\n",
    "        self._center_reduce_vect(env.get_obs(), \"x\")\n",
    "\n",
    "    def _get_adds_mults_from_name(self, obs, attr_nm):\n",
    "        if attr_nm in [\"prod_p\"]:\n",
    "            add_tmp = np.array([-0.5 * (pmax + pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "            mult_tmp = np.array([1. / max((pmax - pmin), 0.) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"prod_q\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / max(abs(val), 1.0) for val in obs.prod_q])\n",
    "        elif attr_nm in [\"load_p\", \"load_q\"]:\n",
    "            add_tmp = np.array([-val for val in getattr(obs, attr_nm)])\n",
    "            mult_tmp = 0.5\n",
    "        elif attr_nm in [\"load_v\", \"prod_v\", \"v_or\", \"v_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / val for val in getattr(obs, attr_nm)])\n",
    "        elif attr_nm == \"hour_of_day\":\n",
    "            add_tmp = -12.\n",
    "            mult_tmp = 1.0 / 12\n",
    "        elif attr_nm == \"minute_of_hour\":\n",
    "            add_tmp = -30.\n",
    "            mult_tmp = 1.0 / 30\n",
    "        elif attr_nm == \"day_of_week\":\n",
    "            add_tmp = -4.\n",
    "            mult_tmp = 1.0 / 4\n",
    "        elif attr_nm == \"day\":\n",
    "            add_tmp = -15.\n",
    "            mult_tmp = 1.0 / 15.\n",
    "        elif attr_nm in [\"target_dispatch\", \"actual_dispatch\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / (pmax - pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_or\", \"q_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1.0 / max(val, 1.0) for val in getattr(obs, attr_nm)])\n",
    "        else:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = 1.0\n",
    "        return add_tmp, mult_tmp\n",
    "\n",
    "    def _center_reduce_vect(self, obs, nn_part):\n",
    "        \"\"\"\n",
    "        compute the xxxx_adds and xxxx_mults for one part of the neural network called nn_part,\n",
    "        depending on what attribute of the observation is extracted\n",
    "        \"\"\"\n",
    "        if not isinstance(obs, grid2op.Observation.BaseObservation):\n",
    "            # in multi processing i receive a set of observation there so i might need\n",
    "            # to extract only the first one\n",
    "            obs = obs[0]\n",
    "\n",
    "        li_attr_obs = getattr(self, \"list_attr_obs_{}\".format(nn_part))\n",
    "        adds = []\n",
    "        mults = []\n",
    "        for attr_nm in li_attr_obs:\n",
    "            add_tmp, mult_tmp = self._get_adds_mults_from_name(obs, attr_nm)\n",
    "            mults.append(mult_tmp)\n",
    "            adds.append(add_tmp)\n",
    "        setattr(self, \"{}_adds\".format(nn_part), adds)\n",
    "        setattr(self, \"{}_mults\".format(nn_part), mults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fd74f",
   "metadata": {},
   "source": [
    "## To save training parameters of the model\n",
    "\n",
    "A class to store the training parameters of the models.\n",
    "\t\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "buffer_size: ``int``\n",
    "        Size of the replay buffer\n",
    "\n",
    "minibatch_size: ``int``\n",
    "        Size of the training minibatch\n",
    "update_freq: ``int``\n",
    "        Frequency at which the model is trained. Model is trained once every `update_freq` steps using `minibatch_size`\n",
    "        from an experience replay buffer.\n",
    "\n",
    "final_epsilon: ``float``\n",
    "        value for the final epsilon (for the e-greedy)\n",
    "initial_epsilon: ``float``\n",
    "        value for the initial epsilon (for the e-greedy)\n",
    "step_for_final_epsilon: ``int``\n",
    "        number of step at which the final epsilon (for the epsilon greedy exploration) will be reached\n",
    "\n",
    "min_observation: ``int``\n",
    "        number of observations before starting to train the neural nets. Before this number of iterations, the agent\n",
    "        will simply interact with the environment.\n",
    "\n",
    "lr: ``float``\n",
    "        The initial learning rate\n",
    "\n",
    "lr_decay_steps: ``int``\n",
    "        The learning rate decay step\n",
    "\n",
    "lr_decay_rate: ``float``\n",
    "        The learning rate decay rate\n",
    "\n",
    "num_frames: ``int``\n",
    "        Currently not used\n",
    "\n",
    "discount_factor: ``float``\n",
    "        The discount factor (a high discount factor is in favor of longer episode, a small one not really). This is\n",
    "        often called \"gamma\" in some RL paper. It's the gamma in: \"RL wants to minize the sum of the dicounted reward,\n",
    "        which are sum_{t >= t_0} \\gamma^{t - t_0} r_t\n",
    "\n",
    "tau: ``float``\n",
    "        Update the target model. Target model is updated according to\n",
    "        $target_model_weights[i] = self.training_param.tau * model_weights[i] + (1 - self.training_param.tau) * \\\n",
    "                                              target_model_weights[i]$\n",
    "\n",
    "min_iter: ``int``\n",
    "        It is possible in the training schedule to limit the number of time steps an episode can last. This is mainly\n",
    "        useful at beginning of training, to not get in a state where the grid has been modified so much the agent\n",
    "        will never get into a state resembling this one ever again). Stopping the episode before this happens can\n",
    "        help the learning.\n",
    "\n",
    "max_iter: ``int``\n",
    "        Just like \"min_iter\" but instead of being the minimum number of iteration, it's the maximum.\n",
    "\n",
    "update_nb_iter: ``int``\n",
    "        If max_iter_fun is the default one, this numer give the number of time we need to succeed a scenario before\n",
    "        having to increase the maximum number of timestep allowed\n",
    "\n",
    "step_increase_nb_iter: ``int`` or  ``None``\n",
    "        Of how many timestep we increase the maximum number of timesteps allowed per episode. Set it to O to deactivate\n",
    "        this.\n",
    "\n",
    "max_iter_fun: ``function``\n",
    "        A function that return the maximum number of steps an episode can count as for the current epoch. For example\n",
    "        it can be `max_iter_fun = lambda epoch_num : np.sqrt(50 * epoch_num)`\n",
    "        [default lambda x: x / self.update_nb_iter]\n",
    "\n",
    "oversampling_rate: ``float`` or ``None``\n",
    "        Set it to None to deactivate the oversampling of hard scenarios. Otherwise, this oversampling is done\n",
    "        with something like `proba = 1. / (time_step_lived**oversampling_rate + 1)` where `proba` is the probability\n",
    "        to be selected at the next call to \"reset\" and `time_step_lived` is the number of time steps\n",
    "\n",
    "random_sample_datetime_start: ``int`` or ``None``\n",
    "        If ``None`` during training the chronics will always start at the datetime the chronics start.\n",
    "        Otherwise, the training scheme will skip a number of time steps between 0 and  `random_sample_datetime_start`\n",
    "        when loading the next chronics. This is particularly useful when you want your agent to learn to operate\n",
    "        the grid regardless of the hour of day or day of the week.\n",
    "\n",
    "update_tensorboard_freq: ``int``\n",
    "        Frequency at which tensorboard is refresh (tensorboard summaries are saved every update_tensorboard_freq\n",
    "        steps)\n",
    "\n",
    "save_model_each: ``int``\n",
    "        Frequency at which the model is saved (it is saved every \"save_model_each\" steps)\n",
    "\n",
    "max_global_norm_grad: ``float``\n",
    "        Maximum gradient norm allowed (can make the training more stable) default to None if deactivated.\n",
    "        Not all baselines are compatible.\n",
    "\n",
    "max_value_grad: ``float``\n",
    "        Maximum value the gradient can take. Assign it to ``None`` to deactivate it. This can make the training\n",
    "        more stable in some cases, but can slow down the training process too. Not all baselines are compatible.\n",
    "\n",
    "max_loss: ``float``\n",
    "        Clip the value of the loss function. Set it to ``None`` to deactivate it. Again, this can make the training\n",
    "        more stable but possibly slower. Not all baselines are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88dd7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParam(object):\n",
    "    \n",
    "    _tol_float_equal = float(1e-8)\n",
    "\n",
    "    _int_attr = [\"buffer_size\", \"minibatch_size\", \"step_for_final_epsilon\",\n",
    "                 \"min_observation\", \"last_step\", \"num_frames\", \"update_freq\",\n",
    "                 \"min_iter\", \"max_iter\", \"update_tensorboard_freq\", \"save_model_each\", \"_update_nb_iter\",\n",
    "                 \"step_increase_nb_iter\", \"min_observe\", \"sample_one_random_action_begin\"]\n",
    "    _float_attr = [\"_final_epsilon\", \"_initial_epsilon\", \"lr\", \"lr_decay_steps\", \"lr_decay_rate\",\n",
    "                   \"discount_factor\", \"tau\", \"oversampling_rate\",\n",
    "                   \"max_global_norm_grad\", \"max_value_grad\", \"max_loss\"]\n",
    "\n",
    "    def __init__(self,\n",
    "                 buffer_size=40000,\n",
    "                 minibatch_size=64,\n",
    "                 step_for_final_epsilon=100000,  # step at which min_espilon is obtain\n",
    "                 min_observation=5000,  # 5000\n",
    "                 final_epsilon=1./(7*288.),  # have on average 1 random action per week of approx 7*288 time steps\n",
    "                 initial_epsilon=0.4,\n",
    "                 lr=1e-4,\n",
    "                 lr_decay_steps=10000,\n",
    "                 lr_decay_rate=0.999,\n",
    "                 num_frames=1,\n",
    "                 discount_factor=0.99,\n",
    "                 tau=0.01,\n",
    "                 update_freq=256,\n",
    "                 min_iter=50,\n",
    "                 max_iter=8064,  # 1 month\n",
    "                 update_nb_iter=10,\n",
    "                 step_increase_nb_iter=0,  # by default no oversampling / under sampling based on difficulty\n",
    "                 update_tensorboard_freq=1000,  # update tensorboard every \"update_tensorboard_freq\" steps\n",
    "                 save_model_each=10000,  # save the model every \"update_tensorboard_freq\" steps\n",
    "                 random_sample_datetime_start=None,\n",
    "                 oversampling_rate=None,\n",
    "                 max_global_norm_grad=None,\n",
    "                 max_value_grad=None,\n",
    "                 max_loss=None,\n",
    "\n",
    "                 # observer: let the neural network \"observe\" for a given amount of time\n",
    "                 # all actions are replaced by a do nothing\n",
    "                 min_observe=None,\n",
    "\n",
    "                 # i do a random action at the beginning of an episode until a certain number of step\n",
    "                 # is made\n",
    "                 # it's recommended to have \"min_observe\" to be larger that this (this is an int)\n",
    "                 sample_one_random_action_begin=None,\n",
    "                 ):\n",
    "\n",
    "        self.random_sample_datetime_start = random_sample_datetime_start\n",
    "\n",
    "        self.buffer_size = int(buffer_size)\n",
    "        self.minibatch_size = int(minibatch_size)\n",
    "        self.min_observation = int(min_observation)\n",
    "        self._final_epsilon = float(final_epsilon)  # have on average 1 random action per day of approx 288 timesteps at the end (never kill completely the exploration)\n",
    "        self._initial_epsilon = float(initial_epsilon)\n",
    "        self.step_for_final_epsilon = float(step_for_final_epsilon)\n",
    "        self.lr = float(lr)\n",
    "        self.lr_decay_steps = float(lr_decay_steps)\n",
    "        self.lr_decay_rate = float(lr_decay_rate)\n",
    "\n",
    "        # gradient clipping (if supported)\n",
    "        self.max_global_norm_grad = max_global_norm_grad\n",
    "        self.max_value_grad = max_value_grad\n",
    "        self.max_loss = max_loss\n",
    "\n",
    "        # observer\n",
    "        self.min_observe = min_observe\n",
    "        self.sample_one_random_action_begin = sample_one_random_action_begin\n",
    "\n",
    "        self.last_step = int(0)\n",
    "        self.num_frames = int(num_frames)\n",
    "        self.discount_factor = float(discount_factor)\n",
    "        self.tau = float(tau)\n",
    "        self.update_freq = int(update_freq)\n",
    "        self.min_iter = int(min_iter)\n",
    "        self.max_iter = int(max_iter)\n",
    "        self._1_update_nb_iter = None\n",
    "        self._update_nb_iter = int(update_nb_iter)\n",
    "        if step_increase_nb_iter is None:\n",
    "            # 0 and None have the same effect: it disable the feature\n",
    "            step_increase_nb_iter = 0\n",
    "        self.step_increase_nb_iter = step_increase_nb_iter\n",
    "\n",
    "        if oversampling_rate is not None:\n",
    "            self.oversampling_rate = float(oversampling_rate)\n",
    "        else:\n",
    "            self.oversampling_rate = None\n",
    "\n",
    "        self.update_tensorboard_freq = update_tensorboard_freq\n",
    "        self.save_model_each = save_model_each\n",
    "        self.max_iter_fun = self.default_max_iter_fun\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def final_epsilon(self):\n",
    "        return self._final_epsilon\n",
    "\n",
    "    @final_epsilon.setter\n",
    "    def final_epsilon(self, final_epsilon):\n",
    "        self._final_epsilon = final_epsilon\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def initial_epsilon(self):\n",
    "        return self._initial_epsilon\n",
    "\n",
    "    @initial_epsilon.setter\n",
    "    def initial_epsilon(self, initial_epsilon):\n",
    "        self._initial_epsilon = initial_epsilon\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def update_nb_iter(self):\n",
    "        return self._update_nb_iter\n",
    "\n",
    "    @update_nb_iter.setter\n",
    "    def update_nb_iter(self, update_nb_iter):\n",
    "        self._update_nb_iter = update_nb_iter\n",
    "        if self._update_nb_iter is not None and self._update_nb_iter > 0:\n",
    "            self._1_update_nb_iter = 1.0 / self._update_nb_iter\n",
    "        else:\n",
    "            self._1_update_nb_iter = 1.0\n",
    "\n",
    "    def _compute_exp_facto(self):\n",
    "        if self.final_epsilon is not None and self.initial_epsilon is not None and self.final_epsilon > 0:\n",
    "            self._exp_facto = np.log(self.initial_epsilon/self.final_epsilon)\n",
    "        else:\n",
    "            # TODO\n",
    "            self._exp_facto = 1\n",
    "\n",
    "    def default_max_iter_fun(self, nb_success):\n",
    "        \"\"\"the default max iteration function used\"\"\"\n",
    "        return self.step_increase_nb_iter * int(nb_success * self._1_update_nb_iter)\n",
    "\n",
    "    def tell_step(self, current_step):\n",
    "        \"\"\"tell this instance the number of training steps that have been made\"\"\"\n",
    "        self.last_step = current_step\n",
    "\n",
    "    def get_next_epsilon(self, current_step):\n",
    "        \"\"\"get the next epsilon for the e greedy exploration\"\"\"\n",
    "        self.tell_step(current_step)\n",
    "        if self.step_for_final_epsilon is None or self.initial_epsilon is None \\\n",
    "                or self._exp_facto is None or self.final_epsilon is None:\n",
    "            res = 0.\n",
    "        else:\n",
    "            if current_step > self.step_for_final_epsilon:\n",
    "                res = self.final_epsilon\n",
    "            else:\n",
    "                # exponential decrease\n",
    "                res = self.initial_epsilon * np.exp(- (current_step / self.step_for_final_epsilon) * self._exp_facto )\n",
    "        return res\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"serialize this instance to a dictionnary.\"\"\"\n",
    "        res = {}\n",
    "        for attr_nm in self._int_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = int(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._float_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = float(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(tmp):\n",
    "        \"\"\"initialize this instance from a dictionary\"\"\"\n",
    "        if not isinstance(tmp, dict):\n",
    "            raise RuntimeError(\"TrainingParam from dict must be called with a dictionary, and not {}\".format(tmp))\n",
    "        res = TrainingParam()\n",
    "        for attr_nm in TrainingParam._int_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    setattr(res, attr_nm, int(tmp_))\n",
    "                else:\n",
    "                    setattr(res, attr_nm, None)\n",
    "\n",
    "        for attr_nm in TrainingParam._float_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    setattr(res, attr_nm, float(tmp_))\n",
    "                else:\n",
    "                    setattr(res, attr_nm, None)\n",
    "        res.update_nb_iter = res._update_nb_iter\n",
    "        res.initial_epsilon = res._initial_epsilon\n",
    "        res._compute_exp_facto()\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_path):\n",
    "        \"\"\"initialize this instance from a json\"\"\"\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"No path are located at \\\"{}\\\"\".format(json_path))\n",
    "        with open(json_path, \"r\") as f:\n",
    "            dict_ = json.load(f)\n",
    "        return TrainingParam.from_dict(dict_)\n",
    "\n",
    "    def save_as_json(self, path, name=None):\n",
    "        \"\"\"save this instance as a json\"\"\"\n",
    "        res = self.to_dict()\n",
    "        if name is None:\n",
    "            name = \"training_parameters.json\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"Directory \\\"{}\\\" not found to save the training parameters\".format(path))\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(\"\\\"{}\\\" should be a directory\".format(path))\n",
    "        path_out = os.path.join(path, name)\n",
    "        with open(path_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, fp=f, indent=4, sort_keys=True)\n",
    "\n",
    "    def do_train(self):\n",
    "        \"\"\"return whether or not i should train the model at this time step\"\"\"\n",
    "        return self.last_step % self.update_freq == 0\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        res = True\n",
    "        for el in self._int_attr:\n",
    "            me_ = getattr(self, el)\n",
    "            oth_ = getattr(other, el)\n",
    "            if me_ is None and oth_ is not None:\n",
    "                res = False\n",
    "                break\n",
    "            if oth_ is None and me_ is not None:\n",
    "                res = False\n",
    "                break\n",
    "            if me_ is None and oth_ is None:\n",
    "                continue\n",
    "            if int(me_) != int(oth_):\n",
    "                res = False\n",
    "                break\n",
    "        if res:\n",
    "            for el in self._float_attr:\n",
    "                me_ = getattr(self, el)\n",
    "                oth_ = getattr(other, el)\n",
    "                if me_ is None and oth_ is not None:\n",
    "                    res = False\n",
    "                    break\n",
    "                if oth_ is None and me_ is not None:\n",
    "                    res = False\n",
    "                    break\n",
    "                if me_ is None and oth_ is None:\n",
    "                    continue\n",
    "                if abs(float(me_) - float(oth_)) > self._tol_float_equal:\n",
    "                    res = False\n",
    "                    break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab1f53",
   "metadata": {},
   "source": [
    "## Create neural network using abstract class created above\n",
    "Constructs the desired deep q learning network\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "schedule_lr_model:\n",
    "        The schedule for the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca815187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NN(BaseDeepSARSA):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "        BaseDeepSARSA.__init__(self,\n",
    "                           nn_params,\n",
    "                           training_param)\n",
    "        self.schedule_lr_model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "        This function will make 2 identical models, one will serve as a target model, the other one will be trained\n",
    "        regurlarly.\n",
    "        \"\"\"\n",
    "        self._model = Sequential()\n",
    "        input_layer = Input(shape=(self._nn_archi.observation_size,),\n",
    "                            name=\"state\")\n",
    "        lay = input_layer\n",
    "        for lay_num, (size, act) in enumerate(zip(self._nn_archi.sizes, self._nn_archi.activs)):\n",
    "            lay = Dense(size, name=\"layer_{}\".format(lay_num))(lay)  # put at self.action_size\n",
    "            lay = Activation(act)(lay)\n",
    "\n",
    "        output = Dense(self._action_size, name=\"output\")(lay)\n",
    "\n",
    "        self._model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._schedule_lr_model, self._optimizer_model = self.make_optimiser()\n",
    "        self._model.compile(loss='mse', optimizer=self._optimizer_model)\n",
    "\n",
    "        self._target_model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._target_model.set_weights(self._model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68582acd",
   "metadata": {},
   "source": [
    "## Define parameters for deep neural nets\n",
    "This defined the specific parameters for the DeepQ network. \n",
    "Nothing really different compared to the base class except that :attr:`l2rpn_baselines.utils.NNParam.nn_class` (nn_class) is :class:`deepQ_NN.DeepQ_NN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abcf1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NNParam(NNParam):\n",
    "    _int_attr = copy.deepcopy(NNParam._int_attr)\n",
    "    _float_attr = copy.deepcopy(NNParam._float_attr)\n",
    "    _str_attr = copy.deepcopy(NNParam._str_attr)\n",
    "    _list_float = copy.deepcopy(NNParam._list_float)\n",
    "    _list_str = copy.deepcopy(NNParam._list_str)\n",
    "    _list_int = copy.deepcopy(NNParam._list_int)\n",
    "\n",
    "    nn_class = DeepQ_NN\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,  # TODO this might not be usefull\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs\n",
    "                 ):\n",
    "        NNParam.__init__(self,\n",
    "                         action_size,\n",
    "                         observation_size,  # TODO this might not be usefull\n",
    "                         sizes,\n",
    "                         activs,\n",
    "                         list_attr_obs\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e239c0",
   "metadata": {},
   "source": [
    "## A Skeleton Class That Inherits DeepSARSAAgent Class\n",
    "A simple deep q learning algorithm. It does nothing different thant its base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f70c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQSimple(DeepSARSAAgent):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713eaf0",
   "metadata": {},
   "source": [
    "## A function to train the deep SARSA agent created above\n",
    "\n",
    "This function implements the \"training\" part of the balines \"DeepQSimple\".\n",
    "        \n",
    "#### Parameters\n",
    "\n",
    "env: :class:`grid2op.Environment`\n",
    "        Then environment on which you need to train your agent.\n",
    "\n",
    "name: ``str``\n",
    "        The name of your agent.\n",
    "\n",
    "iterations: ``int``\n",
    "        For how many iterations (steps) do you want to train your agent. NB these are not episode, these are steps.\n",
    "\n",
    "save_path: ``str``\n",
    "        Where do you want to save your baseline.\n",
    "\n",
    "load_path: ``str``\n",
    "        If you want to reload your baseline, specify the path where it is located. **NB** if a baseline is reloaded\n",
    "        some of the argument provided to this function will not be used.\n",
    "\n",
    "logs_dir: ``str``\n",
    "        Where to store the tensorboard generated logs during the training. ``None`` if you don't want to log them.\n",
    "\n",
    "training_param: :class:`l2rpn_baselines.utils.TrainingParam`\n",
    "        The parameters describing the way you will train your model.\n",
    "\n",
    "filter_action_fun: ``function``\n",
    "        A function to filter the action space. See\n",
    "        `IdToAct.filter_action <https://grid2op.readthedocs.io/en/latest/converter.html#grid2op.Converter.IdToAct.filter_action>`_\n",
    "        documentation.\n",
    "\n",
    "verbose: ``bool``\n",
    "        If you want something to be printed on the terminal (a better logging strategy will be put at some point)\n",
    "\n",
    "kwargs_converters: ``dict``\n",
    "        A dictionary containing the key-word arguments pass at this initialization of the\n",
    "        :class:`grid2op.Converter.IdToAct` that serves as \"Base\" for the Agent.\n",
    "\n",
    "kwargs_archi: ``dict``\n",
    "        Key word arguments used for making the :class:`DeepQ_NNParam` object that will be used to build the baseline.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "baseline: :class:`DeepQSimple`\n",
    "        The trained baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2972d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,\n",
    "          name=DEFAULT_NAME,\n",
    "          iterations=1,\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          logs_dir=None,\n",
    "          training_param=None,\n",
    "          filter_action_fun=None,\n",
    "          kwargs_converters={},\n",
    "          kwargs_archi={},\n",
    "          verbose=True):\n",
    "    \n",
    "    import tensorflow as tf  # lazy import to save import time\n",
    "    # Limit gpu usage\n",
    "    try:\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if len(physical_devices) > 0:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except AttributeError:\n",
    "         # issue of https://stackoverflow.com/questions/59266150/attributeerror-module-tensorflow-core-api-v2-config-has-no-attribute-list-p\n",
    "        try:\n",
    "            physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "            if len(physical_devices) > 0:\n",
    "                tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        except Exception:\n",
    "            warnings.warn(_WARN_GPU_MEMORY)\n",
    "    except Exception:\n",
    "        warnings.warn(_WARN_GPU_MEMORY)\n",
    "\n",
    "    if training_param is None:\n",
    "        training_param = TrainingParam()\n",
    "\n",
    "    # compute the proper size for the converter\n",
    "    kwargs_archi[\"action_size\"] = DeepQSimple.get_action_size(env.action_space, filter_action_fun, kwargs_converters)\n",
    "\n",
    "    if load_path is not None:\n",
    "        path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, the architecture parameters will be ignored\")\n",
    "        nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "    else:\n",
    "        nn_archi = DeepQ_NNParam(**kwargs_archi)\n",
    "\n",
    "    baseline = DeepQSimple(action_space=env.action_space,\n",
    "                           nn_archi=nn_archi,\n",
    "                           name=name,\n",
    "                           istraining=True,\n",
    "                           verbose=verbose,\n",
    "                           filter_action_fun=filter_action_fun,\n",
    "                            **kwargs_converters\n",
    "                            )\n",
    "\n",
    "    if load_path is not None:\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, training parameters will be ignored\")\n",
    "        baseline.load(load_path)\n",
    "        training_param = baseline._training_param\n",
    "\n",
    "    baseline.train(env,\n",
    "                   iterations,\n",
    "                   save_path=save_path,\n",
    "                   logdir=logs_dir,\n",
    "                   training_param=training_param)\n",
    "    # as in our example (and in our explanation) we recommend to save the mode regurlarly in the \"train\" function\n",
    "    # it is not necessary to save it again here. But if you chose not to follow these advice, it is more than\n",
    "    # recommended to save the \"baseline\" at the end of this function with:\n",
    "    # baseline.save(path_save)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f723c8",
   "metadata": {},
   "source": [
    "## Method to evaluate the agent\n",
    "How to evaluate the performances of the trained :class:`DeepQSimple` agent.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "env: :class:`grid2op.Environment`\n",
    "        The environment on which you evaluate your agent.\n",
    "\n",
    "name: ``str``\n",
    "        The name of the trained baseline\n",
    "\n",
    "load_path: ``str``\n",
    "        Path where the agent has been stored\n",
    "\n",
    "logs_path: ``str``\n",
    "        Where to write the results of the assessment\n",
    "\n",
    "nb_episode: ``str``\n",
    "        How many episodes to run during the assessment of the performances\n",
    "\n",
    "nb_process: ``int``\n",
    "        On how many process the assessment will be made. (setting this > 1 can lead to some speed ups but can be\n",
    "        unstable on some plaform)\n",
    "\n",
    "max_steps: ``int``\n",
    "        How many steps at maximum your agent will be assessed\n",
    "\n",
    "verbose: ``bool``\n",
    "        Currently un used\n",
    "\n",
    "save_gif: ``bool``\n",
    "        Whether or not you want to save, as a gif, the performance of your agent. It might cause memory issues (might\n",
    "        take a lot of ram) and drastically increase computation time.\n",
    "\n",
    "### Returns\n",
    "\n",
    "agent: :class:`l2rpn_baselines.utils.DeepSARSAAgent`\n",
    "        The loaded agent that has been evaluated thanks to the runner.\n",
    "\n",
    "res: ``list``\n",
    "        The results of the Runner on which the agent was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5157ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env,\n",
    "             name=DEFAULT_NAME,\n",
    "             load_path=None,\n",
    "             logs_path=DEFAULT_LOGS_DIR,\n",
    "             nb_episode=DEFAULT_NB_EPISODE,\n",
    "             nb_process=DEFAULT_NB_PROCESS,\n",
    "             max_steps=DEFAULT_MAX_STEPS,\n",
    "             verbose=False,\n",
    "             save_gif=False,\n",
    "             filter_action_fun=None):\n",
    "    \n",
    "\n",
    "    import tensorflow as tf  # lazy import to save import time\n",
    "    # Limit gpu usage\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    runner_params = env.get_params_for_runner()\n",
    "    runner_params[\"verbose\"] = verbose\n",
    "\n",
    "    if load_path is None:\n",
    "        raise RuntimeError(\"Cannot evaluate a model if there is nothing to be loaded.\")\n",
    "    path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "    nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "\n",
    "    # Run\n",
    "    # Create agent\n",
    "    agent = DeepQSimple(action_space=env.action_space,\n",
    "                        name=name,\n",
    "                        store_action=nb_process == 1,\n",
    "                        nn_archi=nn_archi,\n",
    "                        observation_space=env.observation_space,\n",
    "                        filter_action_fun=filter_action_fun)\n",
    "\n",
    "    # Load weights from file\n",
    "    agent.load(load_path)\n",
    "\n",
    "    # Build runner\n",
    "    runner = Runner(**runner_params,\n",
    "                    agentClass=None,\n",
    "                    agentInstance=agent)\n",
    "\n",
    "    # Print model summary\n",
    "    stringlist = []\n",
    "    agent.deep_sarsa._model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    if verbose:\n",
    "        print(short_model_summary)\n",
    "\n",
    "    # Run\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "    res = runner.run(path_save=logs_path,\n",
    "                     nb_episode=nb_episode,\n",
    "                     nb_process=nb_process,\n",
    "                     max_iter=max_steps,\n",
    "                     pbar=verbose)\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"Evaluation summary:\")\n",
    "        for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "            msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "            msg_tmp += \"\\ttotal score: {:.6f}\".format(cum_reward)\n",
    "            msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step, max_ts)\n",
    "            print(msg_tmp)\n",
    "\n",
    "        if len(agent.dict_action):\n",
    "            # I output some of the actions played\n",
    "            print(\"The agent played {} different action\".format(len(agent.dict_action)))\n",
    "            for id_, (nb, act, types) in agent.dict_action.items():\n",
    "                print(\"Action with ID {} was played {} times\".format(id_, nb))\n",
    "                print(\"{}\".format(act))\n",
    "                print(\"-----------\")\n",
    "\n",
    "    if save_gif:\n",
    "        if verbose:\n",
    "            print(\"Saving the gif of the episodes\")\n",
    "        save_log_gif(logs_path, res)\n",
    "\n",
    "    return agent, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cda061",
   "metadata": {},
   "source": [
    "## Below code snippet defines the network, trains it, and saves it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29810cef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                       | 9999/1000000 [07:59<13:28:12, 20.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 109  92  93  94  95  96  97  98  99]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[40 40  8 28 16 17 36 31 20 40]\n",
      "avg (accross all scenarios) number of timsteps played 17.302083333333332\n",
      "Time alive: [20.  20.   4.  14.   8.   8.5 18.  15.5 10.  20. ]\n",
      "Avg time alive: 8.668402777777779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                     | 19998/1000000 [16:02<12:07:03, 22.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 184 190 189 188 187 186 185 183 175]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[40 12 12 48 60 53 64 21 20 28]\n",
      "avg (accross all scenarios) number of timsteps played 34.40972222222222\n",
      "Time alive: [20.   6.   6.  24.  30.  26.5 32.  10.5 10.  14. ]\n",
      "Avg time alive: 17.22222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                    | 29999/1000000 [23:53<12:12:00, 22.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 204 211 210 209 208 207 206 205 203]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 63  44  29  20  28  24  28 148  65  76]\n",
      "avg (accross all scenarios) number of timsteps played 51.05902777777778\n",
      "Time alive: [31.5 22.  14.5 10.  14.  12.  14.  74.  32.5 38. ]\n",
      "Avg time alive: 25.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                    | 39999/1000000 [31:42<11:33:05, 23.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 382 312 313 314 315 316 317 318 319]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 63 223 705 816 244 104  32 212 232 196]\n",
      "avg (accross all scenarios) number of timsteps played 67.63541666666667\n",
      "Time alive: [ 31.5 111.5 352.5 408.  122.   52.   16.  106.  116.   98. ]\n",
      "Avg time alive: 33.83506944444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                   | 49999/1000000 [39:21<11:08:38, 23.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 303 293 294 295 296 297 298 299 300]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 63 261 377 168  20  60 322  96 228 436]\n",
      "avg (accross all scenarios) number of timsteps played 85.94618055555556\n",
      "Time alive: [ 31.5 130.5 188.5  84.   10.   30.  161.   48.  114.  218. ]\n",
      "Avg time alive: 42.990451388888886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|                                                                  | 59998/1000000 [47:00<11:00:46, 23.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 284 273 274 275 276 277 278 279 280]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 63 235 132  16 193 276  56  40 271 211]\n",
      "avg (accross all scenarios) number of timsteps played 103.85243055555556\n",
      "Time alive: [ 31.5 117.5  66.    8.   96.5 138.   28.   20.  135.5 105.5]\n",
      "Avg time alive: 51.943576388888886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                  | 70000/1000000 [55:00<11:00:05, 23.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 221 207 208 209 210 211 212 213 214]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 63  22  28  24  28  20  29 188   7  19]\n",
      "avg (accross all scenarios) number of timsteps played 121.47395833333333\n",
      "Time alive: [31.5 11.  14.  12.  14.  10.  14.5 94.   3.5  9.5]\n",
      "Avg time alive: 60.75434027777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                               | 80000/1000000 [1:02:46<10:43:41, 23.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[16  9 19 18 17 15 14 13 11 10]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[144  45  17 145 549 124 232 889  80 190]\n",
      "avg (accross all scenarios) number of timsteps played 138.54513888888889\n",
      "Time alive: [ 48.          15.           5.66666667  48.33333333 183.\n",
      "  41.33333333  77.33333333 296.33333333  26.66666667  63.33333333]\n",
      "Avg time alive: 67.95515046296296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                              | 89999/1000000 [1:10:24<10:43:39, 23.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[36 22 24 25 26 27 29 30 31 32]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 318  479  940  530   89   46  328  109  761 1081]\n",
      "avg (accross all scenarios) number of timsteps played 154.50173611111111\n",
      "Time alive: [106.         159.66666667 313.33333333 176.66666667  29.66666667\n",
      "  15.33333333 109.33333333  36.33333333 253.66666667 360.33333333]\n",
      "Avg time alive: 72.69733796296298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                              | 99998/1000000 [1:18:04<10:34:12, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[64 39 58 57 56 55 54 53 52 50]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 469  774  239  252 1327  536 1396  391  105  310]\n",
      "avg (accross all scenarios) number of timsteps played 172.99131944444446\n",
      "Time alive: [156.33333333 258.          79.66666667  84.         442.33333333\n",
      " 178.66666667 465.33333333 130.33333333  35.         103.33333333]\n",
      "Avg time alive: 78.20052083333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                            | 109999/1000000 [1:25:43<12:46:28, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[97 67 79 78 77 76 75 73 72 71]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[261  69 669  52 524 549 235 196  55  83]\n",
      "avg (accross all scenarios) number of timsteps played 190.953125\n",
      "Time alive: [ 87.          23.         223.          17.33333333 174.66666667\n",
      " 183.          78.33333333  65.33333333  18.33333333  27.66666667]\n",
      "Avg time alive: 83.57204861111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                           | 120000/1000000 [1:33:25<10:17:20, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143  93  95  96  97  98  99 100 101 102]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 66  32 680 249 261 341  45  57 288 875]\n",
      "avg (accross all scenarios) number of timsteps played 208.07638888888889\n",
      "Time alive: [ 22.          10.66666667 226.66666667  83.          87.\n",
      " 113.66666667  15.          19.          96.         291.66666667]\n",
      "Avg time alive: 88.68344907407408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                           | 129999/1000000 [1:41:13<10:21:17, 23.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 141 131 132 133 134 135 136 137 138]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 66 183 104 128  12  75 221 109  64  80]\n",
      "avg (accross all scenarios) number of timsteps played 225.41493055555554\n",
      "Time alive: [22.         61.         34.66666667 42.66666667  4.         25.\n",
      " 73.66666667 36.33333333 21.33333333 26.66666667]\n",
      "Avg time alive: 93.89756944444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                          | 139999/1000000 [1:48:58<10:03:20, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 173 160 161 162 163 164 165 166 167]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 66  30 532 261  31 138 721 296 264 238]\n",
      "avg (accross all scenarios) number of timsteps played 242.90104166666666\n",
      "Time alive: [ 22.          10.         177.33333333  87.          10.33333333\n",
      "  46.         240.33333333  98.66666667  88.          79.33333333]\n",
      "Avg time alive: 98.81626157407408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                         | 149998/1000000 [1:56:38<10:06:02, 23.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 184 190 189 188 187 186 185 183 175]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 66 613  29 111  67 284  79 236  76 270]\n",
      "avg (accross all scenarios) number of timsteps played 259.0642361111111\n",
      "Time alive: [ 22.         204.33333333   9.66666667  37.          22.33333333\n",
      "  94.66666667  26.33333333  78.66666667  25.33333333  90.        ]\n",
      "Avg time alive: 102.82783564814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|                                                          | 159999/1000000 [2:04:17<9:50:16, 23.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 205 212 211 210 209 208 207 206 204]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[215 278 284 145  24 768 451 235 503 107]\n",
      "avg (accross all scenarios) number of timsteps played 276.74131944444446\n",
      "Time alive: [ 71.66666667  92.66666667  94.66666667  48.33333333   8.\n",
      " 256.         150.33333333  78.33333333 167.66666667  35.66666667]\n",
      "Avg time alive: 105.51446759259258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                        | 169998/1000000 [2:11:57<12:00:18, 19.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 347 339 340 341 342 343 344 345 346]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 215    4   19  509  804 1092  524  130 1013  543]\n",
      "avg (accross all scenarios) number of timsteps played 295.0138888888889\n",
      "Time alive: [ 71.66666667   1.33333333   6.33333333 169.66666667 268.\n",
      " 364.         174.66666667  43.33333333 337.66666667 181.        ]\n",
      "Avg time alive: 110.11863425925925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|                                                        | 179999/1000000 [2:19:37<9:52:36, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 382 312 313 314 315 316 317 318 319]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 215  614  797  820 1051  120   49  219  259  283]\n",
      "avg (accross all scenarios) number of timsteps played 312.171875\n",
      "Time alive: [ 71.66666667 204.66666667 265.66666667 273.33333333 350.33333333\n",
      "  40.          16.33333333  73.          86.33333333  94.33333333]\n",
      "Avg time alive: 114.67158564814814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                        | 189998/1000000 [2:27:15<9:33:33, 23.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 303 293 294 295 296 297 298 299 300]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 215  497 1755  309  267   64  398  105  232  858]\n",
      "avg (accross all scenarios) number of timsteps played 329.3645833333333\n",
      "Time alive: [ 71.66666667 165.66666667 585.         103.          89.\n",
      "  21.33333333 132.66666667  35.          77.33333333 286.        ]\n",
      "Avg time alive: 117.75173611111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                       | 199999/1000000 [2:35:00<9:27:01, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 269 257 258 259 260 261 262 263 264]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[215 604 834 547 179 315 983 779 697 251]\n",
      "avg (accross all scenarios) number of timsteps played 346.1458333333333\n",
      "Time alive: [ 71.66666667 201.33333333 278.         182.33333333  59.66666667\n",
      " 105.         327.66666667 259.66666667 232.33333333  83.66666667]\n",
      "Avg time alive: 119.75376157407406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|                                                      | 210000/1000000 [2:42:43<9:20:53, 23.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 242 229 230 231 232 233 234 235 236]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[215 784 178 103 539  35  25  57 344  50]\n",
      "avg (accross all scenarios) number of timsteps played 364.40625\n",
      "Time alive: [ 71.66666667 261.33333333  59.33333333  34.33333333 179.66666667\n",
      "  11.66666667   8.33333333  19.         114.66666667  16.66666667]\n",
      "Avg time alive: 124.4950810185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                     | 219998/1000000 [2:50:23<9:12:39, 23.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 227 213 214 215 216 217 218 219 220]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 215   89   11  446 1060  527  187   60  823  207]\n",
      "avg (accross all scenarios) number of timsteps played 381.59027777777777\n",
      "Time alive: [ 71.66666667  29.66666667   3.66666667 148.66666667 353.33333333\n",
      " 175.66666667  62.33333333  20.         274.33333333  69.        ]\n",
      "Avg time alive: 129.89091435185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                     | 230000/1000000 [2:58:07<9:04:41, 23.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 191 197 196 195 194 193 192 190 199]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[215  28 248  95 303  66 255 235  29 239]\n",
      "avg (accross all scenarios) number of timsteps played 398.48090277777777\n",
      "Time alive: [ 71.66666667   9.33333333  82.66666667  31.66666667 101.\n",
      "  22.          85.          78.33333333   9.66666667  79.66666667]\n",
      "Avg time alive: 132.90943287037035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                    | 240000/1000000 [3:05:50<8:52:14, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[30 15 29 28 27 26 25 24 23 22]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 109  323 1038  224 1043   96  761 1172  376  494]\n",
      "avg (accross all scenarios) number of timsteps played 415.85069444444446\n",
      "Time alive: [ 27.25  80.75 259.5   56.   260.75  24.   190.25 293.    94.   123.5 ]\n",
      "Avg time alive: 136.01938657407405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                   | 249998/1000000 [3:13:29<8:47:39, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[40 42 29 30 31 32 34 35 36 37]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[1174  264 1038 1484  882 1199  530 1455  546 1611]\n",
      "avg (accross all scenarios) number of timsteps played 433.7986111111111\n",
      "Time alive: [293.5   66.   259.5  371.   220.5  299.75 132.5  363.75 136.5  402.75]\n",
      "Avg time alive: 138.80280671296293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                   | 260000/1000000 [3:21:13<8:39:46, 23.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[97 52 77 76 75 74 72 71 70 69]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 407  129  768  972  243  973  272   99 1165   44]\n",
      "avg (accross all scenarios) number of timsteps played 450.5208333333333\n",
      "Time alive: [101.75  32.25 192.   243.    60.75 243.25  68.    24.75 291.25  11.  ]\n",
      "Avg time alive: 140.3304398148148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                  | 269998/1000000 [3:28:57<8:34:45, 23.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143  71  91  92  93  94  95  96  97  98]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 66  99 702 531 249 829 838 360 407 434]\n",
      "avg (accross all scenarios) number of timsteps played 468.62152777777777\n",
      "Time alive: [ 16.5   24.75 175.5  132.75  62.25 207.25 209.5   90.   101.75 108.5 ]\n",
      "Avg time alive: 143.0687210648148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|                                                 | 280000/1000000 [3:36:32<8:33:28, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 108 101 102 103 104 105 106 107 109]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 302  112  567 1011  307 1888 1694  367 1559  136]\n",
      "avg (accross all scenarios) number of timsteps played 485.21527777777777\n",
      "Time alive: [ 75.5   28.   141.75 252.75  76.75 472.   423.5   91.75 389.75  34.  ]\n",
      "Avg time alive: 146.63888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                 | 290000/1000000 [3:44:17<8:25:00, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 125 127 128 129 130 131 132 133 134]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[302 282 542 194 295 571 192 707 406 639]\n",
      "avg (accross all scenarios) number of timsteps played 503.40972222222223\n",
      "Time alive: [ 75.5   70.5  135.5   48.5   73.75 142.75  48.   176.75 101.5  159.75]\n",
      "Avg time alive: 150.03949652777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                | 299998/1000000 [3:52:04<8:30:13, 22.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 176 163 164 165 166 167 168 169 170]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[302 390 258 736 305 916 257 822 127 112]\n",
      "avg (accross all scenarios) number of timsteps played 520.0225694444445\n",
      "Time alive: [ 75.5   97.5   64.5  184.    76.25 229.    64.25 205.5   31.75  28.  ]\n",
      "Avg time alive: 151.62037037037035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|                                               | 309999/1000000 [3:59:44<8:05:14, 23.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 217 197 196 195 194 193 192 191 190]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222  195  466  226  807 1051 1348 1019  220  531]\n",
      "avg (accross all scenarios) number of timsteps played 537.8628472222222\n",
      "Time alive: [ 55.5   48.75 116.5   56.5  201.75 262.75 337.   254.75  55.   132.75]\n",
      "Avg time alive: 153.17491319444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                               | 319999/1000000 [4:07:27<7:59:04, 23.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 220 228 227 226 225 224 223 222 221]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[222 294 568 184 274 123 115 407 197 105]\n",
      "avg (accross all scenarios) number of timsteps played 554.8350694444445\n",
      "Time alive: [ 55.5   73.5  142.    46.    68.5   30.75  28.75 101.75  49.25  26.25]\n",
      "Avg time alive: 154.2941261574074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                              | 329998/1000000 [4:15:10<9:29:12, 19.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 332 323 324 325 326 327 328 329 330]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222  567 1095  322  717  453  187  980 1199 1181]\n",
      "avg (accross all scenarios) number of timsteps played 572.7170138888889\n",
      "Time alive: [ 55.5  141.75 273.75  80.5  179.25 113.25  46.75 245.   299.75 295.25]\n",
      "Avg time alive: 157.1997974537037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|                                             | 339998/1000000 [4:22:52<8:00:22, 22.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 306 296 297 298 299 300 301 302 303]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222 1249   82  449  109 1101 1233  268  738  504]\n",
      "avg (accross all scenarios) number of timsteps played 589.21875\n",
      "Time alive: [ 55.5  312.25  20.5  112.25  27.25 275.25 308.25  67.   184.5  126.  ]\n",
      "Avg time alive: 157.6161747685185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                            | 350000/1000000 [4:30:32<7:40:13, 23.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 285 274 275 276 277 278 279 280 281]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222  439 1385  351 1102  372  611 2142  386  241]\n",
      "avg (accross all scenarios) number of timsteps played 607.3072916666666\n",
      "Time alive: [ 55.5  109.75 346.25  87.75 275.5   93.   152.75 535.5   96.5   60.25]\n",
      "Avg time alive: 159.54571759259258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                            | 359999/1000000 [4:38:15<7:28:36, 23.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 240 242 243 244 245 246 247 248 249]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222  377  808 1048  311  539  544  662  724  215]\n",
      "avg (accross all scenarios) number of timsteps played 624.5034722222222\n",
      "Time alive: [ 55.5   94.25 202.   262.    77.75 134.75 136.   165.5  181.    53.75]\n",
      "Avg time alive: 161.57118055555554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                           | 370000/1000000 [4:45:53<7:32:26, 23.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 318 224 225 226 227 228 229 230 231]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[222 331 115 123 274 184 568 202 451 595]\n",
      "avg (accross all scenarios) number of timsteps played 641.8454861111111\n",
      "Time alive: [ 55.5   82.75  28.75  30.75  68.5   46.   142.    50.5  112.75 148.75]\n",
      "Avg time alive: 164.24146412037035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                          | 380000/1000000 [4:53:33<7:17:30, 23.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 206 212 211 210 209 208 207 205 231]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 222  527  489  224  209  824 1500  274  370  595]\n",
      "avg (accross all scenarios) number of timsteps played 659.1892361111111\n",
      "Time alive: [ 55.5  131.75 122.25  56.    52.25 206.   375.    68.5   92.5  148.75]\n",
      "Avg time alive: 166.55295138888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                          | 389998/1000000 [5:01:16<7:08:55, 23.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[14  7 13 12 11  9  8 10  6  5]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 732  408  961  566  920  220  743  432 2507  504]\n",
      "avg (accross all scenarios) number of timsteps played 676.7239583333334\n",
      "Time alive: [146.4  81.6 192.2 113.2 184.   44.  148.6  86.4 501.4 100.8]\n",
      "Avg time alive: 168.25703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                         | 399999/1000000 [5:08:57<7:05:04, 23.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[34 31 22 23 24 25 27 28 29 30]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1006 2574  523  498 1333  834 1167  252 1193 1938]\n",
      "avg (accross all scenarios) number of timsteps played 693.859375\n",
      "Time alive: [201.2 514.8 104.6  99.6 266.6 166.8 233.4  50.4 238.6 387.6]\n",
      "Avg time alive: 169.75190972222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                        | 409998/1000000 [5:16:39<7:04:57, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[66 54 62 61 60 59 58 57 56 55]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1757 2138  890  760  823   78 1277  301 1886  675]\n",
      "avg (accross all scenarios) number of timsteps played 711.5868055555555\n",
      "Time alive: [351.4 427.6 178.  152.  164.6  15.6 255.4  60.2 377.2 135. ]\n",
      "Avg time alive: 171.00711805555557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|                                        | 420000/1000000 [5:24:20<6:50:02, 23.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[100  74  87  86  85  84  83  82  81  80]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 301  981 2012  419  301  971  894  835 1306 1193]\n",
      "avg (accross all scenarios) number of timsteps played 729.0954861111111\n",
      "Time alive: [ 60.2 196.2 402.4  83.8  60.2 194.2 178.8 167.  261.2 238.6]\n",
      "Avg time alive: 172.59505208333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                       | 429998/1000000 [5:31:59<6:39:46, 23.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 110  93  94  95  96  97  98  99 100]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1401  879 1631 1104 1539  564 1064  518  955  301]\n",
      "avg (accross all scenarios) number of timsteps played 745.9427083333334\n",
      "Time alive: [280.2 175.8 326.2 220.8 307.8 112.8 212.8 103.6 191.   60.2]\n",
      "Avg time alive: 174.6756076388889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                      | 439998/1000000 [5:39:38<6:41:08, 23.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 127 118 119 120 121 122 123 124 125]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1401  758 1550  463  773  688  725  543  788  298]\n",
      "avg (accross all scenarios) number of timsteps played 763.8298611111111\n",
      "Time alive: [280.2 151.6 310.   92.6 154.6 137.6 145.  108.6 157.6  59.6]\n",
      "Avg time alive: 176.53098958333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                      | 449998/1000000 [5:47:20<6:25:17, 23.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 145 147 148 149 150 151 152 153 154]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1401 1345 1489  777  834  296 2764  630 1535  851]\n",
      "avg (accross all scenarios) number of timsteps played 779.9270833333334\n",
      "Time alive: [280.2 269.  297.8 155.4 166.8  59.2 552.8 126.  307.  170.2]\n",
      "Avg time alive: 177.80208333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                     | 460000/1000000 [5:55:01<6:18:22, 23.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 165 167 168 169 170 171 172 173 174]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1401 1977  410  826  358  636  225  525  351  135]\n",
      "avg (accross all scenarios) number of timsteps played 798.4184027777778\n",
      "Time alive: [280.2 395.4  82.  165.2  71.6 127.2  45.  105.   70.2  27. ]\n",
      "Avg time alive: 179.88081597222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                    | 469998/1000000 [6:02:46<6:17:23, 23.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 208 215 214 213 212 211 210 209 207]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230 1625 1315  808   78  760  583  217  836  373]\n",
      "avg (accross all scenarios) number of timsteps played 815.2534722222222\n",
      "Time alive: [ 46.  325.  263.  161.6  15.6 152.  116.6  43.4 167.2  74.6]\n",
      "Avg time alive: 179.52986111111113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|                                    | 479998/1000000 [6:10:26<6:05:16, 23.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 226 340 341 342 343 344 345 346 227]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230  441  757 1068 2231  784 1127 1052  719  428]\n",
      "avg (accross all scenarios) number of timsteps played 833.234375\n",
      "Time alive: [ 46.   88.2 151.4 213.6 446.2 156.8 225.4 210.4 143.8  85.6]\n",
      "Avg time alive: 180.9650173611111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                   | 490000/1000000 [6:18:07<6:50:15, 20.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 328 319 320 321 322 323 324 325 326]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230 1275 1224  726 1296  937 1295  584 1056  880]\n",
      "avg (accross all scenarios) number of timsteps played 850.4635416666666\n",
      "Time alive: [ 46.  255.  244.8 145.2 259.2 187.4 259.  116.8 211.2 176. ]\n",
      "Avg time alive: 183.0246527777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                  | 500000/1000000 [6:25:47<6:01:53, 23.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 309 299 300 301 302 303 304 305 306]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230  490 1333 1257  291  815  512  298  655 1292]\n",
      "avg (accross all scenarios) number of timsteps played 867.5954861111111\n",
      "Time alive: [ 46.   98.  266.6 251.4  58.2 163.  102.4  59.6 131.  258.4]\n",
      "Avg time alive: 184.22925347222224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|                                 | 509999/1000000 [6:33:29<5:44:15, 23.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 127 269 270 271 272 273 274 275 276]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230  758 1504  546  816 1345 1303 1517  538 1193]\n",
      "avg (accross all scenarios) number of timsteps played 885.1059027777778\n",
      "Time alive: [ 46.  151.6 300.8 109.2 163.2 269.  260.6 303.4 107.6 238.6]\n",
      "Avg time alive: 184.48906250000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|                                 | 519999/1000000 [6:41:10<5:40:57, 23.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 213 245 246 247 248 249 250 251 252]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230   78  763  776  743 2017  231 1381  379 1118]\n",
      "avg (accross all scenarios) number of timsteps played 902.6041666666666\n",
      "Time alive: [ 46.   15.6 152.6 155.2 148.6 403.4  46.2 276.2  75.8 223.6]\n",
      "Avg time alive: 186.5287326388889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                | 530000/1000000 [6:48:51<5:31:34, 23.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 251 221 222 223 224 225 226 227 228]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 230  379  658  428  490  424  438  441  428 1433]\n",
      "avg (accross all scenarios) number of timsteps played 919.6875\n",
      "Time alive: [ 46.   75.8 131.6  85.6  98.   84.8  87.6  88.2  85.6 286.6]\n",
      "Avg time alive: 187.32899305555554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                | 531007/1000000 [6:49:40<13:35:28,  9.59it/s]"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "env = grid2op.make(\"l2rpn_neurips_2020_track1_small\", reward_class=L2RPNReward)\n",
    "tp = TrainingParam()\n",
    "\n",
    "li_attr_obs_X = [\"day_of_week\", \"hour_of_day\", \"minute_of_hour\", \"prod_p\", \"prod_v\", \"load_p\", \"load_q\",\n",
    "                         \"actual_dispatch\", \"target_dispatch\", \"topo_vect\", \"time_before_cooldown_line\",\n",
    "                         \"time_before_cooldown_sub\", \"rho\", \"timestep_overflow\", \"line_status\"]\n",
    "\n",
    "observation_size = NNParam.get_obs_size(env, li_attr_obs_X)\n",
    "sizes = [800, 800, 800, 494, 494, 494]  # sizes of each hidden layers\n",
    "kwargs_archi = {'observation_size': observation_size,\n",
    "                        'sizes': sizes,\n",
    "                        'activs': [\"relu\" for _ in sizes],  # all relu activation function\n",
    "                        \"list_attr_obs\": li_attr_obs_X}\n",
    "\n",
    "kwargs_converters = {\"all_actions\": None,\n",
    "                             \"set_line_status\": False,\n",
    "                             \"change_bus_vect\": True,\n",
    "                             \"set_topo_vect\": False\n",
    "                             }\n",
    "# define the name of the model\n",
    "nm_ = \"Deep_SARSA_Agent\"\n",
    "try:\n",
    "    train(env,\n",
    "          name=nm_,\n",
    "          iterations=1000000,\n",
    "          save_path=\"./D_SARSA_Agent/model\",\n",
    "          load_path=None,\n",
    "          logs_dir=\"./D_SARSA_Agent/logs\",\n",
    "          training_param=tp,\n",
    "          kwargs_converters=kwargs_converters,\n",
    "          kwargs_archi=kwargs_archi)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19160ae",
   "metadata": {},
   "source": [
    "## Below code snippet evaluates the saved agent and returns the requested data\n",
    "If ``verbose= True`` then a detailed log of the process is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46e0a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.DeepQSimple at 0x238d617d790>,\n",
       " [('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_000',\n",
       "   'Scenario_april_000',\n",
       "   31518.462890625,\n",
       "   617,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_001',\n",
       "   'Scenario_april_001',\n",
       "   39090.1484375,\n",
       "   780,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_002',\n",
       "   'Scenario_april_002',\n",
       "   65330.15625,\n",
       "   1299,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_003',\n",
       "   'Scenario_april_003',\n",
       "   35800.421875,\n",
       "   689,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_004',\n",
       "   'Scenario_april_004',\n",
       "   25113.173828125,\n",
       "   484,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_005',\n",
       "   'Scenario_april_005',\n",
       "   12245.357421875,\n",
       "   239,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_006',\n",
       "   'Scenario_april_006',\n",
       "   9454.0849609375,\n",
       "   183,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_007',\n",
       "   'Scenario_april_007',\n",
       "   50510.890625,\n",
       "   979,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_008',\n",
       "   'Scenario_april_008',\n",
       "   18998.5625,\n",
       "   365,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_009',\n",
       "   'Scenario_april_009',\n",
       "   10778.5986328125,\n",
       "   206,\n",
       "   8062)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = grid2op.make(\"l2rpn_neurips_2020_track1_small\", reward_class=L2RPNReward)\n",
    "evaluate(env,\n",
    "         name=\"Deep_SARSA_Agent\",\n",
    "         load_path=\"./D_SARSA_Agent/model\",\n",
    "         logs_path=\"./D_SARSA_Agent/logs\",\n",
    "         nb_episode=10,\n",
    "         nb_process=1,\n",
    "         max_steps=-1,\n",
    "         verbose=False,\n",
    "         save_gif=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411075bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
