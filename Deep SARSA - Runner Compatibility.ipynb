{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85e26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Callable\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from grid2op.Agent import BaseAgent\n",
    "from grid2op.gym_compat import GymEnv, BoxGymObsSpace, DiscreteActSpace\n",
    "from gym import Env\n",
    "from gym.utils.env_checker import check_env\n",
    "from grid2op.PlotGrid import PlotMatplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ffa34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return torch.from_numpy(obs).unsqueeze(dim=0).float()\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = action.item()\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
    "        reward = torch.tensor(reward).view(1, -1).float()\n",
    "        done = torch.tensor(done).view(1, -1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2fbc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity=1000000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.can_sample(batch_size)\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = zip(*batch)\n",
    "        return [torch.cat(items) for items in batch]\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size * 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79fd0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSarsaAgent(BaseAgent):\n",
    "    \n",
    "    def __init__(self, episodes, env_name=None):\n",
    "        if env_name is None:\n",
    "            raise RuntimeError(\"Environment name must be passed\")\n",
    "        env = grid2op.make(env_name)\n",
    "        self.episodes = episodes\n",
    "        self.gym_env = GymEnv(env)\n",
    "        self.gym_env.observation_space = BoxGymObsSpace(env.observation_space, attr_to_keep=[\"gen_p\", \"load_p\", \"topo_vect\", \"rho\"])\n",
    "        self.gym_env.action_space = DiscreteActSpace(env.action_space, attr_to_keep=[\"set_bus\" , \"change_bus\", \"change_line_status\", \"set_line_status\", \"set_line_status_simple\"])\n",
    "        self.state_dims = self.gym_env.observation_space.shape[0]\n",
    "        self.num_actions = self.gym_env.action_space.n\n",
    "        \n",
    "        self.prepo_gym_env = PreprocessEnv(self.gym_env)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.q_network = self.q_network_def()\n",
    "        self.target_q_network = self.target_network()\n",
    "        self.optim = AdamW(self.q_network.parameters(), lr=0.001) #optimiser to optimise weight calculation of neural networks\n",
    "        self.memory = ReplayMemory() #Initialising memory to store State, Action, Reward, and Next State\n",
    "        self.stats = {'MSE Loss': [], 'Returns': []} #Dict to store statistics\n",
    "        \n",
    "    \n",
    "    def q_network_def(self):\n",
    "        q_network = nn.Sequential(nn.Linear(self.state_dims, 300),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(300, 250),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(250, self.num_actions))\n",
    "        return q_network\n",
    "    \n",
    "    def target_network(self):\n",
    "        q_network = self.q_network_def()\n",
    "        target_q_network = copy.deepcopy(q_network).eval()\n",
    "        return target_q_network\n",
    "    \n",
    "    def policy(self, state, epsilon=0.05):\n",
    "        self.epsilon = 0.05\n",
    "        q_network = self.q_network_def()\n",
    "        if torch.rand(1) < epsilon:\n",
    "            return torch.randint(self.num_actions, (1, 1))\n",
    "        else:\n",
    "            av = q_network(state).detach()\n",
    "            return torch.argmax(av, dim=-1, keepdim=True)\n",
    "    \n",
    "    def train_network(self, alpha=0.001, batch_size=32, gamma=0.99, epsilon=0.):\n",
    "#         self.batch_size = 32\n",
    "#         self.gamma = 0.99\n",
    "#         q_network = self.q_network_def()\n",
    "#         target_q_network = self.target_network()\n",
    "#         optim = AdamW(q_network.parameters(), lr=0.001) #optimiser to optimise weight calculation of neural networks\n",
    "#         memory = ReplayMemory() #Initialising memory to store State, Action, Reward, and Next State\n",
    "#         stats = {'MSE Loss': [], 'Returns': []} #Dict to store statistics\n",
    "        trained = False\n",
    "    \n",
    "        for self.episode in tqdm(range(1, self.episodes + 1)):\n",
    "            state = self.prepo_gym_env.reset() #getting initial state\n",
    "            done = False\n",
    "            ep_return = 0\n",
    "            while not done:\n",
    "                action = self.policy(state, epsilon) #Getting first action greedily with randomisation factor Epsilon\n",
    "                next_state, reward, done, _ = self.prepo_gym_env.step(action) #taking selected action on environment\n",
    "                self.memory.insert([state, action, reward, done, next_state]) #Storing the results to memory\n",
    "                if self.memory.can_sample(self.batch_size): #samples will be created only if memory pool is 10 times of batch size\n",
    "                    state_b, action_b, reward_b, done_b, next_state_b = self.memory.sample(self.batch_size) #creating batches to train neural network\n",
    "                    qsa_b = self.q_network(state_b).gather(1, action_b) #providing the state to neural network and comparing the \n",
    "                                                                #actions with actions stored in memory and gather the experiences\n",
    "                    next_action_b = self.policy(next_state_b) #using greedy epsilon policy to greedily get next actions\n",
    "                    next_qsa_b = self.target_q_network(next_state_b).gather(1, next_action_b) #provide next state and next action to a target neural network\n",
    "                                                                                        #and gather its experiences\n",
    "                    target_b = reward_b + ~done_b * gamma * next_qsa_b #discount the experiences of target network\n",
    "                    loss = F.mse_loss(qsa_b, target_b) #find a Mean square error loss\n",
    "                    self.q_network.zero_grad() #reset the gradients of the network\n",
    "                    loss.backward() #calculate gradients using backward propogation\n",
    "                    self.optim.step() # Iterate over all parameters (tensors) that are supposed \n",
    "                                # to be updated and use internally stored grad to update their values\n",
    "                    loss.item() # get the loss\n",
    "                    self.stats['MSE Loss'].append(loss.item())\n",
    "                    \n",
    "\n",
    "                state = next_state\n",
    "                ep_return += reward.item()\n",
    "\n",
    "            self.stats['Returns'].append(ep_return)\n",
    "\n",
    "            if self.episode % 10 == 0:\n",
    "                self.target_q_network.load_state_dict(self.q_network.state_dict()) #After every 10 episodes load state of original network to\n",
    "                                                                        # target network\n",
    "        trained = True\n",
    "        return self.stats, trained\n",
    "    \n",
    "    def act(self, observation, reward, done=False):\n",
    "        stats , trained = self.train_network()\n",
    "        if trained:\n",
    "            gym_obs = self.gym_env.observation_space.to_gym(observation)\n",
    "            conv_gym_obs = torch.from_numpy(gym_obs).unsqueeze(dim=0).float()\n",
    "            gym_act = self.policy(conv_gym_obs)\n",
    "            grid2op_act = self.gym_env.action_space.from_gym(gym_act)\n",
    "            return grid2op_act\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42f8a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_sarsa_agent = DeepSarsaAgent(episodes=200, env_name = \"rte_case14_realistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aec6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the custom agent are:\n",
      "\tFor chronics with id 000\n",
      "\t\t - cumulative reward: 352.881897\n",
      "\t\t - number of time steps completed: 5 / 5\n",
      "\tFor chronics with id 001\n",
      "\t\t - cumulative reward: 80.713821\n",
      "\t\t - number of time steps completed: 2 / 5\n",
      "\tFor chronics with id 002\n",
      "\t\t - cumulative reward: 354.311279\n",
      "\t\t - number of time steps completed: 5 / 5\n",
      "\tFor chronics with id 003\n",
      "\t\t - cumulative reward: 361.451538\n",
      "\t\t - number of time steps completed: 5 / 5\n",
      "\tFor chronics with id 004\n",
      "\t\t - cumulative reward: 177.294312\n",
      "\t\t - number of time steps completed: 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20 #customize\n",
    "from grid2op.Runner import Runner\n",
    "import os\n",
    "# from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Chronics import GridStateFromFileWithForecasts\n",
    "\n",
    "path_saved_data = './Res'\n",
    "if not os.path.exists(path_saved_data):\n",
    "    os.mkdir(path_saved_data)\n",
    "\n",
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(),\n",
    "                agentInstance=deep_sarsa_agent, agentClass=None)\n",
    "res = runner.run(nb_episode=5, max_iter=max_iter, path_save=path_saved_data)\n",
    "print(\"The results for the custom agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc09db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
