{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe686686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid2op\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Callable\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from grid2op.Agent import BaseAgent\n",
    "from grid2op.gym_compat import GymEnv, BoxGymObsSpace, DiscreteActSpace\n",
    "from gym import Env\n",
    "from gym.utils.env_checker import check_env\n",
    "from grid2op.PlotGrid import PlotMatplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5245f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        print(f'type={ type(obs) }')\n",
    "        return torch.from_numpy(obs).unsqueeze(dim=0).float()\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = action.item()\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
    "        reward = torch.tensor(reward).view(1, -1).float()\n",
    "        done = torch.tensor(done).view(1, -1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7be24804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity=1000000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.can_sample(batch_size)\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = zip(*batch)\n",
    "        return [torch.cat(items) for items in batch]\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        print(f'Memory len is:{len(self.memory)} and condition is: {batch_size * 10}')\n",
    "        return len(self.memory) >= batch_size * 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7385b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSarsaAgent(BaseAgent):\n",
    "    \n",
    "    def __init__(self, episodes, env_name=None):\n",
    "        if env_name is None:\n",
    "            raise RuntimeError(\"Environment name must be passed\")\n",
    "        env = grid2op.make(env_name)\n",
    "        self.episodes = episodes\n",
    "        self.gym_env = GymEnv(env)\n",
    "        self.gym_env.observation_space = BoxGymObsSpace(env.observation_space, attr_to_keep=[\"gen_p\", \"load_p\", \"topo_vect\", \"rho\"])\n",
    "        self.gym_env.action_space = DiscreteActSpace(env.action_space, attr_to_keep=[\"set_bus\" , \"change_bus\", \"change_line_status\", \"set_line_status\", \"set_line_status_simple\"])\n",
    "        self.state_dims = self.gym_env.observation_space.shape[0]\n",
    "        self.num_actions = self.gym_env.action_space.n\n",
    "        \n",
    "        self.prepo_gym_env = PreprocessEnv(self.gym_env)\n",
    "    \n",
    "    def q_network_def(self):\n",
    "        q_network = nn.Sequential(nn.Linear(self.state_dims, 300),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(300, 250),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(250, self.num_actions))\n",
    "        return q_network\n",
    "    \n",
    "    def target_network(self):\n",
    "        q_network = self.q_network_def()\n",
    "        target_q_network = copy.deepcopy(q_network).eval()\n",
    "        return target_q_network\n",
    "    \n",
    "    def policy(self, state, epsilon=0.05):\n",
    "        self.epsilon = 0.05\n",
    "        q_network = self.q_network_def()\n",
    "        if torch.rand(1) < epsilon:\n",
    "            return torch.randint(self.num_actions, (1, 1))\n",
    "        else:\n",
    "            av = q_network(state).detach()\n",
    "            return torch.argmax(av, dim=-1, keepdim=True)\n",
    "    \n",
    "    def train_network(self, policy, alpha=0.001, batch_size=32, gamma=0.99, epsilon=0.):\n",
    "        self.batch_size = 5\n",
    "        self.gamma = 0.99\n",
    "        q_network = self.q_network_def()\n",
    "        target_q_network = self.target_network()\n",
    "        optim = AdamW(q_network.parameters(), lr=0.001) #optimiser to optimise weight calculation of neural networks\n",
    "        memory = ReplayMemory() #Initialising memory to store State, Action, Reward, and Next State\n",
    "        stats = {'MSE Loss': [], 'Returns': []} #Dict to store statistics\n",
    "        self.trained = False\n",
    "    \n",
    "        for self.episode in tqdm(range(1, self.episodes + 1)):\n",
    "            state = self.prepo_gym_env.reset() #getting initial state\n",
    "            done = False\n",
    "            ep_return = 0\n",
    "            while not done:\n",
    "                action = self.policy(state, epsilon) #Getting first action greedily with randomisation factor Epsilon\n",
    "                print(f'The action is: {action}')\n",
    "                next_state, reward, done, _ = self.prepo_gym_env.step(action) #taking selected action on environment\n",
    "                memory.insert([state, action, reward, done, next_state]) #Storing the results to memory\n",
    "                if memory.can_sample(self.batch_size): #samples will be created only if memory pool is 10 times of batch size\n",
    "                    state_b, action_b, reward_b, done_b, next_state_b = memory.sample(self.batch_size) #creating batches to train neural network\n",
    "                    qsa_b = q_network(state_b).gather(1, action_b) #providing the state to neural network and comparing the \n",
    "                                                                #actions with actions stored in memory and gather the experiences\n",
    "                    next_action_b = self.policy(next_state_b) #using greedy epsilon policy to greedily get next actions\n",
    "                    next_qsa_b = target_q_network(next_state_b).gather(1, next_action_b) #provide next state and next action to a target neural network\n",
    "                                                                                        #and gather its experiences\n",
    "                    target_b = reward_b + ~done_b * gamma * next_qsa_b #discount the experiences of target network\n",
    "                    loss = F.mse_loss(qsa_b, target_b) #find a Mean square error loss\n",
    "                    q_network.zero_grad() #reset the gradients of the network\n",
    "                    loss.backward() #calculate gradients using backward propogation\n",
    "                    optim.step() # Iterate over all parameters (tensors) that are supposed \n",
    "                                # to be updated and use internally stored grad to update their values\n",
    "                    loss.item() # get the loss\n",
    "                    stats['MSE Loss'].append(loss.item())\n",
    "                    grid2op_act = self.gym_env.action_space.from_gym(action)\n",
    "\n",
    "                state = next_state\n",
    "                ep_return += reward.item()\n",
    "\n",
    "            stats['Returns'].append(ep_return)\n",
    "\n",
    "            if self.episode % 10 == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict()) #After every 10 episodes load state of original network to\n",
    "                                                                        # target network\n",
    "        self.trained = True\n",
    "        return stats\n",
    "    \n",
    "    def act(self):\n",
    "        if self.trained:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3df8c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_sarsa_agent = DeepSarsaAgent(episodes=20, env_name = \"rte_case14_realistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a7b4f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:00<00:06,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[200]])\n",
      "Memory len is:1 and condition is: 50\n",
      "The action is: tensor([[177]])\n",
      "Memory len is:2 and condition is: 50\n",
      "The action is: tensor([[143]])\n",
      "Memory len is:3 and condition is: 50\n",
      "The action is: tensor([[390]])\n",
      "Memory len is:4 and condition is: 50\n",
      "The action is: tensor([[159]])\n",
      "Memory len is:5 and condition is: 50\n",
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[327]])\n",
      "Memory len is:6 and condition is: 50\n",
      "The action is: tensor([[226]])\n",
      "Memory len is:7 and condition is: 50\n",
      "The action is: tensor([[307]])\n",
      "Memory len is:8 and condition is: 50\n",
      "The action is: tensor([[342]])\n",
      "Memory len is:9 and condition is: 50\n",
      "The action is: tensor([[162]])\n",
      "Memory len is:10 and condition is: 50\n",
      "The action is: tensor([[350]])\n",
      "Memory len is:11 and condition is: 50\n",
      "The action is: tensor([[192]])\n",
      "Memory len is:12 and condition is: 50\n",
      "The action is: tensor([[51]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:00<00:06,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory len is:13 and condition is: 50\n",
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[255]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:01<00:05,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory len is:14 and condition is: 50\n",
      "The action is: tensor([[116]])\n",
      "Memory len is:15 and condition is: 50\n",
      "The action is: tensor([[107]])\n",
      "Memory len is:16 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:01<00:05,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[378]])\n",
      "Memory len is:17 and condition is: 50\n",
      "The action is: tensor([[377]])\n",
      "Memory len is:18 and condition is: 50\n",
      "The action is: tensor([[417]])\n",
      "Memory len is:19 and condition is: 50\n",
      "The action is: tensor([[197]])\n",
      "Memory len is:20 and condition is: 50\n",
      "The action is: tensor([[274]])\n",
      "Memory len is:21 and condition is: 50\n",
      "The action is: tensor([[225]])\n",
      "Memory len is:22 and condition is: 50\n",
      "The action is: tensor([[320]])\n",
      "Memory len is:23 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [00:01<00:04,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[65]])\n",
      "Memory len is:24 and condition is: 50\n",
      "The action is: tensor([[108]])\n",
      "Memory len is:25 and condition is: 50\n",
      "The action is: tensor([[69]])\n",
      "Memory len is:26 and condition is: 50\n",
      "The action is: tensor([[192]])\n",
      "Memory len is:27 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [00:01<00:04,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[194]])\n",
      "Memory len is:28 and condition is: 50\n",
      "The action is: tensor([[122]])\n",
      "Memory len is:29 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [00:02<00:03,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[1]])\n",
      "Memory len is:30 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:02<00:03,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[413]])\n",
      "Memory len is:31 and condition is: 50\n",
      "The action is: tensor([[109]])\n",
      "Memory len is:32 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:02<00:03,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[192]])\n",
      "Memory len is:33 and condition is: 50\n",
      "The action is: tensor([[363]])\n",
      "Memory len is:34 and condition is: 50\n",
      "The action is: tensor([[365]])\n",
      "Memory len is:35 and condition is: 50\n",
      "The action is: tensor([[115]])\n",
      "Memory len is:36 and condition is: 50\n",
      "The action is: tensor([[337]])\n",
      "Memory len is:37 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:03<00:03,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[235]])\n",
      "Memory len is:38 and condition is: 50\n",
      "The action is: tensor([[260]])\n",
      "Memory len is:39 and condition is: 50\n",
      "The action is: tensor([[156]])\n",
      "Memory len is:40 and condition is: 50\n",
      "The action is: tensor([[135]])\n",
      "Memory len is:41 and condition is: 50\n",
      "The action is: tensor([[454]])\n",
      "Memory len is:42 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:03<00:02,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[154]])\n",
      "Memory len is:43 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:03<00:02,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[286]])\n",
      "Memory len is:44 and condition is: 50\n",
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[223]])\n",
      "Memory len is:45 and condition is: 50\n",
      "The action is: tensor([[405]])\n",
      "Memory len is:46 and condition is: 50\n",
      "The action is: tensor([[226]])\n",
      "Memory len is:47 and condition is: 50\n",
      "The action is: tensor([[352]])\n",
      "Memory len is:48 and condition is: 50\n",
      "The action is: tensor([[334]])\n",
      "Memory len is:49 and condition is: 50\n",
      "The action is: tensor([[71]])\n",
      "Memory len is:50 and condition is: 50\n",
      "Memory len is:50 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:03<00:02,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "5162.18359375\n",
      "type=<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [00:04<00:01,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action is: tensor([[120]])\n",
      "Memory len is:51 and condition is: 50\n",
      "Memory len is:51 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3612.47412109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:04<00:01,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[296]])\n",
      "Memory len is:52 and condition is: 50\n",
      "Memory len is:52 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "1795.0924072265625\n",
      "The action is: tensor([[174]])\n",
      "Memory len is:53 and condition is: 50\n",
      "Memory len is:53 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3004.345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [00:04<00:01,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[213]])\n",
      "Memory len is:54 and condition is: 50\n",
      "Memory len is:54 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3463.921875\n",
      "The action is: tensor([[68]])\n",
      "Memory len is:55 and condition is: 50\n",
      "Memory len is:55 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4651.3408203125\n",
      "The action is: tensor([[180]])\n",
      "Memory len is:56 and condition is: 50\n",
      "Memory len is:56 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4673.5087890625\n",
      "The action is: tensor([[5]])\n",
      "Memory len is:57 and condition is: 50\n",
      "Memory len is:57 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3401.93603515625\n",
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[13]])\n",
      "Memory len is:58 and condition is: 50\n",
      "Memory len is:58 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3278.74072265625\n",
      "The action is: tensor([[329]])\n",
      "Memory len is:59 and condition is: 50\n",
      "Memory len is:59 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4810.267578125\n",
      "The action is: tensor([[390]])\n",
      "Memory len is:60 and condition is: 50\n",
      "Memory len is:60 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "2878.966552734375\n",
      "The action is: tensor([[407]])\n",
      "Memory len is:61 and condition is: 50\n",
      "Memory len is:61 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "3014.062744140625\n",
      "The action is: tensor([[287]])\n",
      "Memory len is:62 and condition is: 50\n",
      "Memory len is:62 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4625.19482421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:05<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[43]])\n",
      "Memory len is:63 and condition is: 50\n",
      "Memory len is:63 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "2740.381591796875\n",
      "The action is: tensor([[199]])\n",
      "Memory len is:64 and condition is: 50\n",
      "Memory len is:64 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "14.391748428344727\n",
      "The action is: tensor([[130]])\n",
      "Memory len is:65 and condition is: 50\n",
      "Memory len is:65 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "1473.408447265625\n",
      "The action is: tensor([[271]])\n",
      "Memory len is:66 and condition is: 50\n",
      "Memory len is:66 and condition is: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:05<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "5641.1455078125\n",
      "type=<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [00:05<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action is: tensor([[63]])\n",
      "Memory len is:67 and condition is: 50\n",
      "Memory len is:67 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4780.2734375\n",
      "The action is: tensor([[58]])\n",
      "Memory len is:68 and condition is: 50\n",
      "Memory len is:68 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "4065.79248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:06<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=<class 'numpy.ndarray'>\n",
      "The action is: tensor([[6]])\n",
      "Memory len is:69 and condition is: 50\n",
      "Memory len is:69 and condition is: 50\n",
      "its coming here 1\n",
      "its coming here 2\n",
      "its coming here 3\n",
      "its coming here 4\n",
      "its coming here 5\n",
      "its coming here 6\n",
      "its coming here 7\n",
      "its coming here 8\n",
      "its coming here 9\n",
      "its coming here 10\n",
      "2948.8623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '_redispatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-3e16fa8ee37b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m runner = Runner(**env.get_params_for_runner(),\n\u001b[0;32m     16\u001b[0m                 agentInstance=deep_sarsa_agent, agentClass=None)\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_saved_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The results for the custom agent are:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchron_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcum_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_time_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_ts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\masters\\sem 2\\analytics project\\code\\grid2op\\grid2op\\Runner\\runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, nb_episode, nb_process, path_save, max_iter, pbar, env_seeds, agent_seeds, episode_id, add_detailed_output)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnb_process\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sequential runner used.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                     res = self._run_sequential(\n\u001b[0m\u001b[0;32m   1192\u001b[0m                         \u001b[0mnb_episode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m                         \u001b[0mpath_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_save\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\masters\\sem 2\\analytics project\\code\\grid2op\\grid2op\\Runner\\runner.py\u001b[0m in \u001b[0;36m_run_sequential\u001b[1;34m(self, nb_episode, path_save, pbar, env_seeds, agent_seeds, max_iter, episode_id, add_detailed_output)\u001b[0m\n\u001b[0;32m    799\u001b[0m                     \u001b[0mnb_time_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[0mepisode_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m                 \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_one_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m                     \u001b[0mpath_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_save\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m                     \u001b[0mindx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\masters\\sem 2\\analytics project\\code\\grid2op\\grid2op\\Runner\\runner.py\u001b[0m in \u001b[0;36mrun_one_episode\u001b[1;34m(self, indx, path_save, pbar, env_seed, max_iter, agent_seed, episode_id, detailed_output)\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             res = _aux_run_one_episode(\n\u001b[0m\u001b[0;32m    706\u001b[0m                 \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\masters\\sem 2\\analytics project\\code\\grid2op\\grid2op\\Runner\\aux_fun.py\u001b[0m in \u001b[0;36m_aux_run_one_episode\u001b[1;34m(env, agent, logger, indx, path_save, pbar, env_seed, agent_seed, max_iter, detailed_output)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mtime_act\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mend__\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeg__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# should load the first time stamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m             \u001b[0mcum_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\masters\\sem 2\\analytics project\\code\\grid2op\\grid2op\\Environment\\BaseEnv.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   2781\u001b[0m         \u001b[0mexcept_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2782\u001b[0m         \u001b[0mdetailed_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2783\u001b[1;33m         \u001b[0minit_disp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_redispatch\u001b[0m  \u001b[1;31m# dispatching action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2784\u001b[0m         \u001b[0maction_storage_power\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage_power\u001b[0m  \u001b[1;31m# battery information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2785\u001b[0m         \u001b[0mattack_duration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute '_redispatch'"
     ]
    }
   ],
   "source": [
    "max_iter = 100 #customize\n",
    "from grid2op.Runner import Runner\n",
    "import os\n",
    "# from grid2op.Agent import DoNothingAgent\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Chronics import GridStateFromFileWithForecasts\n",
    "\n",
    "path_saved_data = './Res'\n",
    "if not os.path.exists(path_saved_data):\n",
    "    os.mkdir(path_saved_data)\n",
    "\n",
    "env_name = \"rte_case14_realistic\"\n",
    "env = grid2op.make(env_name)\n",
    "\n",
    "runner = Runner(**env.get_params_for_runner(),\n",
    "                agentInstance=deep_sarsa_agent, agentClass=None)\n",
    "res = runner.run(nb_episode=20, max_iter=max_iter, path_save=path_saved_data)\n",
    "print(\"The results for the custom agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372718b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
